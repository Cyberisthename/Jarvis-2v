{
  "visited_urls": [
    "https://en.wikipedia.org/wiki/Glossary_of_botanical_terms",
    "https://en.wikipedia.org/wiki/Category:CS1_German-language_sources_(de)",
    "https://en.wikipedia.org/wiki/Glossary_of_developmental_biology",
    "https://en.wikipedia.org/wiki/Glossary_of_engineering:_A%E2%80%93L",
    "https://en.wikipedia.org/wiki/Category:Pages_displaying_short_descriptions_of_redirect_targets_via_Module:Annotated_link",
    "https://en.wikipedia.org/wiki/Glossary_of_entomology_terms",
    "https://en.wikipedia.org/wiki/Glossary_of_architecture",
    "https://en.wikipedia.org/wiki/Category:Computational_neuroscience",
    "https://en.wikipedia.org/wiki/Talcott_Parsons",
    "https://en.wikipedia.org/wiki/Help:Category",
    "https://en.wikipedia.org/wiki/Category:CS1_errors:_unrecognized_parameter",
    "https://en.wikipedia.org/wiki/Glossary_of_computer_hardware_terms",
    "https://en.wikipedia.org/wiki/Glossary_of_cellular_and_molecular_biology_(0%E2%80%93L)",
    "https://en.wikipedia.org/wiki/Stuart_Kauffman",
    "https://en.wikipedia.org/wiki/Glossary_of_quantum_computing",
    "https://en.wikipedia.org/wiki/Walter_Pitts",
    "https://en.wikipedia.org/wiki/Glossary_of_geography_terms_(A%E2%80%93M)",
    "https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch",
    "https://en.wikipedia.org/wiki/Glossary_of_robotics",
    "https://en.wikipedia.org/wiki/Machine_learning",
    "https://en.wikipedia.org/wiki/Glossary_of_meteorology",
    "https://en.wikipedia.org/wiki/Category:Articles_with_excerpts",
    "https://en.wikipedia.org/wiki/Glossary_of_agriculture",
    "https://en.wikipedia.org/wiki/Glossary_of_geology",
    "https://en.wikipedia.org/wiki/Glossary_of_Hebrew_toponyms",
    "https://en.wikipedia.org/wiki/Glossary_of_mechanical_engineering",
    "https://en.wikipedia.org//en.wikipedia.org/w/index.php?title=Artificial_intelligence&mobileaction=toggle_view_mobile",
    "https://en.wikipedia.org/wiki/Category:Wikipedia_indefinitely_semi-protected_pages",
    "https://en.wikipedia.org/wiki/Science",
    "https://en.wikipedia.org/wiki/Stuart_Umpleby",
    "https://en.wikipedia.org/wiki/Category:Computational_fields_of_study",
    "https://en.wikipedia.org/wiki/Category:Formal_sciences",
    "https://en.wikipedia.org/wiki/Glossary_of_ecology",
    "https://en.wikipedia.org/wiki/Category:Pages_using_Sister_project_links_with_hidden_wikidata",
    "https://en.wikipedia.org/wiki/Template:Glossaries_of_science_and_engineering",
    "https://en.wikipedia.org/wiki/Category:Cybernetics",
    "https://en.wikipedia.org/wiki/Valentin_Turchin",
    "https://en.wikipedia.org/wiki/Glossary_of_medicine",
    "https://en.wikipedia.org/wiki/Glossary_of_geography_terms_(N%E2%80%93Z)",
    "https://en.wikipedia.org/wiki/Glossary_of_archaeology",
    "https://en.wikipedia.org/wiki/Engineering",
    "https://en.wikipedia.org/wiki/Glossary_of_nanotechnology",
    "https://en.wikipedia.org/wiki/Glossary_of_genetics_and_evolutionary_biology",
    "https://en.wikipedia.org/wiki/Category:Articles_with_short_description",
    "https://en.wikipedia.org/wiki/Glossary_of_engineering:_M%E2%80%93Z",
    "https://en.wikipedia.org/wiki/Glossary_of_biology",
    "https://en.wikipedia.org/wiki/Glossary_of_aerospace_engineering",
    "https://en.wikipedia.org/wiki/Glossary_of_civil_engineering",
    "https://en.wikipedia.org/wiki/Category:Intelligence_by_type",
    "https://en.wikipedia.org/wiki/Category:All_accuracy_disputes",
    "https://en.wikipedia.org/wiki/Category:CS1_Japanese-language_sources_(ja)",
    "https://en.wikipedia.org/wiki/Glossary_of_virology",
    "https://en.wikipedia.org/wiki/Glossary_of_physics",
    "https://en.wikipedia.org/wiki/Glossary_of_clinical_research",
    "https://en.wikipedia.org/wiki/Glossary_of_calculus",
    "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence",
    "https://en.wikipedia.org/wiki/Glossary_of_psychiatry",
    "https://en.wikipedia.org/wiki/Category:Artificial_intelligence",
    "https://en.wikipedia.org/wiki/Glossary_of_probability_and_statistics",
    "https://en.wikipedia.org/wiki/Glossary_of_astronomy",
    "https://en.wikipedia.org/wiki/Glossary_of_computer_science",
    "https://en.wikipedia.org/wiki/Glossary_of_Arabic_toponyms",
    "https://en.wikipedia.org/wiki/Glossary_of_cell_biology",
    "https://en.wikipedia.org/wiki/Category:Data_science",
    "https://en.wikipedia.org/wiki/Glossary_of_cellular_and_molecular_biology_(M%E2%80%93Z)",
    "https://en.wikipedia.org/wiki/Glossary_of_economics",
    "https://en.wikipedia.org/wiki/Glossary_of_scientific_naming",
    "https://en.wikipedia.org/wiki/W._Ross_Ashby",
    "https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License",
    "https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata",
    "https://en.wikipedia.org/wiki/Valentino_Braitenberg",
    "https://en.wikipedia.org/wiki/Sergei_P._Kurdyumov",
    "https://en.wikipedia.org/wiki/Glossary_of_areas_of_mathematics",
    "https://en.wikipedia.org/wiki/Glossary_of_structural_engineering",
    "https://en.wikipedia.org/wiki/Glossary_of_electrical_and_electronics_engineering",
    "https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=1315925624",
    "https://en.wikipedia.org/wiki/Oikonyms_in_Western_and_South_Asia",
    "https://en.wikipedia.org/wiki/Glossary_of_bird_terms",
    "https://en.wikipedia.org/wiki/Glossary_of_ichthyology",
    "https://en.wikipedia.org/wiki/Category:Articles_with_disputed_statements_from_July_2024",
    "https://en.wikipedia.org/wiki/Wikipedia:About",
    "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "https://en.wikipedia.org/wiki/Category:CS1:_long_volume_value",
    "https://en.wikipedia.org/wiki/Glossary_of_environmental_science",
    "https://en.wikipedia.org/wiki/Category:Use_dmy_dates_from_July_2023",
    "https://en.wikipedia.org/wiki/Stafford_Beer",
    "https://en.wikipedia.org/wiki/Category:CS1_Russian-language_sources_(ru)",
    "https://en.wikipedia.org/wiki/Category:Articles_with_Internet_Encyclopedia_of_Philosophy_links",
    "https://en.wikipedia.org/wiki/Help:Authority_control",
    "https://en.wikipedia.org/wiki/William_Grey_Walter",
    "https://en.wikipedia.org/wiki/Glossary_of_mycology",
    "https://en.wikipedia.org//en.wikipedia.org/wiki/Wikipedia:Contact_us",
    "https://en.wikipedia.org/wiki/Template_talk:Glossaries_of_science_and_engineering",
    "https://en.wikipedia.org/wiki/Ulla_Mitzdorf",
    "https://en.wikipedia.org/wiki/Special:EditPage/Template:Glossaries_of_science_and_engineering",
    "https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer",
    "https://en.wikipedia.org/wiki/Glossary_of_chemistry_terms",
    "https://en.wikipedia.org/wiki/Glossary_of_machine_vision",
    "https://en.wikipedia.org/wiki/Walter_Bradford_Cannon",
    "https://en.wikipedia.org/wiki/Category:Webarchive_template_wayback_links"
  ],
  "collected_data": [
    {
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
      "text": "\n Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]\n High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3]\n Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI)\u2014AI that can complete virtually any cognitive task at least as well as a human.\n Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]\n Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]\n Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\n Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22]\n A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.\n Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]\n An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences\u2014there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]\n In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\n In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\n A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]\n Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]\n Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]\n There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]\n In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]\n Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]\n Natural language processing (NLP) allows programs to read, write and communicate in human languages.[50] Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]\n Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\n Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]\n Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]\n The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61] object tracking,[62] and robotic perception.[63]\n Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction.\n However, this tends to give na\u00efve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]\n A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68]\n AI research uses a wide variety of techniques to accomplish the goals above.[b]\n AI can solve many problems by intelligently searching through many possible solutions.[69] There are two very different kinds of search used in AI: state space search and local search.\n State space search searches through a tree of possible states to try to find a goal state.[70] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[71]\n Simple exhaustive searches[72] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[73]\n Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[74]\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[75]\n Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[76] through the backpropagation algorithm.\n Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[77]\n Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[78]\n Formal logic is used for reasoning and knowledge representation.[79]\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[80] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[81]\n Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[82] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\n Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[83] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[84]\n Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[85]\n Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[86]\n Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.\n Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[87] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[88] and information value theory.[89] These tools include models such as Markov decision processes,[90] dynamic decision networks,[91] game theory and mechanism design.[92]\n Bayesian networks[93] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][95] learning (using the expectation\u2013maximization algorithm),[h][97] planning (using decision networks)[98] and perception (using dynamic Bayesian networks).[91]\n Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[91]\n The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[99] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]\n There are many kinds of classifiers in use.[100] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102]\nThe naive Bayes classifier is reportedly the \"most widely used learner\"[103] at Google, due in part to its scalability.[104]\nNeural networks are also used as classifiers.[105]\n An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105]\n Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107]\n In feedforward neural networks the signal passes in only one direction.[108] The term perceptron typically refers to a single-layer neural network.[109] In contrast, deep learning uses many layers.[110] Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem.[111] Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.[112]\n Deep learning uses several layers of neurons between the network's inputs and outputs.[110] The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[114]\n Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[115] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[116] The sudden success of deep learning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]\n Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.[124] Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.[125][126]\n Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI.[127] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[128]\n In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[129] Specialized programming languages such as Prolog were used in early AI research,[130] but general-purpose programming languages like Python have become predominant.[131]\n The transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster,[132] a trend sometimes called Huang's law,[133] named after Nvidia co-founder and CEO Jensen Huang.\n AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\n The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[134] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[135][136]\n For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[137] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[137][138] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[139] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[140] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[141][142]\n Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[143] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[144] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[145] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[146] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[147] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[148] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[149] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[150] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[151]\n Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[152] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[153] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[154] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[155] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[156] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[157]\n Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve[158] all from Google DeepMind,[159] Llemma from EleutherAI[160] or Julius.[161]\n When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.[162]\n Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[163]\n Topological deep learning integrates various topological approaches.\n Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[164]\n According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[165]\n Various countries are deploying AI military applications.[166] The main applications enhance command and control, communications, sensors, integration and interoperability.[167] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[166] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.[167]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[166][168][169][170]\n Generative artificial intelligence (Generative AI, GenAI,[171] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data.[172][173][174] These models learn the underlying patterns and structures of their training data and use them to produce new data[175][176] based on the input, which often comes in the form of natural language prompts.[177][178]\n Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora.[179][180][181][182][183] Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.[177][184][185]\n AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[196][197][198]\n Microsoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries[199] and step-by-step reasoning based of information from web publishers, ranked in Bing Search.[200] \nFor safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.[201]\n Google officially pushed its AI Search at its Google I/O event on May 20, 2025.[202] It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.[203]\n Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions,[204] AI-integrated sex toys (e.g., teledildonics),[205] AI-generated sexual education content,[206] and AI agents that simulate sexual and romantic partners (e.g., Replika).[207]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[208]\n AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[209][210]\n There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[211] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\n AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.[212][213][214]\n In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\n During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[215]\n AI has potential benefits and potential risks.[218] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[219] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[220][221] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[222]\n Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\n AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\n Sensitive user data collected may include online activity records, geolocation data, video, or audio.[223] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[224] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[225]\n AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[226] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[227]\n Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[228][229] Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[230] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[231][232] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[233]\n The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[234][235][236] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[237][238]\n In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[239] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[240]\n Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[241]\n A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[242] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[243]\n In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million.[244] Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.[245]\n In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[246] The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.[247]\n After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[248] Taiwan aims to phase out nuclear power by 2025.[248] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[248]\n Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI.[249] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[249]\n On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[250] \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[250]\n In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300\u2013500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.[251]\n YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[252] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[253] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.[254]\n In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing,[255] while realistic AI-generated videos became feasible in the mid-2020s.[256][257][258] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda;[259] one such potential malicious use is deepfakes for computational propaganda.[260] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[261]\n AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.[262]\n Machine learning applications will be biased[k] if they learn from biased data.[264] The developers may not be aware that the bias exists.[265] Bias can be introduced by the way training data is selected and by the way a model is deployed.[266][264] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[267] The field of fairness studies how to prevent harms from algorithmic biases.\n On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[268] a problem called \"sample size disparity\".[269] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[270]\n COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[271] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[273]\n A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[274] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[275]\n Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[276] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]\n Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[269]\n There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[263]\n At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious \u2013 discuss][278]\n Many AI systems are so complex that their designers cannot explain how they reach their decisions.[279] Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[280]\n It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[281] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[282]\n People who have been harmed by an algorithm's decision have a right to an explanation.[283] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[284]\n DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[285]\n Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[286] LIME can locally approximate a model's outputs with a simpler, interpretable model.[287] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[288] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[289] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[290]\n Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\n A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[292] Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.[292] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[293] By 2015, over fifty countries were reported to be researching battlefield robots.[294]\n AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[295] All these technologies have been available since 2020 or earlier\u2014AI facial recognition systems are already being used for mass surveillance in China.[296][297]\n There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[298]\n Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[299]\n In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[300] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[301] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][303] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[299] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[304][305]\n Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[306] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[307]\n From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[308]\n It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[309] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways.\n First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer).[311] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[312] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[313]\n Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[314]\n The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[315] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[316] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\n In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\".[317] He notably mentioned risks of an AI takeover,[318] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[319]\n In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[320]\n Some other researchers were more optimistic. AI pioneer J\u00fcrgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[321] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[322][323] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested interests.\"[324] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[325] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[326] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[327]\n Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[328]\n Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[329]\nThe field of machine ethics is also called computational morality,[329]\nand was founded at an AAAI symposium in 2005.[330]\n Other approaches include Wendell Wallach's \"artificial moral agents\"[331] and Stuart J. Russell's three principles for developing provably beneficial machines.[332]\n Active organizations in the AI open-source community include Hugging Face,[333] Google,[334] EleutherAI and Meta.[335] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[336][337] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[338] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[339]\n Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:[340][341]\n Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[342] however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.[343]\n Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[344]\n The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[345]\n The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[346] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[347] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[348][349] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[350] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[350] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[350] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[351] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[352] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics.[353] On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation.[354] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[355]\n In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[348] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[356] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[357][358]\n In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[359] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[360][361] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[362][363]\n The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[365][366] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[368] such as McCulloch and Pitts design for \"artificial neurons\" in 1943,[117] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[369][366]\n The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[366]\n Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[373] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[374] In 1967 Marvin Minsky agreed, writing that \"within a generation\u00a0... the problem of creating 'artificial intelligence' will substantially be solved\".[375] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[377] and ongoing pressure from the U.S. Congress to fund more productive projects.[378] Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[379] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9]\n In the early 1980s, AI research was revived by the commercial success of expert systems,[380] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]\n Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[381] and began to look into \"sub-symbolic\" approaches.[382] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[87][387] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[388] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[389]\n AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[390] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[391]\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[68]\n Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y]\nDeep learning's success was based on both hardware improvements (faster computers,[393] graphics processing units, cloud computing[394]) and access to large amounts of data[395] (including curated datasets,[394] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019.[350]\n In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[327]\n In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[396] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[397] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[398] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[399] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[400] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[401]\n Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[402] Another major focus has been whether machines can be conscious, and the associated ethical implications.[403] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[404] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[403]\n Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[405] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[405] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[369] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[406]\n Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[408] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[409]\n McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[410] Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".[411] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine\u2014and no other philosophical discussion is required, or may not even be possible.\n Another definition has been adopted by Google,[412] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself[413] including discussing the many AI narratives and myths to be found within societal, political and academic discourses.[414] Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms,[415] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[416]\n There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.[417]\n No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n Symbolic AI (or \"GOFAI\")[419] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[420]\n However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[421] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[422] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]\n The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[424][425] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[426] but eventually was seen as irrelevant. Modern AI has elements of both.\n Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[427][428] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[429] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[430] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[431]\n Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[432]\n Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[436]\n It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[437] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[438][439] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[438] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[440]\n In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[441] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.[442][443]\n Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[439][438]\n A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[428] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[444]\n However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[445]\n Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[446]\n Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[447]\n Thought-capable artificial beings have appeared as storytelling devices since antiquity,[448] and have been a persistent theme in science fiction.[449]\n A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[450]\n Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[451] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[452]\n Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[453]\n The two most widely used textbooks in 2023 (see the Open Syllabus):\n The four most widely used AI textbooks in 2008:\n Other textbooks:\n",
      "timestamp": "2025-10-09 19:11:50.428174"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
      "title": "Artificial intelligence - Wikipedia",
      "description": "",
      "text": "\n Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]\n High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3]\n Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI)\u2014AI that can complete virtually any cognitive task at least as well as a human.\n Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]\n Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]\n Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\n Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22]\n A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.\n Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]\n An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences\u2014there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]\n In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\n In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\n A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]\n Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]\n Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]\n There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]\n In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]\n Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]\n Natural language processing (NLP) allows programs to read, write and communicate in human languages.[50] Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]\n Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\n Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]\n Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]\n The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61] object tracking,[62] and robotic perception.[63]\n Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction.\n However, this tends to give na\u00efve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]\n A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68]\n AI research uses a wide variety of techniques to accomplish the goals above.[b]\n AI can solve many problems by intelligently searching through many possible solutions.[69] There are two very different kinds of search used in AI: state space search and local search.\n State space search searches through a tree of possible states to try to find a goal state.[70] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[71]\n Simple exhaustive searches[72] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[73]\n Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[74]\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[75]\n Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[76] through the backpropagation algorithm.\n Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[77]\n Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[78]\n Formal logic is used for reasoning and knowledge representation.[79]\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[80] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[81]\n Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[82] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\n Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[83] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[84]\n Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[85]\n Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[86]\n Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.\n Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[87] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[88] and information value theory.[89] These tools include models such as Markov decision processes,[90] dynamic decision networks,[91] game theory and mechanism design.[92]\n Bayesian networks[93] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][95] learning (using the expectation\u2013maximization algorithm),[h][97] planning (using decision networks)[98] and perception (using dynamic Bayesian networks).[91]\n Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[91]\n The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[99] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]\n There are many kinds of classifiers in use.[100] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102]\nThe naive Bayes classifier is reportedly the \"most widely used learner\"[103] at Google, due in part to its scalability.[104]\nNeural networks are also used as classifiers.[105]\n An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105]\n Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107]\n In feedforward neural networks the signal passes in only one direction.[108] The term perceptron typically refers to a single-layer neural network.[109] In contrast, deep learning uses many layers.[110] Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem.[111] Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.[112]\n Deep learning uses several layers of neurons between the network's inputs and outputs.[110] The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[114]\n Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[115] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[116] The sudden success of deep learning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]\n Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.[124] Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.[125][126]\n Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI.[127] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[128]\n In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[129] Specialized programming languages such as Prolog were used in early AI research,[130] but general-purpose programming languages like Python have become predominant.[131]\n The transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster,[132] a trend sometimes called Huang's law,[133] named after Nvidia co-founder and CEO Jensen Huang.\n AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\n The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[134] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[135][136]\n For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[137] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[137][138] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[139] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[140] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[141][142]\n Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[143] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[144] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[145] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[146] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[147] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[148] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[149] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[150] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[151]\n Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[152] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[153] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[154] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[155] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[156] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[157]\n Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve[158] all from Google DeepMind,[159] Llemma from EleutherAI[160] or Julius.[161]\n When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.[162]\n Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[163]\n Topological deep learning integrates various topological approaches.\n Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[164]\n According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[165]\n Various countries are deploying AI military applications.[166] The main applications enhance command and control, communications, sensors, integration and interoperability.[167] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[166] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.[167]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[166][168][169][170]\n Generative artificial intelligence (Generative AI, GenAI,[171] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data.[172][173][174] These models learn the underlying patterns and structures of their training data and use them to produce new data[175][176] based on the input, which often comes in the form of natural language prompts.[177][178]\n Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora.[179][180][181][182][183] Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.[177][184][185]\n AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[196][197][198]\n Microsoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries[199] and step-by-step reasoning based of information from web publishers, ranked in Bing Search.[200] \nFor safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.[201]\n Google officially pushed its AI Search at its Google I/O event on May 20, 2025.[202] It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.[203]\n Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions,[204] AI-integrated sex toys (e.g., teledildonics),[205] AI-generated sexual education content,[206] and AI agents that simulate sexual and romantic partners (e.g., Replika).[207]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[208]\n AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[209][210]\n There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[211] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\n AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.[212][213][214]\n In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\n During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[215]\n AI has potential benefits and potential risks.[218] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[219] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[220][221] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[222]\n Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\n AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\n Sensitive user data collected may include online activity records, geolocation data, video, or audio.[223] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[224] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[225]\n AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[226] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[227]\n Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[228][229] Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[230] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[231][232] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[233]\n The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[234][235][236] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[237][238]\n In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[239] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[240]\n Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[241]\n A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[242] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[243]\n In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million.[244] Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.[245]\n In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[246] The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.[247]\n After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[248] Taiwan aims to phase out nuclear power by 2025.[248] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[248]\n Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI.[249] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[249]\n On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[250] \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[250]\n In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300\u2013500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.[251]\n YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[252] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[253] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.[254]\n In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing,[255] while realistic AI-generated videos became feasible in the mid-2020s.[256][257][258] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda;[259] one such potential malicious use is deepfakes for computational propaganda.[260] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[261]\n AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.[262]\n Machine learning applications will be biased[k] if they learn from biased data.[264] The developers may not be aware that the bias exists.[265] Bias can be introduced by the way training data is selected and by the way a model is deployed.[266][264] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[267] The field of fairness studies how to prevent harms from algorithmic biases.\n On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[268] a problem called \"sample size disparity\".[269] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[270]\n COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[271] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[273]\n A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[274] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[275]\n Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[276] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]\n Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[269]\n There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[263]\n At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious \u2013 discuss][278]\n Many AI systems are so complex that their designers cannot explain how they reach their decisions.[279] Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[280]\n It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[281] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[282]\n People who have been harmed by an algorithm's decision have a right to an explanation.[283] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[284]\n DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[285]\n Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[286] LIME can locally approximate a model's outputs with a simpler, interpretable model.[287] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[288] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[289] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[290]\n Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\n A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[292] Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.[292] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[293] By 2015, over fifty countries were reported to be researching battlefield robots.[294]\n AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[295] All these technologies have been available since 2020 or earlier\u2014AI facial recognition systems are already being used for mass surveillance in China.[296][297]\n There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[298]\n Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[299]\n In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[300] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[301] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][303] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[299] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[304][305]\n Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[306] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[307]\n From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[308]\n It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[309] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways.\n First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer).[311] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[312] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[313]\n Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[314]\n The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[315] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[316] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\n In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\".[317] He notably mentioned risks of an AI takeover,[318] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[319]\n In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[320]\n Some other researchers were more optimistic. AI pioneer J\u00fcrgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[321] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[322][323] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested interests.\"[324] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[325] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[326] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[327]\n Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[328]\n Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[329]\nThe field of machine ethics is also called computational morality,[329]\nand was founded at an AAAI symposium in 2005.[330]\n Other approaches include Wendell Wallach's \"artificial moral agents\"[331] and Stuart J. Russell's three principles for developing provably beneficial machines.[332]\n Active organizations in the AI open-source community include Hugging Face,[333] Google,[334] EleutherAI and Meta.[335] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[336][337] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[338] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[339]\n Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:[340][341]\n Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[342] however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.[343]\n Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[344]\n The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[345]\n The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[346] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[347] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[348][349] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[350] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[350] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[350] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[351] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[352] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics.[353] On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation.[354] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[355]\n In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[348] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[356] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[357][358]\n In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[359] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[360][361] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[362][363]\n The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[365][366] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[368] such as McCulloch and Pitts design for \"artificial neurons\" in 1943,[117] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[369][366]\n The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[366]\n Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[373] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[374] In 1967 Marvin Minsky agreed, writing that \"within a generation\u00a0... the problem of creating 'artificial intelligence' will substantially be solved\".[375] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[377] and ongoing pressure from the U.S. Congress to fund more productive projects.[378] Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[379] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9]\n In the early 1980s, AI research was revived by the commercial success of expert systems,[380] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]\n Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[381] and began to look into \"sub-symbolic\" approaches.[382] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[87][387] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[388] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[389]\n AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[390] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[391]\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[68]\n Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y]\nDeep learning's success was based on both hardware improvements (faster computers,[393] graphics processing units, cloud computing[394]) and access to large amounts of data[395] (including curated datasets,[394] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019.[350]\n In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[327]\n In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[396] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[397] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[398] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[399] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[400] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[401]\n Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[402] Another major focus has been whether machines can be conscious, and the associated ethical implications.[403] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[404] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[403]\n Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[405] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[405] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[369] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[406]\n Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[408] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[409]\n McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[410] Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".[411] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine\u2014and no other philosophical discussion is required, or may not even be possible.\n Another definition has been adopted by Google,[412] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself[413] including discussing the many AI narratives and myths to be found within societal, political and academic discourses.[414] Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms,[415] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[416]\n There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.[417]\n No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n Symbolic AI (or \"GOFAI\")[419] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[420]\n However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[421] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[422] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]\n The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[424][425] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[426] but eventually was seen as irrelevant. Modern AI has elements of both.\n Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[427][428] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[429] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[430] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[431]\n Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[432]\n Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[436]\n It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[437] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[438][439] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[438] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[440]\n In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[441] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.[442][443]\n Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[439][438]\n A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[428] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[444]\n However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[445]\n Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[446]\n Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[447]\n Thought-capable artificial beings have appeared as storytelling devices since antiquity,[448] and have been a persistent theme in science fiction.[449]\n A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[450]\n Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[451] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[452]\n Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[453]\n The two most widely used textbooks in 2023 (see the Open Syllabus):\n The four most widely used AI textbooks in 2008:\n Other textbooks:\n",
      "timestamp": "2025-10-09 19:11:50.447205"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Machine_learning",
      "text": "\n Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\n Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[4][5]\n From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimization under this framework.\n The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[6][7] The synonym self-teaching computers was also used in this time period.[8][9]\n The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[10] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[11] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[10] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[10]\n By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.[12] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[13] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[14] In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[15]\n Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\"[16] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[17]\n Modern day Machine Learning algorithms are broken into 3 algorithms types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.[18]\n As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics.[20] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[21]:\u200a488\u200a\n However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[21]:\u200a488\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favour.[22] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[21]:\u200a708\u2013710,\u200a755\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[21]:\u200a25\u200a\n Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[22]\n There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".[23][24][25]\n An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space \u2135, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[26]\n According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.\n Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[27] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[28]\n In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[29]\n Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[30]\n Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).[33]\n Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns.[34]\n Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[35]\n Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[36] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\n Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[37]\n Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks.[38] Statistical physics is thus finding applications in the area of medical diagnostics.[39]\n A core objective of a learner is to generalise from its experience.[3][40] Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning  model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\u2013variance decomposition is one way to quantify generalisation error.\n For the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.[41]\n In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n \n Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n Although each algorithm has advantages and limitations, no single algorithm works for all problems.[42][43][44]\n Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[45] The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[46] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[16]\n Types of supervised-learning algorithms include active learning, classification and regression.[47] Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.[48]\n Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n Unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[5] and density estimation.[49]\n Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\n A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.[50][51]\n Semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.\n In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[52]\n Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques.[53] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[54] In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularisation.\n Other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.[55]\n Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[56][57] It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[58]\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n It is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.[59]\n Several learning algorithms aim at discovering better representations of the inputs provided during training.[60] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\n Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation[61] and various forms of clustering.[62][63][64]\n Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[65] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[66]\n Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[67] A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[68]\n In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[69] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[70]\n In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[71]\n Three broad categories of anomaly detection techniques exist.[72] Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[73][74] and finally meta-learning (e.g. MAML).\n Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[75]\n Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[76] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[77] For example, the rule \n\n\n\n{\n\no\nn\ni\no\nn\ns\n,\np\no\nt\na\nt\no\ne\ns\n\n}\n\u21d2\n{\n\nb\nu\nr\ng\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[78]\n Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\n Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[79][80][81] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[82] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions.[83] By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[84]\n Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\n The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[85]\n Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n Random forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data from the training set. This random selection of RFR for training enables model to reduce bias predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor tasks. This makes RFR compatible to be used in various applications.[86][87]\n Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[88] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[89]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\n Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,[90] which are inherently multi-dimensional.\n A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\n Given a set of observed points, or input\u2013output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\n Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation.\n A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[92][93] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[94]\n The theory of belief functions, also referred to as evidence theory or Dempster\u2013Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities.[95] However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[96][7] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems,[97] association rule learning,[98] artificial immune systems,[99] and other similar models. These methods extract patterns from data and evolve rules over time.\n Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[100]\n There are many applications for machine learning, including:\n In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[104] Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[105] In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis.[106] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[107] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists.[108] In 2019 Springer Nature published the first research book created using machine learning.[109] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[110] Machine learning was recently applied to predict the pro-environmental behaviour of travellers.[111] Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone.[112][113][114] When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[115]\n Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[116]\n Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[117][118][119] Other applications have been focusing on pre evacuation decisions in building fires.[120][121]\n Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[122][123][124] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[125]\n The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[126] The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.[126]\n In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[127] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[128][129] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[130]\n Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[131]\n Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[132] It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[133] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.[134]\n Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[135] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.[136][137]\n Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[138] Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.[139]\n Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[140][141][142]\n Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[143]\n In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[144]\n \n The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[145] This includes algorithmic biases, fairness,[146] automated decision-making,[147] accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation,[148] how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[145]\n Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[149]\n Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.[150] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.[149] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[151][152] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.[153]\n While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[154] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.[155] Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[155]\n Language models learned from data have been shown to contain human-like biases.[156][157] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[158][159] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[160]\n In an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\".[153] In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas.[161] Similar issues with recognising non-white people have been found in many other systems.[162]\n Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[163] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and\u2014most importantly\u2014it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"[164]\n There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[165]\n Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[166] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[167] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[168][169]\n Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency.[170] Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.\n Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.[171]\n A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[172][173]\n Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.[174][175][176][177] Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration,[178][179] approximate computing,[180] and model optimisation.[181][182] Common optimisation techniques include pruning, quantization, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing.\n Software suites containing a variety of machine learning algorithms include the following:\n",
      "timestamp": "2025-10-09 19:11:51.333382"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Machine_learning",
      "title": "Machine learning - Wikipedia",
      "description": "",
      "text": "\n Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\n Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[4][5]\n From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimization under this framework.\n The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[6][7] The synonym self-teaching computers was also used in this time period.[8][9]\n The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[10] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[11] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[10] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[10]\n By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.[12] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[13] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[14] In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[15]\n Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\"[16] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[17]\n Modern day Machine Learning algorithms are broken into 3 algorithms types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.[18]\n As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics.[20] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[21]:\u200a488\u200a\n However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[21]:\u200a488\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favour.[22] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[21]:\u200a708\u2013710,\u200a755\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[21]:\u200a25\u200a\n Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[22]\n There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".[23][24][25]\n An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space \u2135, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[26]\n According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.\n Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[27] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[28]\n In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[29]\n Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[30]\n Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).[33]\n Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns.[34]\n Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[35]\n Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[36] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\n Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[37]\n Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks.[38] Statistical physics is thus finding applications in the area of medical diagnostics.[39]\n A core objective of a learner is to generalise from its experience.[3][40] Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning  model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\u2013variance decomposition is one way to quantify generalisation error.\n For the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.[41]\n In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n \n Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n Although each algorithm has advantages and limitations, no single algorithm works for all problems.[42][43][44]\n Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[45] The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[46] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[16]\n Types of supervised-learning algorithms include active learning, classification and regression.[47] Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.[48]\n Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n Unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[5] and density estimation.[49]\n Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\n A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.[50][51]\n Semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.\n In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[52]\n Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques.[53] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[54] In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularisation.\n Other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.[55]\n Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[56][57] It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[58]\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n It is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.[59]\n Several learning algorithms aim at discovering better representations of the inputs provided during training.[60] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\n Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation[61] and various forms of clustering.[62][63][64]\n Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[65] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[66]\n Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[67] A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[68]\n In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[69] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[70]\n In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[71]\n Three broad categories of anomaly detection techniques exist.[72] Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[73][74] and finally meta-learning (e.g. MAML).\n Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[75]\n Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[76] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[77] For example, the rule \n\n\n\n{\n\no\nn\ni\no\nn\ns\n,\np\no\nt\na\nt\no\ne\ns\n\n}\n\u21d2\n{\n\nb\nu\nr\ng\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[78]\n Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\n Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[79][80][81] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[82] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions.[83] By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[84]\n Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\n The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[85]\n Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n Random forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data from the training set. This random selection of RFR for training enables model to reduce bias predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor tasks. This makes RFR compatible to be used in various applications.[86][87]\n Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[88] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[89]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\n Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,[90] which are inherently multi-dimensional.\n A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\n Given a set of observed points, or input\u2013output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\n Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation.\n A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[92][93] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[94]\n The theory of belief functions, also referred to as evidence theory or Dempster\u2013Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities.[95] However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[96][7] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems,[97] association rule learning,[98] artificial immune systems,[99] and other similar models. These methods extract patterns from data and evolve rules over time.\n Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[100]\n There are many applications for machine learning, including:\n In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[104] Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[105] In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis.[106] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[107] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists.[108] In 2019 Springer Nature published the first research book created using machine learning.[109] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[110] Machine learning was recently applied to predict the pro-environmental behaviour of travellers.[111] Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone.[112][113][114] When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[115]\n Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[116]\n Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[117][118][119] Other applications have been focusing on pre evacuation decisions in building fires.[120][121]\n Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[122][123][124] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[125]\n The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[126] The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.[126]\n In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[127] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[128][129] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[130]\n Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[131]\n Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[132] It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[133] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.[134]\n Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[135] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.[136][137]\n Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[138] Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.[139]\n Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[140][141][142]\n Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[143]\n In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[144]\n \n The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[145] This includes algorithmic biases, fairness,[146] automated decision-making,[147] accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation,[148] how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[145]\n Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[149]\n Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.[150] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.[149] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[151][152] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.[153]\n While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[154] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.[155] Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[155]\n Language models learned from data have been shown to contain human-like biases.[156][157] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[158][159] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[160]\n In an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\".[153] In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas.[161] Similar issues with recognising non-white people have been found in many other systems.[162]\n Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[163] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and\u2014most importantly\u2014it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"[164]\n There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[165]\n Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[166] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[167] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[168][169]\n Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency.[170] Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.\n Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.[171]\n A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[172][173]\n Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.[174][175][176][177] Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration,[178][179] approximate computing,[180] and model optimisation.[181][182] Common optimisation techniques include pruning, quantization, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing.\n Software suites containing a variety of machine learning algorithms include the following:\n",
      "timestamp": "2025-10-09 19:11:51.344878"
    },
    {
      "url": "https://en.wikipedia.org//en.wikipedia.org/w/index.php?title=Artificial_intelligence&mobileaction=toggle_view_mobile",
      "text": "/en.wikipedia.org/w/index.php?mobileaction=toggle_view_mobile&title=Artificial_intelligence We could not find the above page on our servers. Did you mean: /wiki/en.wikipedia.org/w/index.php?mobileaction=toggle_view_mobile&title=Artificial_intelligence Alternatively, you can visit the Main Page or read more information about this type of error.",
      "timestamp": "2025-10-09 19:11:51.615028"
    },
    {
      "url": "https://en.wikipedia.org//en.wikipedia.org/w/index.php?title=Artificial_intelligence&mobileaction=toggle_view_mobile",
      "title": "Not Found",
      "description": "",
      "text": "/en.wikipedia.org/w/index.php?mobileaction=toggle_view_mobile&title=Artificial_intelligence We could not find the above page on our servers. Did you mean: /wiki/en.wikipedia.org/w/index.php?mobileaction=toggle_view_mobile&title=Artificial_intelligence Alternatively, you can visit the Main Page or read more information about this type of error.",
      "timestamp": "2025-10-09 19:11:51.615511"
    },
    {
      "url": "https://en.wikipedia.org//en.wikipedia.org/wiki/Wikipedia:Contact_us",
      "text": "/en.wikipedia.org/wiki/Wikipedia:Contact_us We could not find the above page on our servers. Did you mean: /wiki/en.wikipedia.org/wiki/Wikipedia:Contact_us Alternatively, you can visit the Main Page or read more information about this type of error.",
      "timestamp": "2025-10-09 19:11:52.205126"
    },
    {
      "url": "https://en.wikipedia.org//en.wikipedia.org/wiki/Wikipedia:Contact_us",
      "title": "Not Found",
      "description": "",
      "text": "/en.wikipedia.org/wiki/Wikipedia:Contact_us We could not find the above page on our servers. Did you mean: /wiki/en.wikipedia.org/wiki/Wikipedia:Contact_us Alternatively, you can visit the Main Page or read more information about this type of error.",
      "timestamp": "2025-10-09 19:11:52.205448"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer",
      "text": "\n Wikipedia makes no guarantee of validity\n Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.\n That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However, Wikipedia cannot guarantee the validity of the information found here. The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works also have disclaimers.\n Our active community of editors uses tools such as the Special:RecentChanges and Special:NewPages feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual peer review, they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or featured article processes may later have been edited inappropriately, just before you view them.\n None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages.\n Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in any way connected with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.\n There is no agreement or understanding between you and Wikipedia regarding your use or modification of this information beyond the Creative Commons Attribution-ShareAlike 4.0 Unported License (CC BY-SA) and the GNU Free Documentation License (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.\n Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.\n Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to personality rights, independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use. You are solely responsible for ensuring that you do not infringe someone else's personality rights.\n Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.\n If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.\n",
      "timestamp": "2025-10-09 19:11:52.531385"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer",
      "title": "Wikipedia:General disclaimer - Wikipedia",
      "description": "",
      "text": "\n Wikipedia makes no guarantee of validity\n Wikipedia is an online open-content collaborative encyclopedia; that is, a voluntary association of individuals and groups working to develop a common resource of human knowledge. The structure of the project allows anyone with an Internet connection to alter its content. Please be advised that nothing found here has necessarily been reviewed by people with the expertise required to provide you with complete, accurate, or reliable information.\n That is not to say that you will not find valuable and accurate information in Wikipedia; much of the time you will. However, Wikipedia cannot guarantee the validity of the information found here. The content of any given article may recently have been changed, vandalized, or altered by someone whose opinion does not correspond with the state of knowledge in the relevant fields. Note that most other encyclopedias and reference works also have disclaimers.\n Our active community of editors uses tools such as the Special:RecentChanges and Special:NewPages feeds to monitor new and changing content. However, Wikipedia is not uniformly peer reviewed; while readers may correct errors or engage in casual peer review, they have no legal duty to do so and thus all information read here is without any implied warranty of fitness for any purpose or use whatsoever. Even articles that have been vetted by informal peer review or featured article processes may later have been edited inappropriately, just before you view them.\n None of the contributors, sponsors, administrators, or anyone else connected with Wikipedia in any way whatsoever can be responsible for the appearance of any inaccurate or libelous information or for your use of the information contained in or linked from these web pages.\n Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and the owners or users of this site, the owners of the servers upon which it is housed, the individual Wikipedia contributors, any project administrators, sysops, or anyone else who is in any way connected with this project or sister projects subject to your claims against them directly. You are being granted a limited license to copy anything from this site; it does not create or imply any contractual or extracontractual liability on the part of Wikipedia or any of its agents, members, organizers, or other users.\n There is no agreement or understanding between you and Wikipedia regarding your use or modification of this information beyond the Creative Commons Attribution-ShareAlike 4.0 Unported License (CC BY-SA) and the GNU Free Documentation License (GFDL); neither is anyone at Wikipedia responsible should someone change, edit, modify, or remove any information that you may post on Wikipedia or any of its associated projects.\n Any of the trademarks, service marks, collective marks, design rights, or similar rights that are mentioned, used, or cited in the articles of the Wikipedia encyclopedia are the property of their respective owners. Their use here does not imply that you may use them for any purpose other than for the same or a similar informational use as contemplated by the original authors of these Wikipedia articles under the CC BY-SA and GFDL licensing schemes. Unless otherwise stated, Wikipedia and Wikimedia sites are neither endorsed by, nor affiliated with, any of the holders of any such rights, and as such, Wikipedia cannot grant any rights to use any otherwise protected materials. Your use of any such or similar incorporeal property is at your own risk.\n Wikipedia contains material which may portray an identifiable person who is alive or recently-deceased. The use of images of living or recently-deceased individuals is, in some jurisdictions, restricted by laws pertaining to personality rights, independent from their copyright status. Before using these types of content, please ensure that you have the right to use it under the laws which apply in the circumstances of your intended use. You are solely responsible for ensuring that you do not infringe someone else's personality rights.\n Publication of information found in Wikipedia may be in violation of the laws of the country or jurisdiction from where you are viewing this information. The Wikipedia database is stored on servers in the United States of America, and is maintained in reference to the protections afforded under local and federal law. Laws in your country or jurisdiction may not protect or allow the same kinds of speech or distribution. Wikipedia does not encourage the violation of any laws, and cannot be responsible for any violations of such laws, should you link to this domain, or use, reproduce, or republish the information contained herein.\n If you need specific advice (for example, medical, legal, financial, or risk management), please seek a professional who is licensed or knowledgeable in that area.\n",
      "timestamp": "2025-10-09 19:11:52.533489"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Wikipedia:About",
      "text": "\n \n \nWikipedia is a free online encyclopedia that anyone can edit, and millions already have.\n Wikipedia's purpose is to benefit readers by presenting information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, with articles that usually contain numerous links guiding readers to more information.\n Written collaboratively by volunteers known as Wikipedians, Wikipedia articles can be edited by anyone with Internet access, except in limited cases in which editing is restricted to prevent disruption or vandalism. Since its creation on January 15, 2001, it has grown into the world's largest reference website, attracting over a billion visitors each month. Wikipedia currently has more than sixty-five million articles in more than 300 languages, including 7,071,718 articles in English, with 113,534 active contributors in the past month.\n Wikipedia's fundamental principles are summarized in its five pillars. While the Wikipedia community has developed many policies and guidelines, new editors do not need to be familiar with them before they start contributing.\n Anyone can edit Wikipedia's text, data, references, and images. The quality of content is more important than the expertise of who contributes it. Wikipedia's content must conform with its policies, including being verifiable by published reliable sources. Contributions based on personal opinions, beliefs, or personal experiences, unreviewed research, libellous material, and copyright violations are not allowed, and will not remain. Wikipedia's software makes it easy to reverse errors, and experienced editors watch and patrol bad edits.\n Wikipedia differs from printed references in important ways. Anyone can instantly improve it, add quality information, remove misinformation, and fix errors and vandalism. Since Wikipedia is continually updated, encyclopedic articles on major news events appear within minutes.\n For over 24 years, editors have volunteered their time and talents to create history's most comprehensive encyclopedia while providing references and other resources to researchers worldwide (see Researching with Wikipedia). In summary, Wikipedia has tested the wisdom of the crowd since 2001 and has found that it succeeds.\n",
      "timestamp": "2025-10-09 19:11:52.860566"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Wikipedia:About",
      "title": "Wikipedia:About - Wikipedia",
      "description": "",
      "text": "\n \n \nWikipedia is a free online encyclopedia that anyone can edit, and millions already have.\n Wikipedia's purpose is to benefit readers by presenting information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, with articles that usually contain numerous links guiding readers to more information.\n Written collaboratively by volunteers known as Wikipedians, Wikipedia articles can be edited by anyone with Internet access, except in limited cases in which editing is restricted to prevent disruption or vandalism. Since its creation on January 15, 2001, it has grown into the world's largest reference website, attracting over a billion visitors each month. Wikipedia currently has more than sixty-five million articles in more than 300 languages, including 7,071,718 articles in English, with 113,534 active contributors in the past month.\n Wikipedia's fundamental principles are summarized in its five pillars. While the Wikipedia community has developed many policies and guidelines, new editors do not need to be familiar with them before they start contributing.\n Anyone can edit Wikipedia's text, data, references, and images. The quality of content is more important than the expertise of who contributes it. Wikipedia's content must conform with its policies, including being verifiable by published reliable sources. Contributions based on personal opinions, beliefs, or personal experiences, unreviewed research, libellous material, and copyright violations are not allowed, and will not remain. Wikipedia's software makes it easy to reverse errors, and experienced editors watch and patrol bad edits.\n Wikipedia differs from printed references in important ways. Anyone can instantly improve it, add quality information, remove misinformation, and fix errors and vandalism. Since Wikipedia is continually updated, encyclopedic articles on major news events appear within minutes.\n For over 24 years, editors have volunteered their time and talents to create history's most comprehensive encyclopedia while providing references and other resources to researchers worldwide (see Researching with Wikipedia). In summary, Wikipedia has tested the wisdom of the crowd since 2001 and has found that it succeeds.\n",
      "timestamp": "2025-10-09 19:11:52.862980"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License",
      "text": "You are free:\n for any purpose, even commercially.\n The licensor cannot revoke these freedoms as long as you follow the license terms.\n Under the following terms:\n Notices:\n You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n Your exercise of the Licensed Rights is expressly made subject to the following conditions.\n Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \"Licensor.\" The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark \"Creative Commons\" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\n",
      "timestamp": "2025-10-09 19:11:53.162980"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License",
      "title": "Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License - Wikipedia",
      "description": "",
      "text": "You are free:\n for any purpose, even commercially.\n The licensor cannot revoke these freedoms as long as you follow the license terms.\n Under the following terms:\n Notices:\n You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n Your exercise of the Licensed Rights is expressly made subject to the following conditions.\n Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \"Licensor.\" The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark \"Creative Commons\" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\n",
      "timestamp": "2025-10-09 19:11:53.164080"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Articles_with_Internet_Encyclopedia_of_Philosophy_links",
      "text": "Articles with external links including {{IEP}}, creating links to the Internet Encyclopedia of Philosophy. Note that some of these links are used as proper citations like the template implies but it is often used in external links sections. The template may be split in two for the distinct use cases.\n The following 200 pages are in this category, out of approximately 421 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:53.490918"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Articles_with_Internet_Encyclopedia_of_Philosophy_links",
      "title": "Category:Articles with Internet Encyclopedia of Philosophy links - Wikipedia",
      "description": "",
      "text": "Articles with external links including {{IEP}}, creating links to the Internet Encyclopedia of Philosophy. Note that some of these links are used as proper citations like the template implies but it is often used in external links sections. The template may be split in two for the distinct use cases.\n The following 200 pages are in this category, out of approximately 421 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:53.492476"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1_errors:_unrecognized_parameter",
      "text": "This is a tracking category for CS1 citations with unnamed parameters.\n Text \"????\" ignored\n Unlike many Wikipedia templates, the Citation Style 1 and Citation Style 2 templates do not use unnamed or positional parameters. When a citation contains text between vertical bars and that text does not contain an equal sign (=), CS1|2 ignores the text and reports the error. This is true even when the text is the name of a valid parameter.\n This error can also be caused by vertical bars (pipe characters) that are part of URLs or titles. When vertical bars occur in URLs, replace each vertical bar with %7C. When vertical bars occur in parameter values that are not URLs, replace each vertical bar with &#124; or {{!}}.\n To resolve this error, remove the extraneous text, add '=', add an appropriate parameter name from the template you're using to complete the parameter, or properly encode vertical bars in URLs and titles.\n Pages with this error are automatically placed in Category:CS1 errors: unrecognized parameter.[a]\n The following 149 pages are in this category, out of  149 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:54.307350"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1_errors:_unrecognized_parameter",
      "title": "Category:CS1 errors: unrecognized parameter - Wikipedia",
      "description": "",
      "text": "This is a tracking category for CS1 citations with unnamed parameters.\n Text \"????\" ignored\n Unlike many Wikipedia templates, the Citation Style 1 and Citation Style 2 templates do not use unnamed or positional parameters. When a citation contains text between vertical bars and that text does not contain an equal sign (=), CS1|2 ignores the text and reports the error. This is true even when the text is the name of a valid parameter.\n This error can also be caused by vertical bars (pipe characters) that are part of URLs or titles. When vertical bars occur in URLs, replace each vertical bar with %7C. When vertical bars occur in parameter values that are not URLs, replace each vertical bar with &#124; or {{!}}.\n To resolve this error, remove the extraneous text, add '=', add an appropriate parameter name from the template you're using to complete the parameter, or properly encode vertical bars in URLs and titles.\n Pages with this error are automatically placed in Category:CS1 errors: unrecognized parameter.[a]\n The following 149 pages are in this category, out of  149 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:54.308859"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Pages_using_Sister_project_links_with_hidden_wikidata",
      "text": "The following 200 pages are in this category, out of approximately 6,719 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:54.606590"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Pages_using_Sister_project_links_with_hidden_wikidata",
      "title": "Category:Pages using Sister project links with hidden wikidata - Wikipedia",
      "description": "",
      "text": "The following 200 pages are in this category, out of approximately 6,719 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:54.608020"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1:_long_volume_value",
      "text": "This is a tracking category for CS1 citations that use a value of |volume= longer than four characters and not wholly numeric or uppercase Roman numerals. \n No error or maintenance message is displayed. No changes are required to remove pages from this category, but some values of |volume= may fit better in other parameters.\n Pages in this category should only be added by Module:Citation/CS1. \n The following 200 pages are in this category, out of approximately 73,910 total. This list may not reflect recent changes.\n The following 8 files are in this category, out of 8 total.\n",
      "timestamp": "2025-10-09 19:11:55.301211"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1:_long_volume_value",
      "title": "Category:CS1: long volume value - Wikipedia",
      "description": "",
      "text": "This is a tracking category for CS1 citations that use a value of |volume= longer than four characters and not wholly numeric or uppercase Roman numerals. \n No error or maintenance message is displayed. No changes are required to remove pages from this category, but some values of |volume= may fit better in other parameters.\n Pages in this category should only be added by Module:Citation/CS1. \n The following 200 pages are in this category, out of approximately 73,910 total. This list may not reflect recent changes.\n The following 8 files are in this category, out of 8 total.\n",
      "timestamp": "2025-10-09 19:11:55.302819"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Pages_displaying_short_descriptions_of_redirect_targets_via_Module:Annotated_link",
      "text": "\n This category may be populated by pages displaying short descriptions, derived from the target of a redirected link, as part of link annotations transcluded by an invocation of Module:Annotated link directly or via any other module or template using it, currently including {{annotated link}}. Instances of this category being transcluded via the module may be indicated in situ by the application of some CSS via e.g. your common CSS:\n The following 200 pages are in this category, out of approximately 3,738 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:55.801876"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Pages_displaying_short_descriptions_of_redirect_targets_via_Module:Annotated_link",
      "title": "Category:Pages displaying short descriptions of redirect targets via Module:Annotated link - Wikipedia",
      "description": "",
      "text": "\n This category may be populated by pages displaying short descriptions, derived from the target of a redirected link, as part of link annotations transcluded by an invocation of Module:Annotated link directly or via any other module or template using it, currently including {{annotated link}}. Instances of this category being transcluded via the module may be indicated in situ by the application of some CSS via e.g. your common CSS:\n The following 200 pages are in this category, out of approximately 3,738 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:55.803720"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Articles_with_disputed_statements_from_July_2024",
      "text": "\nThis category combines all articles with disputed statements from July 2024 (2024-07) to enable us to work through the backlog more systematically. It is a member of Category:Articles with disputed statements. The following 82 pages are in this category, out of  82 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:56.295889"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Articles_with_disputed_statements_from_July_2024",
      "title": "Category:Articles with disputed statements from July 2024 - Wikipedia",
      "description": "",
      "text": "\nThis category combines all articles with disputed statements from July 2024 (2024-07) to enable us to work through the backlog more systematically. It is a member of Category:Articles with disputed statements. The following 82 pages are in this category, out of  82 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:56.298128"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:All_accuracy_disputes",
      "text": "\n This category contains all pages labeled with {{Self-published}}, {{Disputed}}, {{Dubious}}, and {{Disputed-inline}}, and exists primarily for bot-based monitoring of accuracy disputes. By-month categories are located in Category:Accuracy disputes.\n The following 200 pages are in this category, out of approximately 15,730 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:56.827539"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:All_accuracy_disputes",
      "title": "Category:All accuracy disputes - Wikipedia",
      "description": "",
      "text": "\n This category contains all pages labeled with {{Self-published}}, {{Disputed}}, {{Dubious}}, and {{Disputed-inline}}, and exists primarily for bot-based monitoring of accuracy disputes. By-month categories are located in Category:Accuracy disputes.\n The following 200 pages are in this category, out of approximately 15,730 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:56.829290"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Articles_with_excerpts",
      "text": "This category is used for tracking articles with excerpts. Add it to your watchlist to be notified when a new article includes an excerpt.\n This category has only the following subcategory.\n The following 200 pages are in this category, out of approximately 11,606 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:57.405417"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Articles_with_excerpts",
      "title": "Category:Articles with excerpts - Wikipedia",
      "description": "",
      "text": "This category is used for tracking articles with excerpts. Add it to your watchlist to be notified when a new article includes an excerpt.\n This category has only the following subcategory.\n The following 200 pages are in this category, out of approximately 11,606 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:57.406958"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Wikipedia_indefinitely_semi-protected_pages",
      "text": "This category contains pages which have been indefinitely semi-protected from editing. Semi-protection prevents anonymous and newly registered users from editing; contributing accounts must be at least 4 days old and must have made at least 10 edits to non-protected pages (including deleted ones).\n Use {{Pp-semi-indef}} to add pages to this category. This should only be done if the page is in fact semi-protected \u2013 adding the template does not in itself protect the page.\n This category has only the following subcategory.\n The following 200 pages are in this category, out of approximately 9,815 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:57.759992"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Wikipedia_indefinitely_semi-protected_pages",
      "title": "Category:Wikipedia indefinitely semi-protected pages - Wikipedia",
      "description": "",
      "text": "This category contains pages which have been indefinitely semi-protected from editing. Semi-protection prevents anonymous and newly registered users from editing; contributing accounts must be at least 4 days old and must have made at least 10 edits to non-protected pages (including deleted ones).\n Use {{Pp-semi-indef}} to add pages to this category. This should only be done if the page is in fact semi-protected \u2013 adding the template does not in itself protect the page.\n This category has only the following subcategory.\n The following 200 pages are in this category, out of approximately 9,815 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:57.761709"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Use_dmy_dates_from_July_2023",
      "text": "Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the first main contributor rule or by virtue of close national ties to the subject belong in this category. Use {{Use dmy dates}} to add an article to this category. See MOS:DATE.\n This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and not as a clean up.\n The following 200 pages are in this category, out of approximately 10,551 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:58.343508"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Use_dmy_dates_from_July_2023",
      "title": "Category:Use dmy dates from July 2023 - Wikipedia",
      "description": "",
      "text": "Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the first main contributor rule or by virtue of close national ties to the subject belong in this category. Use {{Use dmy dates}} to add an article to this category. See MOS:DATE.\n This system of tagging or categorisation is used as a status monitor of all articles that use dd mm yyyy date formats, and not as a clean up.\n The following 200 pages are in this category, out of approximately 10,551 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:58.345655"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata",
      "text": "This category contains articles with short descriptions that do not match the description field on Wikidata. \n No action is needed at this time. This is a tracking category only.\n The following 200 pages are in this category, out of approximately 4,383,485 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:58.674321"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata",
      "title": "Category:Short description is different from Wikidata - Wikipedia",
      "description": "",
      "text": "This category contains articles with short descriptions that do not match the description field on Wikidata. \n No action is needed at this time. This is a tracking category only.\n The following 200 pages are in this category, out of approximately 4,383,485 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:58.675923"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Articles_with_short_description",
      "text": "This category is for articles with short descriptions defined on Wikipedia by {{short description}} (either within the page itself or via another template).\n This category has the following 4 subcategories, out of 4 total.\n The following 200 pages are in this category, out of approximately 6,079,356 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:58.989559"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Articles_with_short_description",
      "title": "Category:Articles with short description - Wikipedia",
      "description": "",
      "text": "This category is for articles with short descriptions defined on Wikipedia by {{short description}} (either within the page itself or via another template).\n This category has the following 4 subcategories, out of 4 total.\n The following 200 pages are in this category, out of approximately 6,079,356 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:11:58.991679"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1_Japanese-language_sources_(ja)",
      "text": "This is a tracking category for CS1 citations that use the parameter |language=ja to identify a source in Japanese. Pages in this category should only be added by CS1 templates and Module:Citation/CS1.\n \n The following 200 pages are in this category, out of approximately 83,387 total. This list may not reflect recent changes.\n The following 21 files are in this category, out of 21 total.\n",
      "timestamp": "2025-10-09 19:11:59.545002"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1_Japanese-language_sources_(ja)",
      "title": "Category:CS1 Japanese-language sources (ja) - Wikipedia",
      "description": "",
      "text": "This is a tracking category for CS1 citations that use the parameter |language=ja to identify a source in Japanese. Pages in this category should only be added by CS1 templates and Module:Citation/CS1.\n \n The following 200 pages are in this category, out of approximately 83,387 total. This list may not reflect recent changes.\n The following 21 files are in this category, out of 21 total.\n",
      "timestamp": "2025-10-09 19:11:59.547105"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1_Russian-language_sources_(ru)",
      "text": "This is a tracking category for CS1 citations that use the parameter |language=ru to identify a source in Russian. Pages in this category should only be added by CS1 templates and Module:Citation/CS1.\n The following 200 pages are in this category, out of approximately 69,031 total. This list may not reflect recent changes.\n The following 17 files are in this category, out of 17 total.\n",
      "timestamp": "2025-10-09 19:12:00.157969"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1_Russian-language_sources_(ru)",
      "title": "Category:CS1 Russian-language sources (ru) - Wikipedia",
      "description": "",
      "text": "This is a tracking category for CS1 citations that use the parameter |language=ru to identify a source in Russian. Pages in this category should only be added by CS1 templates and Module:Citation/CS1.\n The following 200 pages are in this category, out of approximately 69,031 total. This list may not reflect recent changes.\n The following 17 files are in this category, out of 17 total.\n",
      "timestamp": "2025-10-09 19:12:00.159831"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1_German-language_sources_(de)",
      "text": "This is a tracking category for CS1 citations that use the parameter |language=de to identify a source in German. Pages in this category should only be added by CS1 templates and Module:Citation/CS1.\n The following 200 pages are in this category, out of approximately 164,059 total. This list may not reflect recent changes.\n The following 49 files are in this category, out of 49 total.\n",
      "timestamp": "2025-10-09 19:12:00.558628"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:CS1_German-language_sources_(de)",
      "title": "Category:CS1 German-language sources (de) - Wikipedia",
      "description": "",
      "text": "This is a tracking category for CS1 citations that use the parameter |language=de to identify a source in German. Pages in this category should only be added by CS1 templates and Module:Citation/CS1.\n The following 200 pages are in this category, out of approximately 164,059 total. This list may not reflect recent changes.\n The following 49 files are in this category, out of 49 total.\n",
      "timestamp": "2025-10-09 19:12:00.560491"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Webarchive_template_wayback_links",
      "text": "\n The following 200 pages are in this category, out of approximately 628,402 total. This list may not reflect recent changes.\n The following 182 files are in this category, out of 182 total.\n",
      "timestamp": "2025-10-09 19:12:00.941534"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Webarchive_template_wayback_links",
      "title": "Category:Webarchive template wayback links - Wikipedia",
      "description": "",
      "text": "\n The following 200 pages are in this category, out of approximately 628,402 total. This list may not reflect recent changes.\n The following 182 files are in this category, out of 182 total.\n",
      "timestamp": "2025-10-09 19:12:00.943747"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Intelligence_by_type",
      "text": "This category has the following 5 subcategories, out of 5 total.\n The following 18 pages are in this category, out of  18 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:01.418516"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Intelligence_by_type",
      "title": "Category:Intelligence by type - Wikipedia",
      "description": "",
      "text": "This category has the following 5 subcategories, out of 5 total.\n The following 18 pages are in this category, out of  18 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:01.419794"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Formal_sciences",
      "text": "This category has the following 4 subcategories, out of 4 total.\n The following 35 pages are in this category, out of  35 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:01.916074"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Formal_sciences",
      "title": "Category:Formal sciences - Wikipedia",
      "description": "",
      "text": "This category has the following 4 subcategories, out of 4 total.\n The following 35 pages are in this category, out of  35 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:01.917267"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Data_science",
      "text": "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy, structured and unstructured data, and apply knowledge from data across a broad range of application domains.\n This category has only the following subcategory.\n The following 24 pages are in this category, out of  24 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:02.374397"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Data_science",
      "title": "Category:Data science - Wikipedia",
      "description": "",
      "text": "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy, structured and unstructured data, and apply knowledge from data across a broad range of application domains.\n This category has only the following subcategory.\n The following 24 pages are in this category, out of  24 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:02.375484"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Cybernetics",
      "text": "Cybernetics is a transdisciplinary approach for exploring regulatory systems with feedback, their structures, constraints, and possibilities. Cybernetics is relevant to the study of systems, such as mechanical, physical, biological, cognitive, and social.\n This category has the following 4 subcategories, out of 4 total.\n The following 152 pages are in this category, out of  152 total. This list may not reflect recent changes.\n The following 3 files are in this category, out of 3 total.\n",
      "timestamp": "2025-10-09 19:12:02.690239"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Cybernetics",
      "title": "Category:Cybernetics - Wikipedia",
      "description": "",
      "text": "Cybernetics is a transdisciplinary approach for exploring regulatory systems with feedback, their structures, constraints, and possibilities. Cybernetics is relevant to the study of systems, such as mechanical, physical, biological, cognitive, and social.\n This category has the following 4 subcategories, out of 4 total.\n The following 152 pages are in this category, out of  152 total. This list may not reflect recent changes.\n The following 3 files are in this category, out of 3 total.\n",
      "timestamp": "2025-10-09 19:12:02.692012"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Computational_neuroscience",
      "text": "This category has the following 7 subcategories, out of 7 total.\n The following 126 pages are in this category, out of  126 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:03.032391"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Computational_neuroscience",
      "title": "Category:Computational neuroscience - Wikipedia",
      "description": "",
      "text": "This category has the following 7 subcategories, out of 7 total.\n The following 126 pages are in this category, out of  126 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:03.033856"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Computational_fields_of_study",
      "text": "Computational fields of study are areas of research in an existing field using the power of computation, and which are usually named for that. Computational fields of study as a group are sometimes also be referred to as Computational X\n This category has the following 22 subcategories, out of 22 total.\n The following 110 pages are in this category, out of  110 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:03.527508"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Computational_fields_of_study",
      "title": "Category:Computational fields of study - Wikipedia",
      "description": "",
      "text": "Computational fields of study are areas of research in an existing field using the power of computation, and which are usually named for that. Computational fields of study as a group are sometimes also be referred to as Computational X\n This category has the following 22 subcategories, out of 22 total.\n The following 110 pages are in this category, out of  110 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:03.529023"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Artificial_intelligence",
      "text": "This category has the following 36 subcategories, out of 36 total.\n The following 200 pages are in this category, out of  200 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:04.196803"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Category:Artificial_intelligence",
      "title": "Category:Artificial intelligence - Wikipedia",
      "description": "",
      "text": "This category has the following 36 subcategories, out of 36 total.\n The following 200 pages are in this category, out of  200 total. This list may not reflect recent changes.\n",
      "timestamp": "2025-10-09 19:12:04.199024"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Help:Category",
      "text": "\n Categories are intended to group together pages on similar subjects. They are implemented by a MediaWiki feature that adds any page with a text like [[Category:XYZ]] in its wiki markup to the automated listing that is the category with name XYZ. Categories help readers to find, and navigate around, a subject area, to see pages sorted by title, and to thus find article relationships. \n Categories are normally found at the bottom of an article page. Clicking a category name brings up a category page listing the articles (or other pages) that have been added to that particular category. There may also be a section listing the subcategories of that category. The subcategorization feature makes it possible to organize categories into tree-like structures to aid navigation.\n The term category does refer to both the title of a category page\u2014the category pagename\u2014and the category itself. Keeping this in mind while reading about categorization, plus learning a category page layout is a worthwhile investment in research techniques. (See also the search box parameter \"incategory\".) The layout of a category page is mostly text, but see about displaying category trees below.\n The MediaWiki software maintains tables of categories, to which any editable page can be added. To add a page to a category, include \"[[Category:Category name]]\" or \"[[Category:Category name|Sortkey]]\" in that page's wiki markup. The categories to which a page belongs appear in a box at the bottom of the page.\n A category is usually associated with a category page in the \"Category:\" namespace.[1] A category page contains text that can be edited, like any other page, but when the page is displayed, the last part of what is displayed is an automatically generated list of all pages in that category, in the form of links. Other category pages which appear in this list are treated separately, as subcategories.\n A category page is any page in the Category namespace.  They each act as a category, and are termed a \"category\".  The category page has one section titled Subcategories listing other \"categories\", and one section titled Pages, listing pages as categorized (in other namespaces). New categories are created by creating a page in the Category namespace.\n A category page can be edited like any other page. However, when it is displayed, the editable part of the page is followed by automatically generated lists of pages belonging to the category, as follows:\n The items in the lists all link to the pages concerned; in the case of the images this applies both to the image itself and to the text below it (the name of the image).\n For the way in which the lists are ordered, see Sorting category pages below. The first and second lists are divided into sections, according to the first character of the sort key. These initial characters are displayed above the sections. To suppress these, make all sort keys start with a space.\n A category page can only display a limited number of members (currently 200).  If there are more members, there will be a link to the next page.\n  The categories box for the category page appears at the bottom, in the same place as for other pages. This contains the categories to which the current category page has been added, i.e., its parent categories (the categories of which it is a subcategory). Add a category page to other categories in the normal way, using the \"[[Category:Category name]]\" or \"[[Category:Category name|Sortkey]]\" syntax.\n A page becomes part of a category if the page's wiki markup contains a declaration for that category. A category declaration takes the form [[Category:Category name]] or [[Category:Category name|Sortkey]]. The declaration must be processed, i.e. it will not work if it appears between <nowiki>...</nowiki> or <includeonly>...</includeonly> tags, or in a comment. The declaration may however come from a transcluded page; see Categories and templates below.\n A category name can be any string that would be a legitimate page title. If the category name begins with a lower-case letter, it will be capitalized. For initial lower-case letters, as in Category:macOS, see the technical restrictions page.\n On Wikipedia, it is customary to place category declarations at the end of the wiki markup, but before any stub templates (which themselves transclude categories).\n When a page has been added to one or more categories, a categories box appears at the bottom of the page (or possibly elsewhere, if a non-default skin is being used). This box contains a list of the categories the page belongs to, in the order in which the category declarations appear in the processed wiki markup. The category names are linked to the corresponding category pages. They appear as red links if the corresponding category page does not exist. If a user has enabled the HotCat gadget, the categories box will also provide links to quickly add, remove, or modify category declarations on the page, without having to edit the whole page.\n Hidden categories are not displayed, except as described below under Hiding categories.\n The following subsections are ordered from simple actions to more elaborate or rarer actions.\n To link to a category page without putting the current page in that category, precede the link with a colon: [[:Category:Category name]]. Such a link can be piped like a normal wikilink. (The {{cl}} template, and others listed on its documentation page, may sometimes be helpful.)\n Raw information about the members of a category, their sort keys and timestamps (time when last added to the category) can be obtained from the API, using a query of the form:\n Listings of up to 500 members are possible. If there are more members then the results will include text near the end like this: <categorymembers cmcontinue=\"page|NNNN|TITLE\" />.\n This can be added to the previous one, without quotation marks, for the next page of members:  ...&cmcontinue=page|NNNN|TITLE\n By default, a page is sorted under the first character of its name, without the namespace. English Wikipedia groups accented characters together with their unaccented version, so pages starting with \u00c0, \u00c1, \u00c4, will be listed under heading A. Sorting is case-insensitive, so \"ABC\" comes after \"Abacus\".\n Unlike at Special:Allpages and Special:Prefixindex, a space is treated as a space (coming before all other characters), not as an underscore.\n The English Wikipedia has numerical sorting in categories. This means that digit sequences in page names are treated according to their numerical value, not as strings. Thus \"9 dogs\", \"25 dogs\", and \"112 dogs\" will all appear under the \"0\u20139\" heading in numerical order, and V838 Monocerotis will appear before V1309 Scorpii.\n Each of the three lists (subcategories, pages, media files) is arranged in the order explained above (except that, in the subcategories list, the namespace indicator \"Category:\" is not considered). If an item ought to be positioned within a list on the basis of an alternative name (sort key) for that item, then this can be specified in the category tag that places the item in the list:\n For example, to add an article called Albert Einstein to Category:1879 births and have the article sorted by \"Einstein, Albert\", you would type:\n Unlike a piped link (which uses the same syntax), the sort key itself is not displayed to readers. It affects only the order in which pages are listed on the category page.\n It is useful to document the system being used for sort keys on the category page. For guidelines about the use of sort keys on Wikipedia, see WP:SORTKEY.\n It is possible to set a default sort key which is different from {{PAGENAME}} by using the magic word {{DEFAULTSORT:}}:\n This is often used in biography articles, to make sure the subject is sorted by their last name:\n For example, on the Albert Einstein page, {{DEFAULTSORT:Einstein, Albert}} adds the sort key \"Einstein, Albert\" to all his categories, such as Category:1879 births.\n In the case of multiple default sort key tags, the last DEFAULTSORT on the final rendering of a page applies for all categories, regardless of the position of the category tags. This also means that a DEFAULTSORT tag included from a template is not effective if another DEFAULTSORT tag occurs later on the page, even if the later DEFAULTSORT tag is also \"hidden\" (included by another template). Any conflicts are tracked via Category:Pages with DEFAULTSORT conflicts.\n In addition to browsing through hierarchies of categories, it is possible to use the search tool to find specific articles in specific categories. To search for articles in a specific category, type incategory:\"CategoryName\" in the search box.\n A pipe \"|\" can be added to join the contents of one category with the contents of another. For example, enter\n to return all pages that belong to either (or both) of the categories, as here.\n Note that using search to find categories will not find articles which have been categorized using templates. This feature also doesn't return pages in subcategories.\n Special:Categories provides an alphabetic list of all categories, with the number of members of each; this number does not include the content of the subcategories, but it includes the subcategories themselves, i.e., each counting as one.\n The above list contains all categories that have members, regardless of whether they have corresponding category pages. To list all existing category pages (regardless of whether they have members), use Special:AllPages/Category:.\n As described at mw:Help:Magic words, {{PAGESINCATEGORY:Example}} or {{PAGESINCAT:Example}} returns the number of pages in \"Category:Example\". Each subcategory counts as one page; pages in subcategories are not counted.\n The page Special:CategoryTree enables you to see the tree structure of a category (its subcategories, their subcategories and so on; the display of files and other member pages is optional).\n The CategoryTree extension can be used to display such a tree on any page. (This is sometimes done on the category page itself, if the category is split over multiple screens, to make all subcategories available on every screen.) The basic syntax is\n to display just the subcategory tree, and\n to display member pages as well. They will be indicated by italics.\n Dapete's category-visualizer vCat will render charts of the tree structure.\n You may also use Template:Category tree or Template:Category tree all, instead.\n Warning:\n \n Categories can be moved in the same way as an ordinary page; but a certain amount of cleanup may be necessary. A redirect is left at the old category name, and this is not a normal #REDIRECT\u00a0[[...]] but a {{category redirect}}. Once all the pages have been moved out of the old category, it may be left as a category redirect or deleted. For categories entirely populated through templates (see above), modifying the templates enables all affected articles to be moved to another category, but with the refresh problem mentioned. Almost all category name changes are made pursuant to a consensus decision at Wikipedia:Categories for discussion.\n Do not create intercategory redirects other than with a {{category redirect}} template. See Wikipedia:Categories for discussion \u00a7\u00a0Redirecting categories for more on category redirects.\n When the magic word __HIDDENCAT__ is placed on a category page, that category becomes hidden, meaning that it will not be displayed on the pages belonging to that category. On Wikipedia, the magic word is not normally used explicitly, but is applied through the {{hidden category}} template. The feature is mostly used to prevent project maintenance categories from showing up to ordinary readers on article pages.\n For users who are not logged in, hidden categories are displayed on category pages (whether as parent categories or subcategories).\n Hidden categories are displayed at the bottom of each page, after \"Hidden categories:\", for registered users:\n Hidden categories are automatically added to Category:Hidden categories.\n For guidelines on the hiding of categories on Wikipedia, see WP:HIDDENCAT.\n \n Tracking categories are used to track technical featues or problems. They are outside of the category structure for articles. Pages may be inserted into those by the MediaWiki software, or by templates or modules. Usually they are hidden, or even red ie not created. Special:TrackingCategories lists all tracking categories populated by the MediaWiki software. Category:Tracking categories is the top for tracking categories on this wiki.\n The most effective way of finding entries of a category is using the \"What links here\" tool on the category's main article.\n An easy way to find relevant articles for a new category or missing entries in an existing one is by finding the most relevant list and checking its entries. Sometimes categories are about things that are intersections of other categories for which the PetScan tool can be used.\n More relevant articles may also be found linked in a category's main article and the articles already featured in the category \u2212 especially in their \"See also\" sections (if existent) and the automatically suggested \"RELATED ARTICLES\" below them.\n Furthermore, a category's superordinate categories often feature articles that should be subcategorized to the category.\n Other ways to find relevant articles include searching Wikipedia for the category's topic and searching the Web for the topic in quotes \" (with synonyms also in quotes and appended after an OR) and appending the word wiki or Wikipedia or site:Wikipedia.org to them.\n Templates are categorized the same way as articles, except that [[Category: Some-topic templates]] should be placed on the template's documentation page (or inside <noinclude>...</noinclude> tags, if there is no documentation page), this is necessary to avoid categorizing pages by template inclusion (see below).\n A template can be used to add pages to a category, usually by placing the category link inside <includeonly></includeonly> tags on the template (e.g. <includeonly>[[Category:category name]]</includeonly>).  When the template is transcluded into the page, the category link becomes active, and the page is added to the category page.  This is useful for categories that have high turnover or many pages included, like cleanup categories.\n Changes to the template, however, may not be reflected immediately on the category page.  When you edit an article to add a category tag directly, the list of category members is updated immediately when the page is saved.  When a category link is contained in a template, however, this does not happen immediately: instead, whenever a template is edited, all the pages that transclude it are put into the job queue to be recached during periods of low server load. This means that, in busy periods, it may take hours or even days before individual pages are recached and they start to appear in the category list. Performing a  null edit to a page will allow it to jump the queue and be immediately recached.\n To add the template itself to the category page as well, omit the \"includeonly\" tags. To add the template to a category without categorizing pages on which the template is transcluded, place the category declaration between <noinclude>...</noinclude> tags, or add it to the template documentation page between <includeonly></includeonly> (the latter allows recategorizing the template without editing it, which is helpful if it is protected, or so complicated that mere mortals hesitate to touch it).\n Parser functions can be used to make the transcluded categories, or the sort key used in them, dependent on other variables, notably PAGENAME.\n On Wikipedia it is not recommended that templates be used to populate ordinary content categories of articles. See Categorization using templates in the categorization guideline.\n Redirect pages can be categorized and there are conventions on how to do it. The redirect link must be first on the page. On a category page, redirects are listed in italics.\n For a category, the \"Related Changes\" feature, when applied to the corresponding category page, lists recent changes to the pages which are currently listed as belonging to a category. Where those pages are subcategories or image pages, only changes to their editable parts are listed.\n Notice that \"Related Changes\" does not show edits to pages that have been removed from the category.\n Also, \"Related Changes\" does not list recent changes to pages linked from the editable part of the category page (as it would normally, with a non-category page). If a workaround would be required, the links in question could be placed in a template and transcluded onto the category page.\n As usual \u2013 unlike with watchlists \u2013 recent changes to corresponding talk pages are not shown under \"Related Changes\". Pages that one is watching are bolded on the list. This can help to find which pages in a given category one has on one's watchlist.\n The DynamicPageList (third-party) extension provides a list of last edits to the pages in a category, or optionally, just the list of pages; the simpler DynamicPageList (Wikimedia) is installed on Meta, Wikinews, Wikibooks and Wikiversity; the extension mw:Extension:DPLforum is installed on Wikia.\n Since 2016, additions and removals from categories are available via the \"Category changes\" filter on recent changes pages, including watchlists and Special:RecentChangesLinked. For example, category changes to articles in Category:Cannabis stubs can be found here. You can monitor additions and removals from specific categories by adding the categories to your watchlist and making sure the \"Category changes\" filter is active. You can view changes to categories in your watchlist by clicking here. Additional scripts with similar functionality are User:CategoryWatchlistBot and User:Ais523/catwatch.\n",
      "timestamp": "2025-10-09 19:12:04.548777"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Help:Category",
      "title": "Help:Category - Wikipedia",
      "description": "",
      "text": "\n Categories are intended to group together pages on similar subjects. They are implemented by a MediaWiki feature that adds any page with a text like [[Category:XYZ]] in its wiki markup to the automated listing that is the category with name XYZ. Categories help readers to find, and navigate around, a subject area, to see pages sorted by title, and to thus find article relationships. \n Categories are normally found at the bottom of an article page. Clicking a category name brings up a category page listing the articles (or other pages) that have been added to that particular category. There may also be a section listing the subcategories of that category. The subcategorization feature makes it possible to organize categories into tree-like structures to aid navigation.\n The term category does refer to both the title of a category page\u2014the category pagename\u2014and the category itself. Keeping this in mind while reading about categorization, plus learning a category page layout is a worthwhile investment in research techniques. (See also the search box parameter \"incategory\".) The layout of a category page is mostly text, but see about displaying category trees below.\n The MediaWiki software maintains tables of categories, to which any editable page can be added. To add a page to a category, include \"[[Category:Category name]]\" or \"[[Category:Category name|Sortkey]]\" in that page's wiki markup. The categories to which a page belongs appear in a box at the bottom of the page.\n A category is usually associated with a category page in the \"Category:\" namespace.[1] A category page contains text that can be edited, like any other page, but when the page is displayed, the last part of what is displayed is an automatically generated list of all pages in that category, in the form of links. Other category pages which appear in this list are treated separately, as subcategories.\n A category page is any page in the Category namespace.  They each act as a category, and are termed a \"category\".  The category page has one section titled Subcategories listing other \"categories\", and one section titled Pages, listing pages as categorized (in other namespaces). New categories are created by creating a page in the Category namespace.\n A category page can be edited like any other page. However, when it is displayed, the editable part of the page is followed by automatically generated lists of pages belonging to the category, as follows:\n The items in the lists all link to the pages concerned; in the case of the images this applies both to the image itself and to the text below it (the name of the image).\n For the way in which the lists are ordered, see Sorting category pages below. The first and second lists are divided into sections, according to the first character of the sort key. These initial characters are displayed above the sections. To suppress these, make all sort keys start with a space.\n A category page can only display a limited number of members (currently 200).  If there are more members, there will be a link to the next page.\n  The categories box for the category page appears at the bottom, in the same place as for other pages. This contains the categories to which the current category page has been added, i.e., its parent categories (the categories of which it is a subcategory). Add a category page to other categories in the normal way, using the \"[[Category:Category name]]\" or \"[[Category:Category name|Sortkey]]\" syntax.\n A page becomes part of a category if the page's wiki markup contains a declaration for that category. A category declaration takes the form [[Category:Category name]] or [[Category:Category name|Sortkey]]. The declaration must be processed, i.e. it will not work if it appears between <nowiki>...</nowiki> or <includeonly>...</includeonly> tags, or in a comment. The declaration may however come from a transcluded page; see Categories and templates below.\n A category name can be any string that would be a legitimate page title. If the category name begins with a lower-case letter, it will be capitalized. For initial lower-case letters, as in Category:macOS, see the technical restrictions page.\n On Wikipedia, it is customary to place category declarations at the end of the wiki markup, but before any stub templates (which themselves transclude categories).\n When a page has been added to one or more categories, a categories box appears at the bottom of the page (or possibly elsewhere, if a non-default skin is being used). This box contains a list of the categories the page belongs to, in the order in which the category declarations appear in the processed wiki markup. The category names are linked to the corresponding category pages. They appear as red links if the corresponding category page does not exist. If a user has enabled the HotCat gadget, the categories box will also provide links to quickly add, remove, or modify category declarations on the page, without having to edit the whole page.\n Hidden categories are not displayed, except as described below under Hiding categories.\n The following subsections are ordered from simple actions to more elaborate or rarer actions.\n To link to a category page without putting the current page in that category, precede the link with a colon: [[:Category:Category name]]. Such a link can be piped like a normal wikilink. (The {{cl}} template, and others listed on its documentation page, may sometimes be helpful.)\n Raw information about the members of a category, their sort keys and timestamps (time when last added to the category) can be obtained from the API, using a query of the form:\n Listings of up to 500 members are possible. If there are more members then the results will include text near the end like this: <categorymembers cmcontinue=\"page|NNNN|TITLE\" />.\n This can be added to the previous one, without quotation marks, for the next page of members:  ...&cmcontinue=page|NNNN|TITLE\n By default, a page is sorted under the first character of its name, without the namespace. English Wikipedia groups accented characters together with their unaccented version, so pages starting with \u00c0, \u00c1, \u00c4, will be listed under heading A. Sorting is case-insensitive, so \"ABC\" comes after \"Abacus\".\n Unlike at Special:Allpages and Special:Prefixindex, a space is treated as a space (coming before all other characters), not as an underscore.\n The English Wikipedia has numerical sorting in categories. This means that digit sequences in page names are treated according to their numerical value, not as strings. Thus \"9 dogs\", \"25 dogs\", and \"112 dogs\" will all appear under the \"0\u20139\" heading in numerical order, and V838 Monocerotis will appear before V1309 Scorpii.\n Each of the three lists (subcategories, pages, media files) is arranged in the order explained above (except that, in the subcategories list, the namespace indicator \"Category:\" is not considered). If an item ought to be positioned within a list on the basis of an alternative name (sort key) for that item, then this can be specified in the category tag that places the item in the list:\n For example, to add an article called Albert Einstein to Category:1879 births and have the article sorted by \"Einstein, Albert\", you would type:\n Unlike a piped link (which uses the same syntax), the sort key itself is not displayed to readers. It affects only the order in which pages are listed on the category page.\n It is useful to document the system being used for sort keys on the category page. For guidelines about the use of sort keys on Wikipedia, see WP:SORTKEY.\n It is possible to set a default sort key which is different from {{PAGENAME}} by using the magic word {{DEFAULTSORT:}}:\n This is often used in biography articles, to make sure the subject is sorted by their last name:\n For example, on the Albert Einstein page, {{DEFAULTSORT:Einstein, Albert}} adds the sort key \"Einstein, Albert\" to all his categories, such as Category:1879 births.\n In the case of multiple default sort key tags, the last DEFAULTSORT on the final rendering of a page applies for all categories, regardless of the position of the category tags. This also means that a DEFAULTSORT tag included from a template is not effective if another DEFAULTSORT tag occurs later on the page, even if the later DEFAULTSORT tag is also \"hidden\" (included by another template). Any conflicts are tracked via Category:Pages with DEFAULTSORT conflicts.\n In addition to browsing through hierarchies of categories, it is possible to use the search tool to find specific articles in specific categories. To search for articles in a specific category, type incategory:\"CategoryName\" in the search box.\n A pipe \"|\" can be added to join the contents of one category with the contents of another. For example, enter\n to return all pages that belong to either (or both) of the categories, as here.\n Note that using search to find categories will not find articles which have been categorized using templates. This feature also doesn't return pages in subcategories.\n Special:Categories provides an alphabetic list of all categories, with the number of members of each; this number does not include the content of the subcategories, but it includes the subcategories themselves, i.e., each counting as one.\n The above list contains all categories that have members, regardless of whether they have corresponding category pages. To list all existing category pages (regardless of whether they have members), use Special:AllPages/Category:.\n As described at mw:Help:Magic words, {{PAGESINCATEGORY:Example}} or {{PAGESINCAT:Example}} returns the number of pages in \"Category:Example\". Each subcategory counts as one page; pages in subcategories are not counted.\n The page Special:CategoryTree enables you to see the tree structure of a category (its subcategories, their subcategories and so on; the display of files and other member pages is optional).\n The CategoryTree extension can be used to display such a tree on any page. (This is sometimes done on the category page itself, if the category is split over multiple screens, to make all subcategories available on every screen.) The basic syntax is\n to display just the subcategory tree, and\n to display member pages as well. They will be indicated by italics.\n Dapete's category-visualizer vCat will render charts of the tree structure.\n You may also use Template:Category tree or Template:Category tree all, instead.\n Warning:\n \n Categories can be moved in the same way as an ordinary page; but a certain amount of cleanup may be necessary. A redirect is left at the old category name, and this is not a normal #REDIRECT\u00a0[[...]] but a {{category redirect}}. Once all the pages have been moved out of the old category, it may be left as a category redirect or deleted. For categories entirely populated through templates (see above), modifying the templates enables all affected articles to be moved to another category, but with the refresh problem mentioned. Almost all category name changes are made pursuant to a consensus decision at Wikipedia:Categories for discussion.\n Do not create intercategory redirects other than with a {{category redirect}} template. See Wikipedia:Categories for discussion \u00a7\u00a0Redirecting categories for more on category redirects.\n When the magic word __HIDDENCAT__ is placed on a category page, that category becomes hidden, meaning that it will not be displayed on the pages belonging to that category. On Wikipedia, the magic word is not normally used explicitly, but is applied through the {{hidden category}} template. The feature is mostly used to prevent project maintenance categories from showing up to ordinary readers on article pages.\n For users who are not logged in, hidden categories are displayed on category pages (whether as parent categories or subcategories).\n Hidden categories are displayed at the bottom of each page, after \"Hidden categories:\", for registered users:\n Hidden categories are automatically added to Category:Hidden categories.\n For guidelines on the hiding of categories on Wikipedia, see WP:HIDDENCAT.\n \n Tracking categories are used to track technical featues or problems. They are outside of the category structure for articles. Pages may be inserted into those by the MediaWiki software, or by templates or modules. Usually they are hidden, or even red ie not created. Special:TrackingCategories lists all tracking categories populated by the MediaWiki software. Category:Tracking categories is the top for tracking categories on this wiki.\n The most effective way of finding entries of a category is using the \"What links here\" tool on the category's main article.\n An easy way to find relevant articles for a new category or missing entries in an existing one is by finding the most relevant list and checking its entries. Sometimes categories are about things that are intersections of other categories for which the PetScan tool can be used.\n More relevant articles may also be found linked in a category's main article and the articles already featured in the category \u2212 especially in their \"See also\" sections (if existent) and the automatically suggested \"RELATED ARTICLES\" below them.\n Furthermore, a category's superordinate categories often feature articles that should be subcategorized to the category.\n Other ways to find relevant articles include searching Wikipedia for the category's topic and searching the Web for the topic in quotes \" (with synonyms also in quotes and appended after an OR) and appending the word wiki or Wikipedia or site:Wikipedia.org to them.\n Templates are categorized the same way as articles, except that [[Category: Some-topic templates]] should be placed on the template's documentation page (or inside <noinclude>...</noinclude> tags, if there is no documentation page), this is necessary to avoid categorizing pages by template inclusion (see below).\n A template can be used to add pages to a category, usually by placing the category link inside <includeonly></includeonly> tags on the template (e.g. <includeonly>[[Category:category name]]</includeonly>).  When the template is transcluded into the page, the category link becomes active, and the page is added to the category page.  This is useful for categories that have high turnover or many pages included, like cleanup categories.\n Changes to the template, however, may not be reflected immediately on the category page.  When you edit an article to add a category tag directly, the list of category members is updated immediately when the page is saved.  When a category link is contained in a template, however, this does not happen immediately: instead, whenever a template is edited, all the pages that transclude it are put into the job queue to be recached during periods of low server load. This means that, in busy periods, it may take hours or even days before individual pages are recached and they start to appear in the category list. Performing a  null edit to a page will allow it to jump the queue and be immediately recached.\n To add the template itself to the category page as well, omit the \"includeonly\" tags. To add the template to a category without categorizing pages on which the template is transcluded, place the category declaration between <noinclude>...</noinclude> tags, or add it to the template documentation page between <includeonly></includeonly> (the latter allows recategorizing the template without editing it, which is helpful if it is protected, or so complicated that mere mortals hesitate to touch it).\n Parser functions can be used to make the transcluded categories, or the sort key used in them, dependent on other variables, notably PAGENAME.\n On Wikipedia it is not recommended that templates be used to populate ordinary content categories of articles. See Categorization using templates in the categorization guideline.\n Redirect pages can be categorized and there are conventions on how to do it. The redirect link must be first on the page. On a category page, redirects are listed in italics.\n For a category, the \"Related Changes\" feature, when applied to the corresponding category page, lists recent changes to the pages which are currently listed as belonging to a category. Where those pages are subcategories or image pages, only changes to their editable parts are listed.\n Notice that \"Related Changes\" does not show edits to pages that have been removed from the category.\n Also, \"Related Changes\" does not list recent changes to pages linked from the editable part of the category page (as it would normally, with a non-category page). If a workaround would be required, the links in question could be placed in a template and transcluded onto the category page.\n As usual \u2013 unlike with watchlists \u2013 recent changes to corresponding talk pages are not shown under \"Related Changes\". Pages that one is watching are bolded on the list. This can help to find which pages in a given category one has on one's watchlist.\n The DynamicPageList (third-party) extension provides a list of last edits to the pages in a category, or optionally, just the list of pages; the simpler DynamicPageList (Wikimedia) is installed on Meta, Wikinews, Wikibooks and Wikiversity; the extension mw:Extension:DPLforum is installed on Wikia.\n Since 2016, additions and removals from categories are available via the \"Category changes\" filter on recent changes pages, including watchlists and Special:RecentChangesLinked. For example, category changes to articles in Category:Cannabis stubs can be found here. You can monitor additions and removals from specific categories by adding the categories to your watchlist and making sure the \"Category changes\" filter is active. You can view changes to categories in your watchlist by clicking here. Additional scripts with similar functionality are User:CategoryWatchlistBot and User:Ais523/catwatch.\n",
      "timestamp": "2025-10-09 19:12:04.552177"
    },
    {
      "url": "https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=1315925624",
      "text": "\n Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]\n High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3]\n Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI)\u2014AI that can complete virtually any cognitive task at least as well as a human.\n Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]\n Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]\n Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\n Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22]\n A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.\n Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]\n An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences\u2014there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]\n In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\n In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\n A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]\n Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]\n Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]\n There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]\n In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]\n Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]\n Natural language processing (NLP) allows programs to read, write and communicate in human languages.[50] Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]\n Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\n Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]\n Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]\n The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61] object tracking,[62] and robotic perception.[63]\n Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction.\n However, this tends to give na\u00efve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]\n A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68]\n AI research uses a wide variety of techniques to accomplish the goals above.[b]\n AI can solve many problems by intelligently searching through many possible solutions.[69] There are two very different kinds of search used in AI: state space search and local search.\n State space search searches through a tree of possible states to try to find a goal state.[70] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[71]\n Simple exhaustive searches[72] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[73]\n Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[74]\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[75]\n Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[76] through the backpropagation algorithm.\n Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[77]\n Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[78]\n Formal logic is used for reasoning and knowledge representation.[79]\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[80] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[81]\n Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[82] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\n Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[83] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[84]\n Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[85]\n Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[86]\n Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.\n Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[87] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[88] and information value theory.[89] These tools include models such as Markov decision processes,[90] dynamic decision networks,[91] game theory and mechanism design.[92]\n Bayesian networks[93] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][95] learning (using the expectation\u2013maximization algorithm),[h][97] planning (using decision networks)[98] and perception (using dynamic Bayesian networks).[91]\n Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[91]\n The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[99] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]\n There are many kinds of classifiers in use.[100] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102]\nThe naive Bayes classifier is reportedly the \"most widely used learner\"[103] at Google, due in part to its scalability.[104]\nNeural networks are also used as classifiers.[105]\n An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105]\n Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107]\n In feedforward neural networks the signal passes in only one direction.[108] The term perceptron typically refers to a single-layer neural network.[109] In contrast, deep learning uses many layers.[110] Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem.[111] Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.[112]\n Deep learning uses several layers of neurons between the network's inputs and outputs.[110] The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[114]\n Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[115] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[116] The sudden success of deep learning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]\n Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.[124] Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.[125][126]\n Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI.[127] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[128]\n In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[129] Specialized programming languages such as Prolog were used in early AI research,[130] but general-purpose programming languages like Python have become predominant.[131]\n The transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster,[132] a trend sometimes called Huang's law,[133] named after Nvidia co-founder and CEO Jensen Huang.\n AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\n The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[134] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[135][136]\n For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[137] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[137][138] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[139] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[140] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[141][142]\n Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[143] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[144] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[145] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[146] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[147] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[148] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[149] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[150] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[151]\n Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[152] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[153] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[154] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[155] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[156] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[157]\n Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve[158] all from Google DeepMind,[159] Llemma from EleutherAI[160] or Julius.[161]\n When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.[162]\n Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[163]\n Topological deep learning integrates various topological approaches.\n Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[164]\n According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[165]\n Various countries are deploying AI military applications.[166] The main applications enhance command and control, communications, sensors, integration and interoperability.[167] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[166] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.[167]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[166][168][169][170]\n Generative artificial intelligence (Generative AI, GenAI,[171] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data.[172][173][174] These models learn the underlying patterns and structures of their training data and use them to produce new data[175][176] based on the input, which often comes in the form of natural language prompts.[177][178]\n Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora.[179][180][181][182][183] Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.[177][184][185]\n AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[196][197][198]\n Microsoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries[199] and step-by-step reasoning based of information from web publishers, ranked in Bing Search.[200] \nFor safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.[201]\n Google officially pushed its AI Search at its Google I/O event on May 20, 2025.[202] It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.[203]\n Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions,[204] AI-integrated sex toys (e.g., teledildonics),[205] AI-generated sexual education content,[206] and AI agents that simulate sexual and romantic partners (e.g., Replika).[207]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[208]\n AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[209][210]\n There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[211] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\n AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.[212][213][214]\n In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\n During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[215]\n AI has potential benefits and potential risks.[218] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[219] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[220][221] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[222]\n Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\n AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\n Sensitive user data collected may include online activity records, geolocation data, video, or audio.[223] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[224] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[225]\n AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[226] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[227]\n Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[228][229] Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[230] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[231][232] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[233]\n The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[234][235][236] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[237][238]\n In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[239] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[240]\n Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[241]\n A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[242] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[243]\n In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million.[244] Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.[245]\n In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[246] The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.[247]\n After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[248] Taiwan aims to phase out nuclear power by 2025.[248] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[248]\n Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI.[249] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[249]\n On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[250] \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[250]\n In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300\u2013500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.[251]\n YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[252] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[253] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.[254]\n In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing,[255] while realistic AI-generated videos became feasible in the mid-2020s.[256][257][258] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda;[259] one such potential malicious use is deepfakes for computational propaganda.[260] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[261]\n AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.[262]\n Machine learning applications will be biased[k] if they learn from biased data.[264] The developers may not be aware that the bias exists.[265] Bias can be introduced by the way training data is selected and by the way a model is deployed.[266][264] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[267] The field of fairness studies how to prevent harms from algorithmic biases.\n On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[268] a problem called \"sample size disparity\".[269] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[270]\n COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[271] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[273]\n A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[274] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[275]\n Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[276] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]\n Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[269]\n There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[263]\n At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious \u2013 discuss][278]\n Many AI systems are so complex that their designers cannot explain how they reach their decisions.[279] Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[280]\n It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[281] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[282]\n People who have been harmed by an algorithm's decision have a right to an explanation.[283] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[284]\n DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[285]\n Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[286] LIME can locally approximate a model's outputs with a simpler, interpretable model.[287] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[288] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[289] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[290]\n Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\n A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[292] Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.[292] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[293] By 2015, over fifty countries were reported to be researching battlefield robots.[294]\n AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[295] All these technologies have been available since 2020 or earlier\u2014AI facial recognition systems are already being used for mass surveillance in China.[296][297]\n There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[298]\n Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[299]\n In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[300] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[301] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][303] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[299] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[304][305]\n Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[306] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[307]\n From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[308]\n It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[309] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways.\n First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer).[311] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[312] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[313]\n Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[314]\n The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[315] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[316] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\n In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\".[317] He notably mentioned risks of an AI takeover,[318] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[319]\n In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[320]\n Some other researchers were more optimistic. AI pioneer J\u00fcrgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[321] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[322][323] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested interests.\"[324] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[325] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[326] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[327]\n Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[328]\n Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[329]\nThe field of machine ethics is also called computational morality,[329]\nand was founded at an AAAI symposium in 2005.[330]\n Other approaches include Wendell Wallach's \"artificial moral agents\"[331] and Stuart J. Russell's three principles for developing provably beneficial machines.[332]\n Active organizations in the AI open-source community include Hugging Face,[333] Google,[334] EleutherAI and Meta.[335] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[336][337] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[338] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[339]\n Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:[340][341]\n Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[342] however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.[343]\n Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[344]\n The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[345]\n The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[346] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[347] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[348][349] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[350] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[350] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[350] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[351] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[352] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics.[353] On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation.[354] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[355]\n In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[348] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[356] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[357][358]\n In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[359] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[360][361] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[362][363]\n The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[365][366] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[368] such as McCulloch and Pitts design for \"artificial neurons\" in 1943,[117] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[369][366]\n The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[366]\n Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[373] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[374] In 1967 Marvin Minsky agreed, writing that \"within a generation\u00a0... the problem of creating 'artificial intelligence' will substantially be solved\".[375] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[377] and ongoing pressure from the U.S. Congress to fund more productive projects.[378] Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[379] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9]\n In the early 1980s, AI research was revived by the commercial success of expert systems,[380] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]\n Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[381] and began to look into \"sub-symbolic\" approaches.[382] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[87][387] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[388] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[389]\n AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[390] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[391]\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[68]\n Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y]\nDeep learning's success was based on both hardware improvements (faster computers,[393] graphics processing units, cloud computing[394]) and access to large amounts of data[395] (including curated datasets,[394] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019.[350]\n In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[327]\n In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[396] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[397] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[398] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[399] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[400] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[401]\n Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[402] Another major focus has been whether machines can be conscious, and the associated ethical implications.[403] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[404] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[403]\n Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[405] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[405] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[369] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[406]\n Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[408] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[409]\n McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[410] Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".[411] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine\u2014and no other philosophical discussion is required, or may not even be possible.\n Another definition has been adopted by Google,[412] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself[413] including discussing the many AI narratives and myths to be found within societal, political and academic discourses.[414] Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms,[415] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[416]\n There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.[417]\n No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n Symbolic AI (or \"GOFAI\")[419] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[420]\n However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[421] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[422] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]\n The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[424][425] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[426] but eventually was seen as irrelevant. Modern AI has elements of both.\n Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[427][428] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[429] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[430] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[431]\n Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[432]\n Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[436]\n It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[437] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[438][439] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[438] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[440]\n In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[441] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.[442][443]\n Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[439][438]\n A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[428] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[444]\n However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[445]\n Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[446]\n Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[447]\n Thought-capable artificial beings have appeared as storytelling devices since antiquity,[448] and have been a persistent theme in science fiction.[449]\n A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[450]\n Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[451] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[452]\n Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[453]\n The two most widely used textbooks in 2023 (see the Open Syllabus):\n The four most widely used AI textbooks in 2008:\n Other textbooks:\n",
      "timestamp": "2025-10-09 19:12:05.669861"
    },
    {
      "url": "https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=1315925624",
      "title": "Artificial intelligence - Wikipedia",
      "description": "",
      "text": "\n Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]\n High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3]\n Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI)\u2014AI that can complete virtually any cognitive task at least as well as a human.\n Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]\n Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]\n Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\n Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22]\n A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.\n Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]\n An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences\u2014there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]\n In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\n In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\n A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]\n Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]\n Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]\n There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]\n In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]\n Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]\n Natural language processing (NLP) allows programs to read, write and communicate in human languages.[50] Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]\n Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\n Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]\n Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]\n The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61] object tracking,[62] and robotic perception.[63]\n Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction.\n However, this tends to give na\u00efve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]\n A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68]\n AI research uses a wide variety of techniques to accomplish the goals above.[b]\n AI can solve many problems by intelligently searching through many possible solutions.[69] There are two very different kinds of search used in AI: state space search and local search.\n State space search searches through a tree of possible states to try to find a goal state.[70] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[71]\n Simple exhaustive searches[72] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[73]\n Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[74]\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[75]\n Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[76] through the backpropagation algorithm.\n Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[77]\n Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[78]\n Formal logic is used for reasoning and knowledge representation.[79]\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[80] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[81]\n Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[82] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\n Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[83] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[84]\n Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[85]\n Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[86]\n Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.\n Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[87] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[88] and information value theory.[89] These tools include models such as Markov decision processes,[90] dynamic decision networks,[91] game theory and mechanism design.[92]\n Bayesian networks[93] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][95] learning (using the expectation\u2013maximization algorithm),[h][97] planning (using decision networks)[98] and perception (using dynamic Bayesian networks).[91]\n Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[91]\n The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[99] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]\n There are many kinds of classifiers in use.[100] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102]\nThe naive Bayes classifier is reportedly the \"most widely used learner\"[103] at Google, due in part to its scalability.[104]\nNeural networks are also used as classifiers.[105]\n An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105]\n Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107]\n In feedforward neural networks the signal passes in only one direction.[108] The term perceptron typically refers to a single-layer neural network.[109] In contrast, deep learning uses many layers.[110] Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem.[111] Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.[112]\n Deep learning uses several layers of neurons between the network's inputs and outputs.[110] The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[114]\n Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[115] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[116] The sudden success of deep learning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]\n Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.[124] Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.[125][126]\n Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI.[127] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[128]\n In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[129] Specialized programming languages such as Prolog were used in early AI research,[130] but general-purpose programming languages like Python have become predominant.[131]\n The transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster,[132] a trend sometimes called Huang's law,[133] named after Nvidia co-founder and CEO Jensen Huang.\n AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\n The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[134] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[135][136]\n For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[137] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[137][138] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[139] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[140] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[141][142]\n Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[143] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[144] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[145] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[146] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[147] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[148] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[149] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[150] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[151]\n Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[152] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[153] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[154] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[155] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[156] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[157]\n Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve[158] all from Google DeepMind,[159] Llemma from EleutherAI[160] or Julius.[161]\n When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.[162]\n Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[163]\n Topological deep learning integrates various topological approaches.\n Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[164]\n According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[165]\n Various countries are deploying AI military applications.[166] The main applications enhance command and control, communications, sensors, integration and interoperability.[167] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[166] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.[167]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[166][168][169][170]\n Generative artificial intelligence (Generative AI, GenAI,[171] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data.[172][173][174] These models learn the underlying patterns and structures of their training data and use them to produce new data[175][176] based on the input, which often comes in the form of natural language prompts.[177][178]\n Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora.[179][180][181][182][183] Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.[177][184][185]\n AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[196][197][198]\n Microsoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries[199] and step-by-step reasoning based of information from web publishers, ranked in Bing Search.[200] \nFor safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.[201]\n Google officially pushed its AI Search at its Google I/O event on May 20, 2025.[202] It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.[203]\n Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions,[204] AI-integrated sex toys (e.g., teledildonics),[205] AI-generated sexual education content,[206] and AI agents that simulate sexual and romantic partners (e.g., Replika).[207]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[208]\n AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[209][210]\n There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[211] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\n AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.[212][213][214]\n In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\n During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[215]\n AI has potential benefits and potential risks.[218] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[219] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[220][221] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[222]\n Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\n AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\n Sensitive user data collected may include online activity records, geolocation data, video, or audio.[223] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[224] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[225]\n AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[226] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[227]\n Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[228][229] Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[230] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[231][232] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[233]\n The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[234][235][236] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[237][238]\n In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[239] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[240]\n Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[241]\n A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[242] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[243]\n In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million.[244] Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.[245]\n In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[246] The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.[247]\n After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[248] Taiwan aims to phase out nuclear power by 2025.[248] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[248]\n Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI.[249] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[249]\n On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[250] \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[250]\n In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300\u2013500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.[251]\n YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[252] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[253] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.[254]\n In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing,[255] while realistic AI-generated videos became feasible in the mid-2020s.[256][257][258] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda;[259] one such potential malicious use is deepfakes for computational propaganda.[260] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[261]\n AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.[262]\n Machine learning applications will be biased[k] if they learn from biased data.[264] The developers may not be aware that the bias exists.[265] Bias can be introduced by the way training data is selected and by the way a model is deployed.[266][264] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[267] The field of fairness studies how to prevent harms from algorithmic biases.\n On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[268] a problem called \"sample size disparity\".[269] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[270]\n COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[271] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[273]\n A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[274] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[275]\n Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[276] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]\n Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[269]\n There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[263]\n At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious \u2013 discuss][278]\n Many AI systems are so complex that their designers cannot explain how they reach their decisions.[279] Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[280]\n It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[281] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[282]\n People who have been harmed by an algorithm's decision have a right to an explanation.[283] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[284]\n DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[285]\n Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[286] LIME can locally approximate a model's outputs with a simpler, interpretable model.[287] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[288] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[289] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[290]\n Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\n A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[292] Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.[292] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[293] By 2015, over fifty countries were reported to be researching battlefield robots.[294]\n AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[295] All these technologies have been available since 2020 or earlier\u2014AI facial recognition systems are already being used for mass surveillance in China.[296][297]\n There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[298]\n Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[299]\n In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[300] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[301] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][303] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[299] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[304][305]\n Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[306] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[307]\n From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[308]\n It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[309] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways.\n First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer).[311] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[312] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[313]\n Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[314]\n The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[315] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[316] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\n In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\".[317] He notably mentioned risks of an AI takeover,[318] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[319]\n In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[320]\n Some other researchers were more optimistic. AI pioneer J\u00fcrgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[321] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[322][323] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested interests.\"[324] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[325] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[326] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[327]\n Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[328]\n Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[329]\nThe field of machine ethics is also called computational morality,[329]\nand was founded at an AAAI symposium in 2005.[330]\n Other approaches include Wendell Wallach's \"artificial moral agents\"[331] and Stuart J. Russell's three principles for developing provably beneficial machines.[332]\n Active organizations in the AI open-source community include Hugging Face,[333] Google,[334] EleutherAI and Meta.[335] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[336][337] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[338] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[339]\n Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:[340][341]\n Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[342] however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.[343]\n Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[344]\n The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[345]\n The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[346] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[347] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[348][349] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[350] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[350] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[350] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[351] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[352] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics.[353] On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation.[354] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[355]\n In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[348] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[356] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[357][358]\n In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[359] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[360][361] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[362][363]\n The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[365][366] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[368] such as McCulloch and Pitts design for \"artificial neurons\" in 1943,[117] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[369][366]\n The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[366]\n Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[373] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[374] In 1967 Marvin Minsky agreed, writing that \"within a generation\u00a0... the problem of creating 'artificial intelligence' will substantially be solved\".[375] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[377] and ongoing pressure from the U.S. Congress to fund more productive projects.[378] Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[379] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9]\n In the early 1980s, AI research was revived by the commercial success of expert systems,[380] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]\n Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[381] and began to look into \"sub-symbolic\" approaches.[382] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[87][387] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[388] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[389]\n AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[390] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[391]\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[68]\n Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y]\nDeep learning's success was based on both hardware improvements (faster computers,[393] graphics processing units, cloud computing[394]) and access to large amounts of data[395] (including curated datasets,[394] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019.[350]\n In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[327]\n In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[396] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[397] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[398] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[399] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[400] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[401]\n Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[402] Another major focus has been whether machines can be conscious, and the associated ethical implications.[403] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[404] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[403]\n Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[405] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[405] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[369] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[406]\n Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[408] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[409]\n McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[410] Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".[411] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine\u2014and no other philosophical discussion is required, or may not even be possible.\n Another definition has been adopted by Google,[412] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself[413] including discussing the many AI narratives and myths to be found within societal, political and academic discourses.[414] Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms,[415] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[416]\n There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.[417]\n No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n Symbolic AI (or \"GOFAI\")[419] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[420]\n However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[421] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[422] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]\n The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[424][425] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[426] but eventually was seen as irrelevant. Modern AI has elements of both.\n Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[427][428] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[429] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[430] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[431]\n Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[432]\n Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[436]\n It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[437] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[438][439] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[438] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[440]\n In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[441] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.[442][443]\n Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[439][438]\n A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[428] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[444]\n However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[445]\n Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[446]\n Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[447]\n Thought-capable artificial beings have appeared as storytelling devices since antiquity,[448] and have been a persistent theme in science fiction.[449]\n A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[450]\n Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[451] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[452]\n Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[453]\n The two most widely used textbooks in 2023 (see the Open Syllabus):\n The four most widely used AI textbooks in 2008:\n Other textbooks:\n",
      "timestamp": "2025-10-09 19:12:05.692703"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Help:Authority_control",
      "text": "\n Authority control is a way of associating a unique identifier to articles on Wikipedia. This is useful to disambiguate different items with similar or identical headings, as well as establish a single standard title for an item that is commonly known by two or more titles. When used, authority control data links can be found near the bottom of Wikipedia pages, linking to bibliographical records on worldwide library catalogs. Authority control is often used in biographical articles because it is quite common for more than one person to share the same name. It is commonly used in other subject material as well.\n Authority control enables researchers to search more easily for pertinent information on the subject of an article, without needing to disambiguate the subject manually. For example, authority control is used on music articles so that the information in the article can be easily cross-referenced with popular databases.\n More generally, authority control is a method of creating and maintaining index terms for bibliographical material in a library catalog, similar to the Dewey Decimal System. The links produced by the authority control template on Wikipedia go to authority control data in worldwide library catalogs. As an example, the Wikipedia authority control information for Alexander Graham Bell looks like this:\n The abbreviations in the box represent the following: International Standard Name Identifier (ISNI); Virtual International Authority File (VIAF); Library of Congress Control Number (LCCN); and Integrated Authority File (GND), Gemeinsame Normdatei in German. WorldCat is a global cooperative union catalog which itemizes the collections of 72,000 libraries in 170 countries and territories.\n The following authority files are supported on the English Wikipedia:\n",
      "timestamp": "2025-10-09 19:12:06.376026"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Help:Authority_control",
      "title": "Help:Authority control - Wikipedia",
      "description": "",
      "text": "\n Authority control is a way of associating a unique identifier to articles on Wikipedia. This is useful to disambiguate different items with similar or identical headings, as well as establish a single standard title for an item that is commonly known by two or more titles. When used, authority control data links can be found near the bottom of Wikipedia pages, linking to bibliographical records on worldwide library catalogs. Authority control is often used in biographical articles because it is quite common for more than one person to share the same name. It is commonly used in other subject material as well.\n Authority control enables researchers to search more easily for pertinent information on the subject of an article, without needing to disambiguate the subject manually. For example, authority control is used on music articles so that the information in the article can be easily cross-referenced with popular databases.\n More generally, authority control is a method of creating and maintaining index terms for bibliographical material in a library catalog, similar to the Dewey Decimal System. The links produced by the authority control template on Wikipedia go to authority control data in worldwide library catalogs. As an example, the Wikipedia authority control information for Alexander Graham Bell looks like this:\n The abbreviations in the box represent the following: International Standard Name Identifier (ISNI); Virtual International Authority File (VIAF); Library of Congress Control Number (LCCN); and Integrated Authority File (GND), Gemeinsame Normdatei in German. WorldCat is a global cooperative union catalog which itemizes the collections of 72,000 libraries in 170 countries and territories.\n The following authority files are supported on the English Wikipedia:\n",
      "timestamp": "2025-10-09 19:12:06.379375"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_virology",
      "text": "\nThis glossary of virology is a list of definitions of terms and concepts used in virology, the study of viruses, particularly in the description of viruses and their actions. Related fields include microbiology, molecular biology, and genetics.\n Often simply called an antiviral. Also simply called a phage. Also cytopathogenic effect. Also called a gag. Also sometimes called a mycophage. Also called antigenic imprinting and the Hoskins effect. Also called passaging. Also called viral burden and viral titre. Also called a viral particle.",
      "timestamp": "2025-10-09 19:12:06.741870"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_virology",
      "title": "Glossary of virology - Wikipedia",
      "description": "",
      "text": "\nThis glossary of virology is a list of definitions of terms and concepts used in virology, the study of viruses, particularly in the description of viruses and their actions. Related fields include microbiology, molecular biology, and genetics.\n Often simply called an antiviral. Also simply called a phage. Also cytopathogenic effect. Also called a gag. Also sometimes called a mycophage. Also called antigenic imprinting and the Hoskins effect. Also called passaging. Also called viral burden and viral titre. Also called a viral particle.",
      "timestamp": "2025-10-09 19:12:06.744111"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_structural_engineering",
      "text": "\n This glossary of structural engineering terms pertains specifically to structural engineering and its sub-disciplines. Please see Glossary of engineering for a broad overview of the major concepts of engineering.\n Most of the terms listed in glossaries are already defined and explained within itself.  However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together.  You can help enhance this page by adding new terms or writing definitions for existing ones.\n [27]\n[28]\n[29]\n",
      "timestamp": "2025-10-09 19:12:07.137515"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_structural_engineering",
      "title": "Glossary of structural engineering - Wikipedia",
      "description": "",
      "text": "\n This glossary of structural engineering terms pertains specifically to structural engineering and its sub-disciplines. Please see Glossary of engineering for a broad overview of the major concepts of engineering.\n Most of the terms listed in glossaries are already defined and explained within itself.  However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together.  You can help enhance this page by adding new terms or writing definitions for existing ones.\n [27]\n[28]\n[29]\n",
      "timestamp": "2025-10-09 19:12:07.142311"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_scientific_naming",
      "text": "\n This is a list of terms and symbols used in scientific names for organisms, and in describing the names.  For proper parts of the names themselves, see List of Latin and Greek words commonly used in systematic names. Many of the abbreviations are used with or without a stop.\n The main ranks are kingdom (regnum), phylum or division (divisio), class (classis), order (ordo), family (familia), genus and species.  The ranks of section and series are also used in botany for groups within genera, while section is used in zoology for a division of an order. Further levels in the hierarchy can be made by the addition of prefixes such as sub-, super-, infra-, and so on.\n Divisions such as \"morph\", \"form\", \"variety\", \"strain\", \"breed\", \"cultivar\", hybrid (nothospecies) and \"landrace\" are used to describe various sub-specific groups in different fields.\n It is possible for a clade to be unranked, for example Psoroptidia (Yunker, 1955) and the SAR supergroup.  Sometimes a rank is described as clade where the traditional hierarchy cannot accommodate it.\n In zoology the English descriptions, such as \"conserved name\", for example, are acceptable and generally used.  These descriptions can be classified between accepted names (nom. cons., nom. nov., nom. prot.) and unaccepted combinations for different reasons (nom. err., nom. illeg., nom. nud., nom. rej., nom. supp., nom. van.), with some cases in between regarding the use (nom. dub.: used but not fully accepted; nom. obl.: accepted but not fully used, so it yields precedence to a nom. prot).\n These abbreviations indicate taxonomic novelty\u2014newly identified or newly classified taxa.\n \n",
      "timestamp": "2025-10-09 19:12:07.502762"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_scientific_naming",
      "title": "Glossary of scientific naming - Wikipedia",
      "description": "",
      "text": "\n This is a list of terms and symbols used in scientific names for organisms, and in describing the names.  For proper parts of the names themselves, see List of Latin and Greek words commonly used in systematic names. Many of the abbreviations are used with or without a stop.\n The main ranks are kingdom (regnum), phylum or division (divisio), class (classis), order (ordo), family (familia), genus and species.  The ranks of section and series are also used in botany for groups within genera, while section is used in zoology for a division of an order. Further levels in the hierarchy can be made by the addition of prefixes such as sub-, super-, infra-, and so on.\n Divisions such as \"morph\", \"form\", \"variety\", \"strain\", \"breed\", \"cultivar\", hybrid (nothospecies) and \"landrace\" are used to describe various sub-specific groups in different fields.\n It is possible for a clade to be unranked, for example Psoroptidia (Yunker, 1955) and the SAR supergroup.  Sometimes a rank is described as clade where the traditional hierarchy cannot accommodate it.\n In zoology the English descriptions, such as \"conserved name\", for example, are acceptable and generally used.  These descriptions can be classified between accepted names (nom. cons., nom. nov., nom. prot.) and unaccepted combinations for different reasons (nom. err., nom. illeg., nom. nud., nom. rej., nom. supp., nom. van.), with some cases in between regarding the use (nom. dub.: used but not fully accepted; nom. obl.: accepted but not fully used, so it yields precedence to a nom. prot).\n These abbreviations indicate taxonomic novelty\u2014newly identified or newly classified taxa.\n \n",
      "timestamp": "2025-10-09 19:12:07.504722"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_robotics",
      "text": "\n Robotics is the branch of technology that deals with the design, construction, operation, structural disposition, manufacture and application of robots.[1] Robotics is related to the sciences of electronics,  engineering, mechanics, and software.[2]\n The following is a list of common definitions related to the Robotics field.\n \n \n Online Robotics glossary repositories:\n \u00a0This article incorporates public domain material from OSHA Technical Manual - SECTION IV: CHAPTER 4 - INDUSTRIAL ROBOTS AND ROBOT SYSTEM SAFETY. Occupational Safety and Health Administration. Retrieved 2011-01-28.\n",
      "timestamp": "2025-10-09 19:12:07.817928"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_robotics",
      "title": "Glossary of robotics - Wikipedia",
      "description": "",
      "text": "\n Robotics is the branch of technology that deals with the design, construction, operation, structural disposition, manufacture and application of robots.[1] Robotics is related to the sciences of electronics,  engineering, mechanics, and software.[2]\n The following is a list of common definitions related to the Robotics field.\n \n \n Online Robotics glossary repositories:\n \u00a0This article incorporates public domain material from OSHA Technical Manual - SECTION IV: CHAPTER 4 - INDUSTRIAL ROBOTS AND ROBOT SYSTEM SAFETY. Occupational Safety and Health Administration. Retrieved 2011-01-28.\n",
      "timestamp": "2025-10-09 19:12:07.820404"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_quantum_computing",
      "text": "\nThis glossary of quantum computing is a list of definitions of terms and concepts used in quantum computing, its sub-disciplines, and related fields.\n",
      "timestamp": "2025-10-09 19:12:08.240911"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_quantum_computing",
      "title": "Glossary of quantum computing - Wikipedia",
      "description": "",
      "text": "\nThis glossary of quantum computing is a list of definitions of terms and concepts used in quantum computing, its sub-disciplines, and related fields.\n",
      "timestamp": "2025-10-09 19:12:08.246794"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_psychiatry",
      "text": "\n This glossary covers terms found in the psychiatric literature; the word origins are primarily Greek, but there are also Latin, French, German, and English terms. Many of these terms refer to expressions dating from the early days of psychiatry in Europe; some are deprecated, and thus are of historic interest.  \n Abreaction is a process of vividly reliving repressed memories and emotions related to a past event.[1] Sigmund Freud used hypnosis to rid his patients of pathological memories through abreaction.[1]\n Aboulia or Abulia, in neurology, refers to a lack of will or initiative. The individual is unable to act or make decisions independently. The condition may range from subtle to overwhelming in severity.\n Achromatopsia is a term referring to or acquired agnosia for color. This term includes color blindness.\nAchromatopsia is a condition characterized by a partial or total absence of color vision. People with complete achromatopsia cannot perceive any colors; they see only black, white, and shades of gray. Incomplete achromatopsia is a milder form of the condition that allows some color discrimination.\n Achromatopsia also involves other problems with vision, including an increased sensitivity to light and glare (photophobia), involuntary back-and-forth eye movements (nystagmus), and significantly reduced sharpness of vision (low visual acuity). Affected individuals can also have farsightedness (hyperopia) or, less commonly, nearsightedness (myopia). These vision problems develop in the first few months of life.\n Achromatopsia is different from the more common forms of color vision deficiency (also called color blindness), in which people can perceive color but have difficulty distinguishing between certain colors, such as red and green.[2]\n Mild illusions or misperceptions associated with changes in mood; e.g. mistaking a shadow for the presence of a person, perceiving movement in peripheral when there is none.\n Akataphasia (Kraepelin 1896) refers to a syntactic disturbance of speech resulting from dissolution of logical ordering of thoughts. It manifests as rambling speech. Compare Derailment.[3]\n Akathisia refers to a subjective feeling of restlessness in the lower limbs that is related to abnormal activity in the extrapyramidal system in the brain, often due to antipsychotic medication.[1] It tends to manifest as an inability to sit still.[1]\n Alexithymia refers to an inability to identify and describe emotions in the self.[4]\n In Alice in Wonderland experience, individuals perceive objects (including animals and other humans, or parts of humans, animals, or objects) as appearing substantially smaller than in reality. Generally, the object appears far away or extremely close at the same time. An alternate term for this is somaesthetic aura. Also see \u00a7\u00a0Lilliputian hallucinations\n Literally, this term means \"not having words\". The term may refer to either \"poverty of speech\" or \"poverty of thought\". In the former, speech, though adequate in verbiage, conveys very little information and may consist of stock phrases or vague references. In poverty of thought, by contrast, there is a far-reaching impoverishment of the entire thinking of the individual, who, as a result, says very little. It is typically a negative symptom of schizophrenia,[1] although it may also be seen in advanced dementia.\n Amenomania (compound of Latin amoenus, \"cheerful\"; and Greek \u03bc\u03b1\u03bd\u03af\u03b1, \"madness\")[5] is a disused psychiatric diagnosis.\n The phrase \"running amok\" describes the behavior of an individual who is very agitated and may be at danger of causing harm to themselves or others.[6][7] The syndrome of \"Amok\" is found in the DSM-IV TR.[8]\n Anhedonia refers to an inability to experience pleasure, and may be described as a feeling of emotional emptiness.[1] It can be a negative symptom of schizophrenia.[1] It also may be seen in severe depressive states and schizoid personality disorder.\n Anosognosia is a condition in which a person who has a certain disability seems unaware of the existence of their disability. \u00a7\u00a0hemiasomatognosia is a subtype of anosognosia in which the person with hemiplegia neglects one half of their body.\n Anton syndrome, occasionally known as Anton-Babinski syndrome, is a form of cortical blindness in which the individual denies the visual impairment. The individual may attempt to walk, bumping into objects and injuring himself.\nAnton syndrome is caused by damaging the occipital lobes bilaterally or from disrupting the pathway from the primary visual cortex into the visual association cortex.\n Anwesenheit refers to the false perception of an unfamiliar presence. It is commonly associated with periods of grief, schizophrenia and other emotional disturbances.\n This is an alternate term for delusional perception. It is one of the Schneiderian first rank symptoms and is defined as a true perception, to which an individual attributes a false meaning.\n Aphemia is the alternate term for mutism. Mutism is absence of speech with apparently normal level of consciousness. Mutism can be dissociative (hysterical) in which an individual (commonly a child or adolescent) stops speaking at once without involvement of any neurological or physical contributing factor; or it can be elective (selective) in which a child does not speak at all in certain situations (such as in school) but speaks well in other conditions (like at home or at play). A rare cause of mutism is akinetic mutism which results due to a lesion around the third ventricle of the brain.\n Apperception is a normal phenomenon and refers to the ability to understand sensory inputs in their context, to interpret them and to incorporate them into experience. Failure of apperception is seen in delirious states.\n Astasia-abasia is a form of psychogenic gait disturbance in which gait becomes impaired in the absence of any neurological or physical pathology. The person usually walks in a bizarre manner. They stagger and appear as if going to fall, but always manage to catch hold of something in time. Sometimes these people cannot even stand, but on the other hand they are well able to move their legs while lying down or sitting. Often associated with conversion disorder or somatization disorder.\n Asyndesis means loosening of association. A milder form of derailment of thought, it is marked by the individual leaping from topic to topic which have only the most tenuous, if any, connection with each other. This is in contrast with flight of ideas, whereby the individual's successive ideas may be linked and \"understandable\" to the listener.\nSee also \u00a7\u00a0akataphasia and \u00a7\u00a0entgleisen term introduced by (Cameron).\n From aut = \"self\" and -ism = \"state or orientation\". Originally, Eugen Bleuler used this term to describe schizophrenia. In general, it refers to any (pathological) tendency to be self-absorbed to such a degree that the feelings, thoughts and desires of a person are governed by their internal apprehension of the world and not by an external reality shared with others.\n Today the term is used most often to refer to a specific developmental syndrome (see autism spectrum).[9]:\u200ap. 76\u200a\n autistic thinking is an outdated term for egocentric thought processes that have little or no relation to consensus reality. The term does not accurately describe the thinking styles of autistic people.[10]\n Jaspers defined this as a delusion arising without apparent cause. For example, suddenly, without apparent cause, having the delusional belief that one is an alien.\n Autokabalesis is a term for committing suicide by jumping from a very high place.[11]\n Automatic obedience is an exaggerated co-operation with an examiner's request, as if the individual were an \"automaton\" robotically obeying a command. It is often a sign of catatonia.\n Automatisms are sequences of activity that occur without conscious control. They may be simple and repetitive (tic-like) or complex, and are usually natural-looking but purposeless. Automatic behavior is not usually recalled afterwards.\n Autoscopy is the reduplicative hallucination of \"seeing one's own body from the outside\" while still maintaining an egocentric visuo-spatial perspective. Autoscopy is sometimes used synonymously with out-of-body experience.\n Avolition is an inability to initiate and complete goal-directed behavior.[1] It can sometimes be misinterpreted as laziness, but it is actually a negative symptom of schizophrenia.\n Belle indifference or la belle indiff\u00e9rence is characterized by a lack of concern and feeling of indifference about a disability or symptom.[1] It can be seen in conversion disorder.\n Bouff\u00e9e d\u00e9lirante is a French term used in the past for acute and transient psychotic disorders (F23 in ICD-10). In DSM-IV, it is described as \"brief psychotic disorder\" (298.8). The symptoms usually have an acute onset and reach their peak within two weeks. The symptoms start resolving in a few weeks and complete recovery usually occurs within two or three months.[12]\n Brain fag syndrome[13] is an example of a culture-bound syndrome. \"Brain fag\" was once a common term for mental exhaustion. Today, the syndrome describes students (predominantly males, particularly in West Africa) experiencing symptoms including somatic, sleep-related and cognitive complaints, head and neck pains, difficulty in concentrating and retaining information, and eye pain.\n Synonym of \u00a7\u00a0clouding of consciousness.\n Bruxism refers to teeth grinding behavior that is usually seen in children.\n In Capgras syndrome, the individual feels that a person familiar to them, usually a family member, has been replaced by an imposter.[1] This is a type of delusion that can be experienced as part of schizophrenia. Capgras syndrome and several other related disorders are referred to as \"delusional misidentification syndrome\".\n Catalepsy is the term for catatonic rigidity of the limbs which often results in abnormal posturing for long intervals.\n Cataplexy refers to a sudden loss of muscle tone and is commonly precipitated by a strong emotional response.[1]\n Catatonia involves a significant psychomotor disturbance, which can occur as catalepsy, stupor, excessive purposeless motor activity, extreme negativism (seemingly motiveless resistance to movement), mutism, echolalia (imitating speech), or echopraxia (imitating movements).[1] There is a catatonic subtype of schizophrenia.[1]\n Cerea flexibilitas, meaning \"waxy flexibility\", refers to people allowing themselves to be placed in postures by others, and then maintaining those postures for long periods even if they are obviously uncomfortable.[1] It is characterized by an individual's movements having the feeling of a plastic resistance, as if the person were made of wax. This occurs in catatonic schizophrenia, and a person with this condition can have their limbs placed in fixed positions as if the person were in fact made from wax.\n Chorea refers to erratic involuntary movements. The term comes from the Greek word \"choreia\" or \"dance\" since usually large groups of muscles are involved simulating dance-like movements.\n Circumstantial thinking, or circumstantial speech, refers to a person being unable to answer a question without giving excessive, unnecessary detail.[14] This differs from tangential thinking, in that the person does eventually return to the original point, circling back on-topic.\n Clang associations are ideas that are related only by similar or rhyming sounds rather than actual meaning.[14]\n Claparede's paradox refers to retention of non-verbal and implicit memory in people with Korsakoff's syndrome.[15]\n Clouding of consciousness, also known as brain fog or mental fog, is a global impairment in higher central nervous functioning. All aspects of cognitive functioning are affected. On mental status examinations it is manifest by disorientation in time, place and person, memory difficulties caused by failure to register and recall, aphasia, and agnosia. Impaired perception functioning leads to illusions and hallucinations often in the visual sensory modality. This then causes agitation and distress and secondary delusions. The term confusion state is sometimes used to mean clouding of consciousness, but is avoided whenever possible because it is ambiguous.\n Coenestopathic state refers to a situation in which an individual in a coenestopathic state has a localized distortion of body awareness.\n Confabulation is the confusion of imagination with memory, or the confusion of true memories with false memories.\n Conversion disorder involves the unintentional production of symptoms or deficits affecting motor or sensory function that are not fully explained by a neurological or medical condition.[1] This can manifest as paralysis, for example. It generally involves psychological factors, and symptoms may worsen in the context of situational conflict.[1]\n Coprolalia is the involuntary utterance of socially inappropriate phrases. It is a phonic tic associated with Tourette syndrome, although less than 15% of persons with Tourette's have coprolalia.\n Cotard delusion involves the belief in an individual that one or more of their organs has changed in some way, has ceased functioning, or has disappeared entirely.[1]\nThis type of delusion is most commonly seen in patients with schizophrenia.[1]\n Defenestration refers to an individual voluntarily ejecting themselves from a window or another elevated position, usually in the context of attempted suicide. Also see \u00a7\u00a0autokabalesis.[16]\n In d\u00e9j\u00e0 vu, a person feels undue familiarity to an event or a person.\n In d\u00e9j\u00e0 pens\u00e9e, a completely new thought is seen as familiar by an individual, as if it had occurred before. The sensation may be caused by a type of convulsion known as a \"partial seizure\" which occurs in parts of the temporal lobe or other areas of the brain \u2013 typically the individual remains conscious throughout.\n Dementia praecox refers to a chronic, deteriorating psychotic disorder characterized by rapid cognitive disintegration, usually beginning in the late teens or early adulthood.\n Dementia pugilistica, also called \"chronic traumatic encephalopathy\", \"pugilistic Parkinson's syndrome\", \"boxer's syndrome\", and \"punch-drunk syndrome\", is a neurological disorder which affects career boxers and others who receive multiple dazing blows to the head. The condition develops over a period of years, with the average time of onset being about 16 years after the start of a career in boxing.\n Derailment, also known as loosening of associations, refers to disorganized thinking that jumps between ideas that seem entirely unrelated. Compare \u00a7\u00a0akataphasia, \u00a7\u00a0asyndesis, \u00a7\u00a0entgleisen, \u00a7\u00a0flight of ideas, \u00a7\u00a0knight's move thinking, and \u00a7\u00a0logorrhoea.[14] It can be seen in individuals with schizophrenia, as well as those experiencing mania.[1]\n Dereistic means: away from reality, undirected fantasy thinking.[17] Carl Jung wrote, \"This is the basic activity of psychic life, this fantasy making\", and he used the term image not from afterimage, something one has experienced or seen, but says he takes it from poetic usage.[18] Dereistic thinking: An old descriptive term used to refer to thinking not in accordance with the facts of reality and experience and following illogical, idiosyncratic reasoning. This term is also used interchangeably with \u00a7\u00a0autistic thinking though they are not exact synonyms: dereistic emphasizes disconnection from reality and autistic emphasizes preoccupation with inner experience.\n Alternate term for organic hallucinosis and delusional parasitosis, the continuous belief that one's skin or body has been infested by parasites or insects. This state cannot be diagnosed if the hallucinatory state is produced while the individual is under the influence of drugs or alcohol, or if the individual fulfills the criterion for delirium. In general, if an individual is under the influence of a drug, or experiencing the symptoms of withdrawal from that drug, this condition is not psychiatric but medical, and termed formication.\n Dhat syndrome refers to a complaint of premature ejaculation or impotence and a false belief that semen is being passed in the urine.\n The doppelg\u00e4nger is a phenomenon in which the afflicted believe that their exact \"double\" is present alongside them all the time and goes with them wherever they go.\n In \u00e9cho de la pens\u00e9e, meaning \"thought echo\" in French, thoughts seem to be spoken aloud just after being produced. The individual hears the \"echo\" of their thoughts in the form of a voice after they have made the thought. See also \u00a7\u00a0gedankenlautwerden and \u00a7\u00a0thought sonorization.\n From German entgleisen \"to derail\". Alternate term used for derailment of thought (a morbid form of loosening of association or asyndesis). A Schneiderian term by origin. In this form of thought the individual jumps from one topic to another during conversation and both topics have literally no connection with each other. This is in contrast with flight of ideas where connection is present between one topic and another. Compare \u00a7\u00a0akataphasia, \u00a7\u00a0asyndesis, and \u00a7\u00a0derailment.\n Extracampine hallucinations are hallucinations beyond the possible sensory field, e.g., an individual \"seeing\" somebody standing behind them is a visual extracampine hallucination experience.[19][20][21]\n Fantasy is imagining that expresses desires and aims.\n The moods of an individual with fatuous affect resemble the moods of a child. This condition is seen in hebephrenic schizophrenia.[citation needed]\n \"Flight of ideas\" describes excessive speech at a rapid rate that involves causal association between ideas. Links between ideas may involve usage of puns or rhymes.[14][22] It is typical of mania, classically seen in bipolar disorder.[14] Compare \u00a7\u00a0derailment and Racing thoughts.\n Also called \"induced psychosis\", folie \u00e0 deux is a delusional disorder shared by two or more people who are closely related emotionally. One has real psychosis while the symptoms of psychosis are induced in the other or others due to close attachment to the one with psychosis. Separation usually results in symptomatic improvement in the one who is not psychotic.\n Folie communiqu\u00e9e, folie impos\u00e9e, folie induite, and folie simultan\u00e9e are the four subtypes of folie \u00e0 deux.\n Folie communiqu\u00e9e, or subtype C of folie \u00e0 deux, occurs when a normal person has a contagion of their ideas after resisting them for a long time. Once they acquire these beliefs they maintain them despite separation.\n Folie impos\u00e9e, or subtype A of folie \u00e0 deux, is the most common form in which the dominant person imposes a delusion into a person who was not previously mentally ill. Separation of the two results in improvement of the non-dominant person.\n In folie induite, or subtype D of folie \u00e0 deux, a person who is already psychotic adds the delusions of a closely associated person to their own.\n In folie simultan\u00e9e, or subtype B of folie \u00e0 deux, a delusional system emerges simultaneously and independently in two closely related persons, and the separation of the two would not be beneficial in the resolution of psychopathology.\n In Fregoli delusion, a person has a delusional belief that various different people are in fact a certain other person, even if there is no physical resemblance.[1]\n Fregoli syndrome is considered a form of delusional misidentification \"in which the false identification of familiar people occurs in strangers\".[23]\n In Gedankenlautwerden, an individual hears thoughts spoken aloud. Thoughts are heard in the form of a voice at the same time as they are thought, not afterwards. See also \u00a7\u00a0\u00e9cho de la pens\u00e9e and \u00a7\u00a0thought sonorization\n Gegenhalten is a catatonic phenomenon in which the subject opposes all passive movements with the same degree of force as applied by the examiner. It is slightly different from \u00a7\u00a0negativism in which the subject does exactly the opposite to what is asked in addition to showing resistance.\n Hemiasomatognosia is a subtype of anosognosia in which the person with hemiplegia neglects one half of their body.\n Hyposchemazia is characterized by the reduced awareness of one's body image and aschemazia by the absence of it. These disorders can have many varied causes such as physical injuries, mental disorders, or mental or physical states. These include transection of the spinal cord, parietal lobe lesions (e.g. right middle cerebral artery thrombosis), anxiety, depersonalization, epileptic auras, migraines, sensory deprivation, and vertigo (i.e. \"floating on air\").\n Id\u00e9e fixe is an alternate term for an overvalued idea. In this condition, a belief that might seem reasonable both to the individual and to other people comes to dominate completely the individual's thinking and life.\n Thoughts that one's own body part or action is not of one's own.\n Thoughts that one's own action is caused by someone else's will or some other external cause.\n Ideas of reference are a delusional belief that general events are personally directed at oneself.[14]\n An illusion is a false perception of a detectable stimulus.[1]\n Jargon aphasia is characterized by incoherent, meaningless speech with neologisms (newly invented words). These are unconscious thoughts that find expression when one is off one's guard and must be consciously repressed.\n In Kl\u00fcver\u2013Bucy syndrome, an individual will display placidity, hyperorality, hypersexuality, and hyperphagia. This condition results from bilateral destruction of the amygdaloid bodies of the limbic system.\n Knight's move thinking is a complete loosening of associations where there is no logical link between one idea and the next. Based on a knight on a chessboard where the movement can be any L shaped direction, making it difficult to track. Compare \u00a7\u00a0derailment.[24]\n Koro is a culture-specific syndrome, generally seen only among Chinese people. It involves a panicked feeling that one's genitals are retracting into the abdomen, and that this will result in death.[1]\n Kuru (also known as \"laughing sickness\" due to the outbursts of laughter that mark its second phase) was first noted in New Guinea in the early 1900s. Kuru is now known to be a prion disease, one of several known transmissible spongiform encephalopathies.\n Latah is a culture-specific syndrome usually seen in Southeast Asia and involves startle-induced disorganization, hypersuggestibility, automatic obedience, and echopraxia (a tendency to mimic examiner's or other person's actions). It is usually associated with women.\nThere is controversy over whether Latah is a real psychiatric condition, or merely a display of exhibitionism that would otherwise not be socially acceptable.\n In l'homme qui rit (from the French, meaning \"the man who laughs\"), an individual displays inappropriate laughter accompanied by release phenomena of the frontal subdominant lobe.\n Lilliputian hallucinations are characterized by abnormal perception of objects as being shrunken in size but normal in detail. Usually seen in delirium tremens.\n In logoclonia, the individual often repeats the last syllable of a word. Compare echolalia. Often a symptom of Alzheimers or Parkinson's disease.\n Logorrhoea, also known as \"volubility\", is characterized by fluent and rambling speech using numerous words. Compare \u00a7\u00a0derailment.\n Mania is often mirrored as a minor image of depression. Mania is a state of abnormally elevated arousal, affect, and energy level. As mania intensifies, irritability can be more pronounced and result in anxiety or violence. Mania symptoms are elevated mood, flights of ideas, pressure of speech, increased energy, decreased need or desire for sleep, and hyperactivity.\n Mania a potu is an alcohol intoxication state with violent and markedly disinhibited behavior. This condition is different from violent behavior in otherwise normal individuals who are intoxicated.\n Metonymy is a speech disturbance in which patients, commonly with schizophrenia, use inappropriate words or expressions that are related to the proper ones.  Examples include: consume a menu, instead of a meal; lose the piece of string of the conversation, not the thread of the conversation.  See also \u00a7\u00a0word approximation.[25][26]\n Mitgehen (German: [\u02c8m\u026at\u02cc\u0261e\u02d0\u0259n] \u24d8) is an extreme form of mitmachen in which very slight pressure leads to movement in any direction, also called the \"anglepoise\" effect or \"anglepoise lamp sign\". This movement occurs despite instructions to resist the pressure, as individuals with this condition often experience even slight pressure as forcible grasping and pushing.\n In mitmachen (German: [\u02c8m\u026at\u02ccmaxn\u0329] \u24d8), one's body can be put into any posture, despite instructions given to resist. Compare \u00a7\u00a0mitgehen.\n Moria is the condition characterized by euphoric behavior, such as frivolity and the inability to act seriously. In addition, there is a lack of foresight and a general indifference. It is found in frontal lobe lesions, often along with \u00a7\u00a0witzelsucht, particularly when the orbital surface is damaged. Recent research has shown its presence in frontotemporal dementia.\n Resistance to attempts to move the subject, who then does the opposite of what is asked. Negativism is usually a sign of catatonia, and may progress to (catatonic) rigidity. It is slightly different from \u00a7\u00a0gegenhalten, in which the individual resists movement but does not perform the opposite movement. Also see: oppositional defiance disorder (ODD).\n In a neurological or psychopathological context, neologisms are nonsensical words or phrases whose origins are unrecognizable, and are associated with aphasia or schizophrenia. Incorrectly constructed words whose origins are understandable may also be called neologisms, but are more properly referred as \u00a7\u00a0word approximations.[27][28]\n The omega sign is the occurrence of a fold (like the Greek letter omega, \u03a9 ) in the forehead, above the nose, produced by the excessive action of the corrugator muscle. It is sometimes seen in depression.\n From Greek oneiros as meaning \"dream\". In an oneiroid state one feels and behaves as though in a dream. Also known as \"oneirophrenia\" as described by Ladislas J. Meduna.\n See \u00a7\u00a0oneiroid state or oneirophrenia.\n Overvalued ideas are exaggerated beliefs that a person sustains beyond reasons, but are not as unbelievable and are not as persistently held as delusions.[29][30] \nPreoccupation with spouse's possible infidelity can be an overvalued idea if no evidence exists to arouse suspicion.\nBody dysmorphic disorder's obsessive preoccupation that some aspect of one's own appearance is severely flawed is another example of an overvalued idea.[29]\n Palilalia is characterized by the repetition of a word or phrase; i.e., the subject continues to repeat a word or phrase after once having said. It is a form of \u00a7\u00a0perseveration.\n In palinacousis the subject continues to hear a word, a syllable or any sound, even after the withdrawal of stimulus. It is a type of \u00a7\u00a0perseveration.\n In palinopsia a visual image persists after the stimulus has gone (similar to an afterimage seen after looking into a bright light).\n A Freudian slip, or parapraxis, is an error in speech, memory or physical action that is believed to be caused by the unconscious mind.\n A delusion in which a person believes they have seen a face transform into a grotesque form \u2013 often described as a 'monster', 'vampire', 'werewolf' or similar. This is very rare and most likely to be described by people with schizophrenia.\n Paraschemazia is characterized by a distortion of body image. It can be caused by hallucinogenic drugs such as LSD and mescalin, epileptic auras, and sometimes migraines.\n In pareidolia a vague or random stimulus is mistakenly perceived as recognizable. Pareidolia is a type of illusion and hence called \"pareidolic illusion\".\n This term refers to uncontrollable repetition of a particular response, such as a word, phrase, or gesture, despite the absence or cessation of the original stimulus.[31] Usually it is seen in organic disorders of brain, head injury, delirium or dementia, however can be seen in schizophrenia as well.\n This refers to schizophrenia in people with mild learning disabilities.[32]\n Piblokto, pibloktoq, or Arctic hysteria, is a condition exclusively appearing in Inuit societies living within the Arctic Circle. Appearing most prevalently in winter, it is considered to be a form of a culture-specific disorder.[33]\n Symptoms can include intense \"hysteria\" (including screaming and uncontrolled wild behavior), depression, coprophagia, and insensitivity to extreme cold.[34] This condition is most often seen in Inuit women.[35]\n Often associated with schizophrenia, dementia, and severe depression, poverty of ideas is a thought disturbance in which thought spontaneity and productivity are reduced, and are seen in speech that is vague, has many simple or meaningless repetitions, or full of stereotyped phrases.[36]\n Pseudologia fantastica is a condition in which a person grossly exaggerates their symptoms or even tells a lie about their symptoms in order to get medical attention. Seen in malingering and Munchausen syndrome.\n Where the individual holds their head a few centimetres above the bed. No explanation is offered for this. It is a symptom of catatonia and can last for many hours.\n Psychopathology is a term which refers to either the study of mental illness or mental distress or to\nthe manifestation of behaviours and experiences which may be indicative of mental illness or psychological impairment.\n Rabbit syndrome is characterized by rapid, vertical, rhythmic movements of lips so that it resembles a rabbit chewing.[15] It is a type of extrapyramidal symptom, distinct from tardive dyskinesia as it spares the tongue and involves vertical movements only.[37]\n In reduplicative hallucinations there is the perception of seeing a double. Particular kinds of reduplicative hallucination include autoscopy, heautoscopy and out-of-body experiences.\n Reduplicative paramnesia is a delusional misidentification syndrome in which one's surroundings are believed to exist in more than one physical location.\n Reflex hallucinations occur when true sensory input in one sense leads to production of a hallucination in another sense, e.g. seeing a doctor writing (visual) and then feeling him writing across one's stomach (tactile).\n Restlessness has two components: akathisia (subjective \"inner\" restlessness) and psychomotor agitation (an excess of motor activity).\n Mental retardation (more commonly referred to as intellectual disability[38][39]) is a term used when a person has certain limitations in mental functioning and in skills such as communicating, taking care of themselves, and social skills.\n In children, these limitations will cause a child to learn and develop more slowly than a typical child. Children with intellectual disability may take longer to learn to speak, walk, and take care of their personal needs such as dressing or eating. They are likely to have trouble learning in school. They will learn, but it will take them longer. There may be some things they cannot learn.[40]\n Left\u2013right disorientation is one of the four cardinal signs of Gerstmann's syndrome.\n Scanning speech is an ataxic dysarthria in which syllable durations are equalized. It is characteristic of the dysarthria of multiple sclerosis. Together with nystagmus and intention tremor it forms Charcot's triad 1.\n Schizophasia, commonly referred to as word salad, is confused, and often repetitious, language that is symptomatic of various mental illnesses.[41]\n A schnauzkrampf is a grimace resembling pouting sometimes observed in catatonic individuals.\n Sensitiver beziehungswahn, is an alternate term for ideas of reference. In this the person thinks as people are talking about them or observing them or a talk is going on about them on television or radio. Seen in social phobia, depression, delusional disorder and in schizophrenia where they are often present up to a delusional extent.\n The Stockholm syndrome is a psychological response sometimes seen in a hostage, in which the hostage exhibits loyalty to the hostage-taker, in spite of the danger (or at least risk) in which the hostage has been placed.[42] Stockholm syndrome is also sometimes discussed in reference to other situations with similar tensions, such as battered person syndrome,[43] child abuse cases, and bride kidnapping.\n Also spelled syn\u00e6sthesia, synaesthesia, or synesthesia\u2014plural synesthesiae, from the Greek syn- meaning \"union\" and aesthesis meaning \"sensation\", it is a neurological phenomenon in which two or more bodily senses are coupled.\n In telegraphic speech conjunctions and articles are missed out; meaning is retained and few words are used.\n Thought blocking, also referred to as thought withdrawal, refers to an abrupt stop in the middle of a train of thought; the individual might or might not be unable to continue the idea.[14] This is a type of formal thought disorder that can be seen in schizophrenia.[1]\n A combined term for \u00a7\u00a0gedankenlautwerden and \u00a7\u00a0\u00e9cho de la pens\u00e9e (\"thought echo\")\n Torpor in psychopathology is usually taken to mean profound inactivity not caused by reduction in consciousness.\n Tourette syndrome (abbreviated as TS or Tourette's) is a common neurodevelopmental disorder that begins in childhood or adolescence. It is characterized by multiple movement (motor) tics and at least one vocal (phonic) tic. Common tics are blinking, coughing, throat clearing, sniffing, and facial movements. These are typically preceded by an unwanted urge or sensation in the affected muscles, can sometimes be suppressed temporarily, and characteristically change in location, strength, and frequency. Tourette's is at the more severe end of a spectrum of tic disorders. The tics often go unnoticed by casual observers.\n Traumatic bonding occurs as the result of ongoing cycles of abuse in which the intermittent reinforcement of reward and punishment creates powerful emotional bonds that are resistant to change.[44]\n Also known as \"hair pulling disorder\", trichotillomania (TTM) is an impulse control disorder characterised by a long term urge that results in the pulling out of one's hair. This occurs to such a degree that hair loss can be seen. Efforts to stop pulling hair typically fail. Hair removal may occur anywhere; however, the head and around the eyes are most common. The hair pulling is to such a degree that it results in distress\n Verbigeration is a verbal stereotypy (repetition) in which usually one or several sentences or strings of fragmented words are repeated continuously. Sometimes individuals will produce incomprehensible jargon in which stereotypies are embedded. The tone of voice is usually monotonous. This can be produced spontaneously or precipitated by questioning. The term verbigeration was first used in psychiatry by Karl Kahlbaum in 1874, and it referred to a manner of talking which was very fast and incomprehensible. At the time verbigeration was seen as a \"disorder of language\" and represented a central feature of catatonia. The word is derived from the Latin word verbum (also the source of verbiage), plus the verb ger\u0115re, to carry on or conduct, from which the Latin verb verbiger\u0101re, to talk or chat, is derived. However, clinically the term verbigeration never achieved popularity and as such has virtually disappeared from psychiatric terminology. Compare Echolalia.[45]\n An ill-humored mood state often accompanied by low mood and depressive symptoms. The people surrounding the individual often feel upset by this condition.\n In vorbeigehen or vorbeireden, an individual will answer a question in such a way that it is clear the question was understood, though the answer itself is very obviously wrong. For example: \"How many legs does a dog have?\" \u2013 \"Six\".\nThis condition occurs in Ganser syndrome and has been observed in prisoners awaiting trial. Vorbeigehen (German: [fo\u02d0\u0250\u032f\u02c8ba\u026a\u032f\u02cc\u0261e\u02d0\u0259n] \u24d8, giving approximate answers) was the original term used by Ganser but Vorbeireden (talking past the point) is the term generally in use (Goldin 1955). This behavior is also seen in people trying to feign psychiatric disorders (hence its association with prisoners).[46]\n Wahneinfall is an alternate term for autochthonous delusions or delusional intuition. This is one of the types of primary delusions in which a firm belief comes into the individual's mind \"out of the blue\" or as an intuition, hence called \"delusional intuition\". Other types of primary delusions include delusional mood (or atmosphere), delusional (apophanous) perception and delusional memories. Care is taken not to impugn an otherwise-rational individual's instinctive aversion or inexpressible sense of or belief about a thing by dismissing it as wahneinfall.\n Waxy flexibility, also known as \u00a7\u00a0cerea flexibilitas, is characterized by an individual's movements having the feeling of a plastic resistance, as if the person were made of wax. This occurs in catatonic schizophrenia, and a person with this condition can have his limbs placed in fixed positions as if the person were in fact made from wax.\n Compare \u00a7\u00a0mitmachen and \u00a7\u00a0waxy flexibility.\n Windigo (also wendigo, windago, windiga, witiko, and numerous other variants) psychosis is a culture-bound disorder which involves an intense craving for human flesh and the fear that one will turn into a cannibal. This was alleged to have occurred among Algonquian Indian cultures.\n Witzelsucht is a tendency to tell inappropriate jokes and create excessive facetiousness and inappropriate or pointless humor. It is seen in frontal lobe disorders usually along with \u00a7\u00a0moria. Recent research has shown that it may also be seen in frontotemporal dementia.[47]\n Usage of words in an unconventional or inappropriate way (as in \u00a7\u00a0metonymy), or usage of new but understandable words that are conventionally constructed, contrasting with \u00a7\u00a0neologisms, which are new words whose origins cannot be understood.[48][49]\n Word salad (derived from the German: Wortsalat) is characterized by confused, and often repetitious, language with no apparent meaning or relationship attached to them. It is often symptomatic of various mental illnesses, such as psychoses, including schizophrenia. Compare \u00a7\u00a0derailment.\n W\u00fcrgstimme refers to speaking in an odd muffled or strangled voice. It is mainly seen in schizophrenia.\n Zeitraffer (German: [\u02c8t\u0361sa\u026a\u032ft\u02cc\u0281af\u0250] \u24d8) phenomenon, which translates to \"time-lapse\" in English, highlights how events, objects, and processes change and evolve over time, sometimes in ways that are imperceptible in real-time.\n From a philosophical perspective, Zeitraffer can be related to various philosophical themes:\n 1. Temporality: It raises questions about the nature of time, whether it is continuous or discrete, and how our perception of time affects our understanding of reality.\n 2. Impermanence: Zeitraffer reminds us of the transient nature of existence, emphasizing how everything is subject to change and decay.\n 3. Perception and Reality: It underscores the difference between how we perceive the world in real-time and how it actually changes over time, raising questions about the reliability of our senses and the nature of reality.\n Zeitlupenwahrnehmung phenomenon translates to \u201cslow motion perception\u201d in English\n \n One of the paraphilias, characterized by marked distress over, or acting on, urges to indulge in sexual activity that involves animals.\n",
      "timestamp": "2025-10-09 19:12:08.628518"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_psychiatry",
      "title": "Glossary of psychiatry - Wikipedia",
      "description": "",
      "text": "\n This glossary covers terms found in the psychiatric literature; the word origins are primarily Greek, but there are also Latin, French, German, and English terms. Many of these terms refer to expressions dating from the early days of psychiatry in Europe; some are deprecated, and thus are of historic interest.  \n Abreaction is a process of vividly reliving repressed memories and emotions related to a past event.[1] Sigmund Freud used hypnosis to rid his patients of pathological memories through abreaction.[1]\n Aboulia or Abulia, in neurology, refers to a lack of will or initiative. The individual is unable to act or make decisions independently. The condition may range from subtle to overwhelming in severity.\n Achromatopsia is a term referring to or acquired agnosia for color. This term includes color blindness.\nAchromatopsia is a condition characterized by a partial or total absence of color vision. People with complete achromatopsia cannot perceive any colors; they see only black, white, and shades of gray. Incomplete achromatopsia is a milder form of the condition that allows some color discrimination.\n Achromatopsia also involves other problems with vision, including an increased sensitivity to light and glare (photophobia), involuntary back-and-forth eye movements (nystagmus), and significantly reduced sharpness of vision (low visual acuity). Affected individuals can also have farsightedness (hyperopia) or, less commonly, nearsightedness (myopia). These vision problems develop in the first few months of life.\n Achromatopsia is different from the more common forms of color vision deficiency (also called color blindness), in which people can perceive color but have difficulty distinguishing between certain colors, such as red and green.[2]\n Mild illusions or misperceptions associated with changes in mood; e.g. mistaking a shadow for the presence of a person, perceiving movement in peripheral when there is none.\n Akataphasia (Kraepelin 1896) refers to a syntactic disturbance of speech resulting from dissolution of logical ordering of thoughts. It manifests as rambling speech. Compare Derailment.[3]\n Akathisia refers to a subjective feeling of restlessness in the lower limbs that is related to abnormal activity in the extrapyramidal system in the brain, often due to antipsychotic medication.[1] It tends to manifest as an inability to sit still.[1]\n Alexithymia refers to an inability to identify and describe emotions in the self.[4]\n In Alice in Wonderland experience, individuals perceive objects (including animals and other humans, or parts of humans, animals, or objects) as appearing substantially smaller than in reality. Generally, the object appears far away or extremely close at the same time. An alternate term for this is somaesthetic aura. Also see \u00a7\u00a0Lilliputian hallucinations\n Literally, this term means \"not having words\". The term may refer to either \"poverty of speech\" or \"poverty of thought\". In the former, speech, though adequate in verbiage, conveys very little information and may consist of stock phrases or vague references. In poverty of thought, by contrast, there is a far-reaching impoverishment of the entire thinking of the individual, who, as a result, says very little. It is typically a negative symptom of schizophrenia,[1] although it may also be seen in advanced dementia.\n Amenomania (compound of Latin amoenus, \"cheerful\"; and Greek \u03bc\u03b1\u03bd\u03af\u03b1, \"madness\")[5] is a disused psychiatric diagnosis.\n The phrase \"running amok\" describes the behavior of an individual who is very agitated and may be at danger of causing harm to themselves or others.[6][7] The syndrome of \"Amok\" is found in the DSM-IV TR.[8]\n Anhedonia refers to an inability to experience pleasure, and may be described as a feeling of emotional emptiness.[1] It can be a negative symptom of schizophrenia.[1] It also may be seen in severe depressive states and schizoid personality disorder.\n Anosognosia is a condition in which a person who has a certain disability seems unaware of the existence of their disability. \u00a7\u00a0hemiasomatognosia is a subtype of anosognosia in which the person with hemiplegia neglects one half of their body.\n Anton syndrome, occasionally known as Anton-Babinski syndrome, is a form of cortical blindness in which the individual denies the visual impairment. The individual may attempt to walk, bumping into objects and injuring himself.\nAnton syndrome is caused by damaging the occipital lobes bilaterally or from disrupting the pathway from the primary visual cortex into the visual association cortex.\n Anwesenheit refers to the false perception of an unfamiliar presence. It is commonly associated with periods of grief, schizophrenia and other emotional disturbances.\n This is an alternate term for delusional perception. It is one of the Schneiderian first rank symptoms and is defined as a true perception, to which an individual attributes a false meaning.\n Aphemia is the alternate term for mutism. Mutism is absence of speech with apparently normal level of consciousness. Mutism can be dissociative (hysterical) in which an individual (commonly a child or adolescent) stops speaking at once without involvement of any neurological or physical contributing factor; or it can be elective (selective) in which a child does not speak at all in certain situations (such as in school) but speaks well in other conditions (like at home or at play). A rare cause of mutism is akinetic mutism which results due to a lesion around the third ventricle of the brain.\n Apperception is a normal phenomenon and refers to the ability to understand sensory inputs in their context, to interpret them and to incorporate them into experience. Failure of apperception is seen in delirious states.\n Astasia-abasia is a form of psychogenic gait disturbance in which gait becomes impaired in the absence of any neurological or physical pathology. The person usually walks in a bizarre manner. They stagger and appear as if going to fall, but always manage to catch hold of something in time. Sometimes these people cannot even stand, but on the other hand they are well able to move their legs while lying down or sitting. Often associated with conversion disorder or somatization disorder.\n Asyndesis means loosening of association. A milder form of derailment of thought, it is marked by the individual leaping from topic to topic which have only the most tenuous, if any, connection with each other. This is in contrast with flight of ideas, whereby the individual's successive ideas may be linked and \"understandable\" to the listener.\nSee also \u00a7\u00a0akataphasia and \u00a7\u00a0entgleisen term introduced by (Cameron).\n From aut = \"self\" and -ism = \"state or orientation\". Originally, Eugen Bleuler used this term to describe schizophrenia. In general, it refers to any (pathological) tendency to be self-absorbed to such a degree that the feelings, thoughts and desires of a person are governed by their internal apprehension of the world and not by an external reality shared with others.\n Today the term is used most often to refer to a specific developmental syndrome (see autism spectrum).[9]:\u200ap. 76\u200a\n autistic thinking is an outdated term for egocentric thought processes that have little or no relation to consensus reality. The term does not accurately describe the thinking styles of autistic people.[10]\n Jaspers defined this as a delusion arising without apparent cause. For example, suddenly, without apparent cause, having the delusional belief that one is an alien.\n Autokabalesis is a term for committing suicide by jumping from a very high place.[11]\n Automatic obedience is an exaggerated co-operation with an examiner's request, as if the individual were an \"automaton\" robotically obeying a command. It is often a sign of catatonia.\n Automatisms are sequences of activity that occur without conscious control. They may be simple and repetitive (tic-like) or complex, and are usually natural-looking but purposeless. Automatic behavior is not usually recalled afterwards.\n Autoscopy is the reduplicative hallucination of \"seeing one's own body from the outside\" while still maintaining an egocentric visuo-spatial perspective. Autoscopy is sometimes used synonymously with out-of-body experience.\n Avolition is an inability to initiate and complete goal-directed behavior.[1] It can sometimes be misinterpreted as laziness, but it is actually a negative symptom of schizophrenia.\n Belle indifference or la belle indiff\u00e9rence is characterized by a lack of concern and feeling of indifference about a disability or symptom.[1] It can be seen in conversion disorder.\n Bouff\u00e9e d\u00e9lirante is a French term used in the past for acute and transient psychotic disorders (F23 in ICD-10). In DSM-IV, it is described as \"brief psychotic disorder\" (298.8). The symptoms usually have an acute onset and reach their peak within two weeks. The symptoms start resolving in a few weeks and complete recovery usually occurs within two or three months.[12]\n Brain fag syndrome[13] is an example of a culture-bound syndrome. \"Brain fag\" was once a common term for mental exhaustion. Today, the syndrome describes students (predominantly males, particularly in West Africa) experiencing symptoms including somatic, sleep-related and cognitive complaints, head and neck pains, difficulty in concentrating and retaining information, and eye pain.\n Synonym of \u00a7\u00a0clouding of consciousness.\n Bruxism refers to teeth grinding behavior that is usually seen in children.\n In Capgras syndrome, the individual feels that a person familiar to them, usually a family member, has been replaced by an imposter.[1] This is a type of delusion that can be experienced as part of schizophrenia. Capgras syndrome and several other related disorders are referred to as \"delusional misidentification syndrome\".\n Catalepsy is the term for catatonic rigidity of the limbs which often results in abnormal posturing for long intervals.\n Cataplexy refers to a sudden loss of muscle tone and is commonly precipitated by a strong emotional response.[1]\n Catatonia involves a significant psychomotor disturbance, which can occur as catalepsy, stupor, excessive purposeless motor activity, extreme negativism (seemingly motiveless resistance to movement), mutism, echolalia (imitating speech), or echopraxia (imitating movements).[1] There is a catatonic subtype of schizophrenia.[1]\n Cerea flexibilitas, meaning \"waxy flexibility\", refers to people allowing themselves to be placed in postures by others, and then maintaining those postures for long periods even if they are obviously uncomfortable.[1] It is characterized by an individual's movements having the feeling of a plastic resistance, as if the person were made of wax. This occurs in catatonic schizophrenia, and a person with this condition can have their limbs placed in fixed positions as if the person were in fact made from wax.\n Chorea refers to erratic involuntary movements. The term comes from the Greek word \"choreia\" or \"dance\" since usually large groups of muscles are involved simulating dance-like movements.\n Circumstantial thinking, or circumstantial speech, refers to a person being unable to answer a question without giving excessive, unnecessary detail.[14] This differs from tangential thinking, in that the person does eventually return to the original point, circling back on-topic.\n Clang associations are ideas that are related only by similar or rhyming sounds rather than actual meaning.[14]\n Claparede's paradox refers to retention of non-verbal and implicit memory in people with Korsakoff's syndrome.[15]\n Clouding of consciousness, also known as brain fog or mental fog, is a global impairment in higher central nervous functioning. All aspects of cognitive functioning are affected. On mental status examinations it is manifest by disorientation in time, place and person, memory difficulties caused by failure to register and recall, aphasia, and agnosia. Impaired perception functioning leads to illusions and hallucinations often in the visual sensory modality. This then causes agitation and distress and secondary delusions. The term confusion state is sometimes used to mean clouding of consciousness, but is avoided whenever possible because it is ambiguous.\n Coenestopathic state refers to a situation in which an individual in a coenestopathic state has a localized distortion of body awareness.\n Confabulation is the confusion of imagination with memory, or the confusion of true memories with false memories.\n Conversion disorder involves the unintentional production of symptoms or deficits affecting motor or sensory function that are not fully explained by a neurological or medical condition.[1] This can manifest as paralysis, for example. It generally involves psychological factors, and symptoms may worsen in the context of situational conflict.[1]\n Coprolalia is the involuntary utterance of socially inappropriate phrases. It is a phonic tic associated with Tourette syndrome, although less than 15% of persons with Tourette's have coprolalia.\n Cotard delusion involves the belief in an individual that one or more of their organs has changed in some way, has ceased functioning, or has disappeared entirely.[1]\nThis type of delusion is most commonly seen in patients with schizophrenia.[1]\n Defenestration refers to an individual voluntarily ejecting themselves from a window or another elevated position, usually in the context of attempted suicide. Also see \u00a7\u00a0autokabalesis.[16]\n In d\u00e9j\u00e0 vu, a person feels undue familiarity to an event or a person.\n In d\u00e9j\u00e0 pens\u00e9e, a completely new thought is seen as familiar by an individual, as if it had occurred before. The sensation may be caused by a type of convulsion known as a \"partial seizure\" which occurs in parts of the temporal lobe or other areas of the brain \u2013 typically the individual remains conscious throughout.\n Dementia praecox refers to a chronic, deteriorating psychotic disorder characterized by rapid cognitive disintegration, usually beginning in the late teens or early adulthood.\n Dementia pugilistica, also called \"chronic traumatic encephalopathy\", \"pugilistic Parkinson's syndrome\", \"boxer's syndrome\", and \"punch-drunk syndrome\", is a neurological disorder which affects career boxers and others who receive multiple dazing blows to the head. The condition develops over a period of years, with the average time of onset being about 16 years after the start of a career in boxing.\n Derailment, also known as loosening of associations, refers to disorganized thinking that jumps between ideas that seem entirely unrelated. Compare \u00a7\u00a0akataphasia, \u00a7\u00a0asyndesis, \u00a7\u00a0entgleisen, \u00a7\u00a0flight of ideas, \u00a7\u00a0knight's move thinking, and \u00a7\u00a0logorrhoea.[14] It can be seen in individuals with schizophrenia, as well as those experiencing mania.[1]\n Dereistic means: away from reality, undirected fantasy thinking.[17] Carl Jung wrote, \"This is the basic activity of psychic life, this fantasy making\", and he used the term image not from afterimage, something one has experienced or seen, but says he takes it from poetic usage.[18] Dereistic thinking: An old descriptive term used to refer to thinking not in accordance with the facts of reality and experience and following illogical, idiosyncratic reasoning. This term is also used interchangeably with \u00a7\u00a0autistic thinking though they are not exact synonyms: dereistic emphasizes disconnection from reality and autistic emphasizes preoccupation with inner experience.\n Alternate term for organic hallucinosis and delusional parasitosis, the continuous belief that one's skin or body has been infested by parasites or insects. This state cannot be diagnosed if the hallucinatory state is produced while the individual is under the influence of drugs or alcohol, or if the individual fulfills the criterion for delirium. In general, if an individual is under the influence of a drug, or experiencing the symptoms of withdrawal from that drug, this condition is not psychiatric but medical, and termed formication.\n Dhat syndrome refers to a complaint of premature ejaculation or impotence and a false belief that semen is being passed in the urine.\n The doppelg\u00e4nger is a phenomenon in which the afflicted believe that their exact \"double\" is present alongside them all the time and goes with them wherever they go.\n In \u00e9cho de la pens\u00e9e, meaning \"thought echo\" in French, thoughts seem to be spoken aloud just after being produced. The individual hears the \"echo\" of their thoughts in the form of a voice after they have made the thought. See also \u00a7\u00a0gedankenlautwerden and \u00a7\u00a0thought sonorization.\n From German entgleisen \"to derail\". Alternate term used for derailment of thought (a morbid form of loosening of association or asyndesis). A Schneiderian term by origin. In this form of thought the individual jumps from one topic to another during conversation and both topics have literally no connection with each other. This is in contrast with flight of ideas where connection is present between one topic and another. Compare \u00a7\u00a0akataphasia, \u00a7\u00a0asyndesis, and \u00a7\u00a0derailment.\n Extracampine hallucinations are hallucinations beyond the possible sensory field, e.g., an individual \"seeing\" somebody standing behind them is a visual extracampine hallucination experience.[19][20][21]\n Fantasy is imagining that expresses desires and aims.\n The moods of an individual with fatuous affect resemble the moods of a child. This condition is seen in hebephrenic schizophrenia.[citation needed]\n \"Flight of ideas\" describes excessive speech at a rapid rate that involves causal association between ideas. Links between ideas may involve usage of puns or rhymes.[14][22] It is typical of mania, classically seen in bipolar disorder.[14] Compare \u00a7\u00a0derailment and Racing thoughts.\n Also called \"induced psychosis\", folie \u00e0 deux is a delusional disorder shared by two or more people who are closely related emotionally. One has real psychosis while the symptoms of psychosis are induced in the other or others due to close attachment to the one with psychosis. Separation usually results in symptomatic improvement in the one who is not psychotic.\n Folie communiqu\u00e9e, folie impos\u00e9e, folie induite, and folie simultan\u00e9e are the four subtypes of folie \u00e0 deux.\n Folie communiqu\u00e9e, or subtype C of folie \u00e0 deux, occurs when a normal person has a contagion of their ideas after resisting them for a long time. Once they acquire these beliefs they maintain them despite separation.\n Folie impos\u00e9e, or subtype A of folie \u00e0 deux, is the most common form in which the dominant person imposes a delusion into a person who was not previously mentally ill. Separation of the two results in improvement of the non-dominant person.\n In folie induite, or subtype D of folie \u00e0 deux, a person who is already psychotic adds the delusions of a closely associated person to their own.\n In folie simultan\u00e9e, or subtype B of folie \u00e0 deux, a delusional system emerges simultaneously and independently in two closely related persons, and the separation of the two would not be beneficial in the resolution of psychopathology.\n In Fregoli delusion, a person has a delusional belief that various different people are in fact a certain other person, even if there is no physical resemblance.[1]\n Fregoli syndrome is considered a form of delusional misidentification \"in which the false identification of familiar people occurs in strangers\".[23]\n In Gedankenlautwerden, an individual hears thoughts spoken aloud. Thoughts are heard in the form of a voice at the same time as they are thought, not afterwards. See also \u00a7\u00a0\u00e9cho de la pens\u00e9e and \u00a7\u00a0thought sonorization\n Gegenhalten is a catatonic phenomenon in which the subject opposes all passive movements with the same degree of force as applied by the examiner. It is slightly different from \u00a7\u00a0negativism in which the subject does exactly the opposite to what is asked in addition to showing resistance.\n Hemiasomatognosia is a subtype of anosognosia in which the person with hemiplegia neglects one half of their body.\n Hyposchemazia is characterized by the reduced awareness of one's body image and aschemazia by the absence of it. These disorders can have many varied causes such as physical injuries, mental disorders, or mental or physical states. These include transection of the spinal cord, parietal lobe lesions (e.g. right middle cerebral artery thrombosis), anxiety, depersonalization, epileptic auras, migraines, sensory deprivation, and vertigo (i.e. \"floating on air\").\n Id\u00e9e fixe is an alternate term for an overvalued idea. In this condition, a belief that might seem reasonable both to the individual and to other people comes to dominate completely the individual's thinking and life.\n Thoughts that one's own body part or action is not of one's own.\n Thoughts that one's own action is caused by someone else's will or some other external cause.\n Ideas of reference are a delusional belief that general events are personally directed at oneself.[14]\n An illusion is a false perception of a detectable stimulus.[1]\n Jargon aphasia is characterized by incoherent, meaningless speech with neologisms (newly invented words). These are unconscious thoughts that find expression when one is off one's guard and must be consciously repressed.\n In Kl\u00fcver\u2013Bucy syndrome, an individual will display placidity, hyperorality, hypersexuality, and hyperphagia. This condition results from bilateral destruction of the amygdaloid bodies of the limbic system.\n Knight's move thinking is a complete loosening of associations where there is no logical link between one idea and the next. Based on a knight on a chessboard where the movement can be any L shaped direction, making it difficult to track. Compare \u00a7\u00a0derailment.[24]\n Koro is a culture-specific syndrome, generally seen only among Chinese people. It involves a panicked feeling that one's genitals are retracting into the abdomen, and that this will result in death.[1]\n Kuru (also known as \"laughing sickness\" due to the outbursts of laughter that mark its second phase) was first noted in New Guinea in the early 1900s. Kuru is now known to be a prion disease, one of several known transmissible spongiform encephalopathies.\n Latah is a culture-specific syndrome usually seen in Southeast Asia and involves startle-induced disorganization, hypersuggestibility, automatic obedience, and echopraxia (a tendency to mimic examiner's or other person's actions). It is usually associated with women.\nThere is controversy over whether Latah is a real psychiatric condition, or merely a display of exhibitionism that would otherwise not be socially acceptable.\n In l'homme qui rit (from the French, meaning \"the man who laughs\"), an individual displays inappropriate laughter accompanied by release phenomena of the frontal subdominant lobe.\n Lilliputian hallucinations are characterized by abnormal perception of objects as being shrunken in size but normal in detail. Usually seen in delirium tremens.\n In logoclonia, the individual often repeats the last syllable of a word. Compare echolalia. Often a symptom of Alzheimers or Parkinson's disease.\n Logorrhoea, also known as \"volubility\", is characterized by fluent and rambling speech using numerous words. Compare \u00a7\u00a0derailment.\n Mania is often mirrored as a minor image of depression. Mania is a state of abnormally elevated arousal, affect, and energy level. As mania intensifies, irritability can be more pronounced and result in anxiety or violence. Mania symptoms are elevated mood, flights of ideas, pressure of speech, increased energy, decreased need or desire for sleep, and hyperactivity.\n Mania a potu is an alcohol intoxication state with violent and markedly disinhibited behavior. This condition is different from violent behavior in otherwise normal individuals who are intoxicated.\n Metonymy is a speech disturbance in which patients, commonly with schizophrenia, use inappropriate words or expressions that are related to the proper ones.  Examples include: consume a menu, instead of a meal; lose the piece of string of the conversation, not the thread of the conversation.  See also \u00a7\u00a0word approximation.[25][26]\n Mitgehen (German: [\u02c8m\u026at\u02cc\u0261e\u02d0\u0259n] \u24d8) is an extreme form of mitmachen in which very slight pressure leads to movement in any direction, also called the \"anglepoise\" effect or \"anglepoise lamp sign\". This movement occurs despite instructions to resist the pressure, as individuals with this condition often experience even slight pressure as forcible grasping and pushing.\n In mitmachen (German: [\u02c8m\u026at\u02ccmaxn\u0329] \u24d8), one's body can be put into any posture, despite instructions given to resist. Compare \u00a7\u00a0mitgehen.\n Moria is the condition characterized by euphoric behavior, such as frivolity and the inability to act seriously. In addition, there is a lack of foresight and a general indifference. It is found in frontal lobe lesions, often along with \u00a7\u00a0witzelsucht, particularly when the orbital surface is damaged. Recent research has shown its presence in frontotemporal dementia.\n Resistance to attempts to move the subject, who then does the opposite of what is asked. Negativism is usually a sign of catatonia, and may progress to (catatonic) rigidity. It is slightly different from \u00a7\u00a0gegenhalten, in which the individual resists movement but does not perform the opposite movement. Also see: oppositional defiance disorder (ODD).\n In a neurological or psychopathological context, neologisms are nonsensical words or phrases whose origins are unrecognizable, and are associated with aphasia or schizophrenia. Incorrectly constructed words whose origins are understandable may also be called neologisms, but are more properly referred as \u00a7\u00a0word approximations.[27][28]\n The omega sign is the occurrence of a fold (like the Greek letter omega, \u03a9 ) in the forehead, above the nose, produced by the excessive action of the corrugator muscle. It is sometimes seen in depression.\n From Greek oneiros as meaning \"dream\". In an oneiroid state one feels and behaves as though in a dream. Also known as \"oneirophrenia\" as described by Ladislas J. Meduna.\n See \u00a7\u00a0oneiroid state or oneirophrenia.\n Overvalued ideas are exaggerated beliefs that a person sustains beyond reasons, but are not as unbelievable and are not as persistently held as delusions.[29][30] \nPreoccupation with spouse's possible infidelity can be an overvalued idea if no evidence exists to arouse suspicion.\nBody dysmorphic disorder's obsessive preoccupation that some aspect of one's own appearance is severely flawed is another example of an overvalued idea.[29]\n Palilalia is characterized by the repetition of a word or phrase; i.e., the subject continues to repeat a word or phrase after once having said. It is a form of \u00a7\u00a0perseveration.\n In palinacousis the subject continues to hear a word, a syllable or any sound, even after the withdrawal of stimulus. It is a type of \u00a7\u00a0perseveration.\n In palinopsia a visual image persists after the stimulus has gone (similar to an afterimage seen after looking into a bright light).\n A Freudian slip, or parapraxis, is an error in speech, memory or physical action that is believed to be caused by the unconscious mind.\n A delusion in which a person believes they have seen a face transform into a grotesque form \u2013 often described as a 'monster', 'vampire', 'werewolf' or similar. This is very rare and most likely to be described by people with schizophrenia.\n Paraschemazia is characterized by a distortion of body image. It can be caused by hallucinogenic drugs such as LSD and mescalin, epileptic auras, and sometimes migraines.\n In pareidolia a vague or random stimulus is mistakenly perceived as recognizable. Pareidolia is a type of illusion and hence called \"pareidolic illusion\".\n This term refers to uncontrollable repetition of a particular response, such as a word, phrase, or gesture, despite the absence or cessation of the original stimulus.[31] Usually it is seen in organic disorders of brain, head injury, delirium or dementia, however can be seen in schizophrenia as well.\n This refers to schizophrenia in people with mild learning disabilities.[32]\n Piblokto, pibloktoq, or Arctic hysteria, is a condition exclusively appearing in Inuit societies living within the Arctic Circle. Appearing most prevalently in winter, it is considered to be a form of a culture-specific disorder.[33]\n Symptoms can include intense \"hysteria\" (including screaming and uncontrolled wild behavior), depression, coprophagia, and insensitivity to extreme cold.[34] This condition is most often seen in Inuit women.[35]\n Often associated with schizophrenia, dementia, and severe depression, poverty of ideas is a thought disturbance in which thought spontaneity and productivity are reduced, and are seen in speech that is vague, has many simple or meaningless repetitions, or full of stereotyped phrases.[36]\n Pseudologia fantastica is a condition in which a person grossly exaggerates their symptoms or even tells a lie about their symptoms in order to get medical attention. Seen in malingering and Munchausen syndrome.\n Where the individual holds their head a few centimetres above the bed. No explanation is offered for this. It is a symptom of catatonia and can last for many hours.\n Psychopathology is a term which refers to either the study of mental illness or mental distress or to\nthe manifestation of behaviours and experiences which may be indicative of mental illness or psychological impairment.\n Rabbit syndrome is characterized by rapid, vertical, rhythmic movements of lips so that it resembles a rabbit chewing.[15] It is a type of extrapyramidal symptom, distinct from tardive dyskinesia as it spares the tongue and involves vertical movements only.[37]\n In reduplicative hallucinations there is the perception of seeing a double. Particular kinds of reduplicative hallucination include autoscopy, heautoscopy and out-of-body experiences.\n Reduplicative paramnesia is a delusional misidentification syndrome in which one's surroundings are believed to exist in more than one physical location.\n Reflex hallucinations occur when true sensory input in one sense leads to production of a hallucination in another sense, e.g. seeing a doctor writing (visual) and then feeling him writing across one's stomach (tactile).\n Restlessness has two components: akathisia (subjective \"inner\" restlessness) and psychomotor agitation (an excess of motor activity).\n Mental retardation (more commonly referred to as intellectual disability[38][39]) is a term used when a person has certain limitations in mental functioning and in skills such as communicating, taking care of themselves, and social skills.\n In children, these limitations will cause a child to learn and develop more slowly than a typical child. Children with intellectual disability may take longer to learn to speak, walk, and take care of their personal needs such as dressing or eating. They are likely to have trouble learning in school. They will learn, but it will take them longer. There may be some things they cannot learn.[40]\n Left\u2013right disorientation is one of the four cardinal signs of Gerstmann's syndrome.\n Scanning speech is an ataxic dysarthria in which syllable durations are equalized. It is characteristic of the dysarthria of multiple sclerosis. Together with nystagmus and intention tremor it forms Charcot's triad 1.\n Schizophasia, commonly referred to as word salad, is confused, and often repetitious, language that is symptomatic of various mental illnesses.[41]\n A schnauzkrampf is a grimace resembling pouting sometimes observed in catatonic individuals.\n Sensitiver beziehungswahn, is an alternate term for ideas of reference. In this the person thinks as people are talking about them or observing them or a talk is going on about them on television or radio. Seen in social phobia, depression, delusional disorder and in schizophrenia where they are often present up to a delusional extent.\n The Stockholm syndrome is a psychological response sometimes seen in a hostage, in which the hostage exhibits loyalty to the hostage-taker, in spite of the danger (or at least risk) in which the hostage has been placed.[42] Stockholm syndrome is also sometimes discussed in reference to other situations with similar tensions, such as battered person syndrome,[43] child abuse cases, and bride kidnapping.\n Also spelled syn\u00e6sthesia, synaesthesia, or synesthesia\u2014plural synesthesiae, from the Greek syn- meaning \"union\" and aesthesis meaning \"sensation\", it is a neurological phenomenon in which two or more bodily senses are coupled.\n In telegraphic speech conjunctions and articles are missed out; meaning is retained and few words are used.\n Thought blocking, also referred to as thought withdrawal, refers to an abrupt stop in the middle of a train of thought; the individual might or might not be unable to continue the idea.[14] This is a type of formal thought disorder that can be seen in schizophrenia.[1]\n A combined term for \u00a7\u00a0gedankenlautwerden and \u00a7\u00a0\u00e9cho de la pens\u00e9e (\"thought echo\")\n Torpor in psychopathology is usually taken to mean profound inactivity not caused by reduction in consciousness.\n Tourette syndrome (abbreviated as TS or Tourette's) is a common neurodevelopmental disorder that begins in childhood or adolescence. It is characterized by multiple movement (motor) tics and at least one vocal (phonic) tic. Common tics are blinking, coughing, throat clearing, sniffing, and facial movements. These are typically preceded by an unwanted urge or sensation in the affected muscles, can sometimes be suppressed temporarily, and characteristically change in location, strength, and frequency. Tourette's is at the more severe end of a spectrum of tic disorders. The tics often go unnoticed by casual observers.\n Traumatic bonding occurs as the result of ongoing cycles of abuse in which the intermittent reinforcement of reward and punishment creates powerful emotional bonds that are resistant to change.[44]\n Also known as \"hair pulling disorder\", trichotillomania (TTM) is an impulse control disorder characterised by a long term urge that results in the pulling out of one's hair. This occurs to such a degree that hair loss can be seen. Efforts to stop pulling hair typically fail. Hair removal may occur anywhere; however, the head and around the eyes are most common. The hair pulling is to such a degree that it results in distress\n Verbigeration is a verbal stereotypy (repetition) in which usually one or several sentences or strings of fragmented words are repeated continuously. Sometimes individuals will produce incomprehensible jargon in which stereotypies are embedded. The tone of voice is usually monotonous. This can be produced spontaneously or precipitated by questioning. The term verbigeration was first used in psychiatry by Karl Kahlbaum in 1874, and it referred to a manner of talking which was very fast and incomprehensible. At the time verbigeration was seen as a \"disorder of language\" and represented a central feature of catatonia. The word is derived from the Latin word verbum (also the source of verbiage), plus the verb ger\u0115re, to carry on or conduct, from which the Latin verb verbiger\u0101re, to talk or chat, is derived. However, clinically the term verbigeration never achieved popularity and as such has virtually disappeared from psychiatric terminology. Compare Echolalia.[45]\n An ill-humored mood state often accompanied by low mood and depressive symptoms. The people surrounding the individual often feel upset by this condition.\n In vorbeigehen or vorbeireden, an individual will answer a question in such a way that it is clear the question was understood, though the answer itself is very obviously wrong. For example: \"How many legs does a dog have?\" \u2013 \"Six\".\nThis condition occurs in Ganser syndrome and has been observed in prisoners awaiting trial. Vorbeigehen (German: [fo\u02d0\u0250\u032f\u02c8ba\u026a\u032f\u02cc\u0261e\u02d0\u0259n] \u24d8, giving approximate answers) was the original term used by Ganser but Vorbeireden (talking past the point) is the term generally in use (Goldin 1955). This behavior is also seen in people trying to feign psychiatric disorders (hence its association with prisoners).[46]\n Wahneinfall is an alternate term for autochthonous delusions or delusional intuition. This is one of the types of primary delusions in which a firm belief comes into the individual's mind \"out of the blue\" or as an intuition, hence called \"delusional intuition\". Other types of primary delusions include delusional mood (or atmosphere), delusional (apophanous) perception and delusional memories. Care is taken not to impugn an otherwise-rational individual's instinctive aversion or inexpressible sense of or belief about a thing by dismissing it as wahneinfall.\n Waxy flexibility, also known as \u00a7\u00a0cerea flexibilitas, is characterized by an individual's movements having the feeling of a plastic resistance, as if the person were made of wax. This occurs in catatonic schizophrenia, and a person with this condition can have his limbs placed in fixed positions as if the person were in fact made from wax.\n Compare \u00a7\u00a0mitmachen and \u00a7\u00a0waxy flexibility.\n Windigo (also wendigo, windago, windiga, witiko, and numerous other variants) psychosis is a culture-bound disorder which involves an intense craving for human flesh and the fear that one will turn into a cannibal. This was alleged to have occurred among Algonquian Indian cultures.\n Witzelsucht is a tendency to tell inappropriate jokes and create excessive facetiousness and inappropriate or pointless humor. It is seen in frontal lobe disorders usually along with \u00a7\u00a0moria. Recent research has shown that it may also be seen in frontotemporal dementia.[47]\n Usage of words in an unconventional or inappropriate way (as in \u00a7\u00a0metonymy), or usage of new but understandable words that are conventionally constructed, contrasting with \u00a7\u00a0neologisms, which are new words whose origins cannot be understood.[48][49]\n Word salad (derived from the German: Wortsalat) is characterized by confused, and often repetitious, language with no apparent meaning or relationship attached to them. It is often symptomatic of various mental illnesses, such as psychoses, including schizophrenia. Compare \u00a7\u00a0derailment.\n W\u00fcrgstimme refers to speaking in an odd muffled or strangled voice. It is mainly seen in schizophrenia.\n Zeitraffer (German: [\u02c8t\u0361sa\u026a\u032ft\u02cc\u0281af\u0250] \u24d8) phenomenon, which translates to \"time-lapse\" in English, highlights how events, objects, and processes change and evolve over time, sometimes in ways that are imperceptible in real-time.\n From a philosophical perspective, Zeitraffer can be related to various philosophical themes:\n 1. Temporality: It raises questions about the nature of time, whether it is continuous or discrete, and how our perception of time affects our understanding of reality.\n 2. Impermanence: Zeitraffer reminds us of the transient nature of existence, emphasizing how everything is subject to change and decay.\n 3. Perception and Reality: It underscores the difference between how we perceive the world in real-time and how it actually changes over time, raising questions about the reliability of our senses and the nature of reality.\n Zeitlupenwahrnehmung phenomenon translates to \u201cslow motion perception\u201d in English\n \n One of the paraphilias, characterized by marked distress over, or acting on, urges to indulge in sexual activity that involves animals.\n",
      "timestamp": "2025-10-09 19:12:08.633719"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_probability_and_statistics",
      "text": "\nThis glossary of statistics and probability is a list of definitions of terms and concepts used in the mathematical sciences of statistics and probability, their sub-disciplines, and related fields. For additional related terms, see Glossary of mathematics and Glossary of experimental design.\n Also confidence coefficient. Also correlation coefficient. Also expectation, mathematical expectation, first moment, or simply mean or average. Also midspread, middle 50%, and H-spread. Also moving mean and rolling mean.",
      "timestamp": "2025-10-09 19:12:08.988820"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_probability_and_statistics",
      "title": "Glossary of probability and statistics - Wikipedia",
      "description": "",
      "text": "\nThis glossary of statistics and probability is a list of definitions of terms and concepts used in the mathematical sciences of statistics and probability, their sub-disciplines, and related fields. For additional related terms, see Glossary of mathematics and Glossary of experimental design.\n Also confidence coefficient. Also correlation coefficient. Also expectation, mathematical expectation, first moment, or simply mean or average. Also midspread, middle 50%, and H-spread. Also moving mean and rolling mean.",
      "timestamp": "2025-10-09 19:12:08.993680"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_physics",
      "text": "\n This glossary of physics is a list of definitions of terms and concepts relevant to physics, its sub-disciplines, and related fields, including mechanics, materials science, nuclear physics, particle physics, and thermodynamics. For more inclusive glossaries concerning related fields of science and technology, see Glossary of chemistry terms, Glossary of astronomy, Glossary of areas of mathematics, and Glossary of engineering.\n Also called the V-number or constringence. Also \u03b1-decay. Also symbolized by \u03b12+, He2+, and 42He2+. Often abbreviated as amp. Also electronic amplifier or (informally) amp. Also angular speed, radial frequency, circular frequency, orbital frequency, radian frequency, and pulsatance. Also (rarely) moment of momentum or rotational momentum. \n \n \n Also known as audible frequency (AF). Also Balmer lines. Also known as flexure. Also \u03b2-decay. Also polarization angle. Also pedesis. Also centigrade scale. Also Newtonian mechanics. Also mass density. Also simply current. Also simply conductor. Also simply insulator. Also simply resistance. Also abbreviated EM field or EMF. Also abbreviated EM radiation or EMR. Also abbreviated emf. Also called electron spin resonance (ESR) and electron magnetic resonance (EMR). Also fundamental interactions. Also gravity. Also universal gravitational constant and Newton's constant. Also called Kirchhoff's rules or simply Kirchhoff's laws. Also called streamline flow. Also abbreviated LRL vector. Also material balance. Also atomic mass number or nucleon number. Also called fusion. Also abbreviated as nanotech. Also spelled nucleide. Also simply called the periodic table. Also called Planck's constant. Also radioactive nuclide, radioisotope, or radioactive isotope. Also angular kinetic energy. Also speed of revolution. Also modulus of rigidity. Also strong force and strong nuclear force. Also moment or moment of force. Also weak force or weak nuclear force.",
      "timestamp": "2025-10-09 19:12:09.830561"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_physics",
      "title": "Glossary of physics - Wikipedia",
      "description": "",
      "text": "\n This glossary of physics is a list of definitions of terms and concepts relevant to physics, its sub-disciplines, and related fields, including mechanics, materials science, nuclear physics, particle physics, and thermodynamics. For more inclusive glossaries concerning related fields of science and technology, see Glossary of chemistry terms, Glossary of astronomy, Glossary of areas of mathematics, and Glossary of engineering.\n Also called the V-number or constringence. Also \u03b1-decay. Also symbolized by \u03b12+, He2+, and 42He2+. Often abbreviated as amp. Also electronic amplifier or (informally) amp. Also angular speed, radial frequency, circular frequency, orbital frequency, radian frequency, and pulsatance. Also (rarely) moment of momentum or rotational momentum. \n \n \n Also known as audible frequency (AF). Also Balmer lines. Also known as flexure. Also \u03b2-decay. Also polarization angle. Also pedesis. Also centigrade scale. Also Newtonian mechanics. Also mass density. Also simply current. Also simply conductor. Also simply insulator. Also simply resistance. Also abbreviated EM field or EMF. Also abbreviated EM radiation or EMR. Also abbreviated emf. Also called electron spin resonance (ESR) and electron magnetic resonance (EMR). Also fundamental interactions. Also gravity. Also universal gravitational constant and Newton's constant. Also called Kirchhoff's rules or simply Kirchhoff's laws. Also called streamline flow. Also abbreviated LRL vector. Also material balance. Also atomic mass number or nucleon number. Also called fusion. Also abbreviated as nanotech. Also spelled nucleide. Also simply called the periodic table. Also called Planck's constant. Also radioactive nuclide, radioisotope, or radioactive isotope. Also angular kinetic energy. Also speed of revolution. Also modulus of rigidity. Also strong force and strong nuclear force. Also moment or moment of force. Also weak force or weak nuclear force.",
      "timestamp": "2025-10-09 19:12:09.839697"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_bird_terms",
      "text": "\n The following is a glossary of common English language terms used in the description of birds\u2014warm-blooded vertebrates of the class Aves and the only living dinosaurs.[1] Birds, who have feathers and the ability to fly (except for the approximately 60 extant species of flightless birds), are toothless, have beaked jaws, lay hard-shelled eggs, and have a high metabolic rate, a four-chambered heart, and a strong yet lightweight skeleton.\n Among other details such as size, proportions and shape, terms defining bird features developed and are used to describe features unique to the class\u2014especially evolutionary adaptations that developed to aid flight. There are, for example, numerous terms describing the complex structural makeup of feathers (e.g., barbules, rachides and vanes); types of feathers (e.g., filoplume, pennaceous and plumulaceous feathers); and their growth and loss (e.g., colour morph, nuptial plumage and pterylosis).\n There are thousands of terms that are unique to the study of birds. This glossary makes no attempt to cover them all, concentrating on terms that might be found across descriptions of multiple bird species by bird enthusiasts and ornithologists. Though words that are not unique to birds are also covered, such as \"back\" or \"belly,\" they are defined in relation to other unique features of external bird anatomy, sometimes called \"topography.\" As a rule, this glossary does not contain individual entries on any of the approximately 11,000 recognized living individual bird species of the world.[2][3][a]\n \n The haunting call of the loon: \n \n The extraordinary song of the Kookaburra: \n",
      "timestamp": "2025-10-09 19:12:10.468560"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_bird_terms",
      "title": "Glossary of bird terms - Wikipedia",
      "description": "",
      "text": "\n The following is a glossary of common English language terms used in the description of birds\u2014warm-blooded vertebrates of the class Aves and the only living dinosaurs.[1] Birds, who have feathers and the ability to fly (except for the approximately 60 extant species of flightless birds), are toothless, have beaked jaws, lay hard-shelled eggs, and have a high metabolic rate, a four-chambered heart, and a strong yet lightweight skeleton.\n Among other details such as size, proportions and shape, terms defining bird features developed and are used to describe features unique to the class\u2014especially evolutionary adaptations that developed to aid flight. There are, for example, numerous terms describing the complex structural makeup of feathers (e.g., barbules, rachides and vanes); types of feathers (e.g., filoplume, pennaceous and plumulaceous feathers); and their growth and loss (e.g., colour morph, nuptial plumage and pterylosis).\n There are thousands of terms that are unique to the study of birds. This glossary makes no attempt to cover them all, concentrating on terms that might be found across descriptions of multiple bird species by bird enthusiasts and ornithologists. Though words that are not unique to birds are also covered, such as \"back\" or \"belly,\" they are defined in relation to other unique features of external bird anatomy, sometimes called \"topography.\" As a rule, this glossary does not contain individual entries on any of the approximately 11,000 recognized living individual bird species of the world.[2][3][a]\n \n The haunting call of the loon: \n \n The extraordinary song of the Kookaburra: \n",
      "timestamp": "2025-10-09 19:12:10.487873"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_nanotechnology",
      "text": "\n This glossary of nanotechnology is a list of definitions of terms and concepts relevant to nanotechnology, its sub-disciplines, and related fields.\n For more inclusive glossaries concerning related fields of science and technology, see Glossary of chemistry terms, Glossary of physics, Glossary of biology, and Glossary of engineering.\n",
      "timestamp": "2025-10-09 19:12:10.876221"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_nanotechnology",
      "title": "Glossary of nanotechnology - Wikipedia",
      "description": "",
      "text": "\n This glossary of nanotechnology is a list of definitions of terms and concepts relevant to nanotechnology, its sub-disciplines, and related fields.\n For more inclusive glossaries concerning related fields of science and technology, see Glossary of chemistry terms, Glossary of physics, Glossary of biology, and Glossary of engineering.\n",
      "timestamp": "2025-10-09 19:12:10.878443"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_mycology",
      "text": "\n This glossary of mycology is a list of definitions of terms and concepts relevant to mycology, the study of fungi. Terms in common with other fields, if repeated here, generally focus on their mycology-specific meaning. Related terms can be found in glossary of biology and glossary of botany, among others. List of Latin and Greek words commonly used in systematic names and Botanical Latin may also be relevant, although some prefixes and suffixes very common in mycology are repeated here for clarity.\n an- acidophilic pleuroacrogenous attached, adherent pl. aethalia Imperfect state apical veil pl. antheridia, antherid acro- apodal, apodous, sessile pl. apothecia, discocarp applanate pl. appresoria hydrofungi aerole thallic-arthric asco-, ascidi- ascocarp; pl. ascomata Ascomycetes, sac fungi pl. asci vegetative, somatic basidiocarp, pl. basidiomata Basidiomycetes pl. basidia gemmation sphaeridium carpo-, -carp Catenulate Chytridomycetes cirrhus; spore horn clamp, fibula pl. cleistothecia pl. columellae pl. conidiomata fertile hypha pl. conidia rind ascus crook crustaceous pl. cyphellae Cystidia ringworm, tinea dicaryotic, secondary mycelium dimorphism cup fungi eucarpous allochthonous Falciform zymosis filamentose flexuose basal cell fructicole fruticole mycetophagous fungous pl. fungi pl. gemmae pl. glebae guttiferous guttula Gymnomycetes pl. gymnothecia gyrose pl. haustoria heterocaryotic heterocont, Straminipila pl. hila homocaryotic pl. hyphae Hyphales resting spore protothallus pl. isidia isocont karya-, karyo-, cary-, carya-, caryo- caryogamy, nuclear fusion basal body pl. lamellae lanose lentiform lichenen, moss starch xylogenous luniform bioluminescent fungi macular, maculose pl. merosporangia micronemous Fungi imperfecti; Deuteromycetes; ana-holomorph; conidial fungi; asexual fungi mould, Micromycetes, microfungi monocaryotic mucose, mucous mycet-, myceto-, myco- pl. mycelia mycetismus, mushroom poisoning madura foot, maduramycosis funga pl. mycoses Myxomycetes vermivorous pl. oogonia Peronosporomycetes archil, orcein pl. opercula pl. paraphyses pl. penicilli pyrenocarp; pl. perithecia race, strain, biotype mushroom cap polymorphic potato late blight, potato murrain propagulum Pseudomycetes pl. pseudoparenchymata pl. pseudostromata Fuzzball, puff-ball punctate pl. pycnidia piriform fabiform Cryptomycota  saprogen, saprotroph sclerotia scutiform pl. septa pl. somata pl. soredia pl. sori spinuous apical body spori-, sporo-, -spore pl. sporangiola pl. sporangia fruit body, fruiting body pl. sterigmata pl. stromata pl. synnemata Perfect state pl. thalli torulous, torose, moniliform mycose, mushroom sugar tubercule tuberculate xylophagous fungus swarm spore, zo\u00f6spore Zygomycetes zymurgy",
      "timestamp": "2025-10-09 19:12:11.397526"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_mycology",
      "title": "Glossary of mycology - Wikipedia",
      "description": "",
      "text": "\n This glossary of mycology is a list of definitions of terms and concepts relevant to mycology, the study of fungi. Terms in common with other fields, if repeated here, generally focus on their mycology-specific meaning. Related terms can be found in glossary of biology and glossary of botany, among others. List of Latin and Greek words commonly used in systematic names and Botanical Latin may also be relevant, although some prefixes and suffixes very common in mycology are repeated here for clarity.\n an- acidophilic pleuroacrogenous attached, adherent pl. aethalia Imperfect state apical veil pl. antheridia, antherid acro- apodal, apodous, sessile pl. apothecia, discocarp applanate pl. appresoria hydrofungi aerole thallic-arthric asco-, ascidi- ascocarp; pl. ascomata Ascomycetes, sac fungi pl. asci vegetative, somatic basidiocarp, pl. basidiomata Basidiomycetes pl. basidia gemmation sphaeridium carpo-, -carp Catenulate Chytridomycetes cirrhus; spore horn clamp, fibula pl. cleistothecia pl. columellae pl. conidiomata fertile hypha pl. conidia rind ascus crook crustaceous pl. cyphellae Cystidia ringworm, tinea dicaryotic, secondary mycelium dimorphism cup fungi eucarpous allochthonous Falciform zymosis filamentose flexuose basal cell fructicole fruticole mycetophagous fungous pl. fungi pl. gemmae pl. glebae guttiferous guttula Gymnomycetes pl. gymnothecia gyrose pl. haustoria heterocaryotic heterocont, Straminipila pl. hila homocaryotic pl. hyphae Hyphales resting spore protothallus pl. isidia isocont karya-, karyo-, cary-, carya-, caryo- caryogamy, nuclear fusion basal body pl. lamellae lanose lentiform lichenen, moss starch xylogenous luniform bioluminescent fungi macular, maculose pl. merosporangia micronemous Fungi imperfecti; Deuteromycetes; ana-holomorph; conidial fungi; asexual fungi mould, Micromycetes, microfungi monocaryotic mucose, mucous mycet-, myceto-, myco- pl. mycelia mycetismus, mushroom poisoning madura foot, maduramycosis funga pl. mycoses Myxomycetes vermivorous pl. oogonia Peronosporomycetes archil, orcein pl. opercula pl. paraphyses pl. penicilli pyrenocarp; pl. perithecia race, strain, biotype mushroom cap polymorphic potato late blight, potato murrain propagulum Pseudomycetes pl. pseudoparenchymata pl. pseudostromata Fuzzball, puff-ball punctate pl. pycnidia piriform fabiform Cryptomycota  saprogen, saprotroph sclerotia scutiform pl. septa pl. somata pl. soredia pl. sori spinuous apical body spori-, sporo-, -spore pl. sporangiola pl. sporangia fruit body, fruiting body pl. sterigmata pl. stromata pl. synnemata Perfect state pl. thalli torulous, torose, moniliform mycose, mushroom sugar tubercule tuberculate xylophagous fungus swarm spore, zo\u00f6spore Zygomycetes zymurgy",
      "timestamp": "2025-10-09 19:12:11.412200"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_meteorology",
      "text": "\nThis glossary of meteorology is a list of terms and concepts relevant to meteorology and atmospheric science, their sub-disciplines, and related fields.\n Also actiniform. Also adiabatic warming. Also barometric pressure. Sometimes called aerology. Also simply called an area forecast. Also baroclinicity. Also barotropicity. Also clear ice. Also blocking high and blocking anticyclone. Also standing cloud. Also castellatus. Also pilot balloon or pibal. Also climate science. Also irisation. Also cloud genus. Also saddle point and neutral point. Also cold spell and cold snap. Also vortex Crow instability. Also red adaptation goggles. Also daybreak. Also dewpoint or dew-point. Also non-adiabatic process. Also simply diffuse radiation. Also diurnal range. Also drouth. Also heat storm. Also duster or duststorm. Also atmometer. Also fetch length. Also fire devil and fire tornado. Also pyrocumulus and fire cloud. Also beaver's tail. Also white rainbow, mist bow, and cloud bow. Also foehn wind. Also front-flank downdraft. Often used interchangeably with scud. Also simply called the F scale. Also glazed frost. Also soft hail and snow pellets. Also gust front tornado. Also tropical cell. Also Lower Atmosphere Severity Index. Also apparent temperature, felt air temperature, and humiture. Also velocity diagram. Also huayco. Also the doldrums or the calms. Also simply jet. Also jet stream core or jet maximum. Also George's index. Also catabatic wind, drainage wind, or fall wind. Also chamsin, hamsin, and khamaseen. Also lee depression, orographic depression, and dynamic trough. Also rendered as LIDAR, LiDAR, or LADAR. Also maritime climate. Also moisture content or water content. Also mudslide. Also mother-of-pearl cloud. Also northeaster. Also Canterbury arch; associated with nor'wester. Also octa. Also orographic uplift. Also gust front. Also the ozone shield and ozonosphere. Also scud; often used interchangeably with fractus. Also sun dog or mock sun. Also Pascal's principle. Also Fujita-Pearson scale or F-P-P scale. Also cap cloud or scarf cloud. Also polar anticyclone. Also polar-air depression. Also total precipitable water (TPW). Also psychrometry and hygrometry. Also solarimeter. Also stratospheric oscillation. Also geostrophic approximation and pseudogeostrophic approximation. Also radio-sounding device. Also wet season and green season. Also udometer, pluviometer, and ombrometer. Often simply showers. Also retrograde motion. Also wedge. Also long wave or planetary wave. Also simply called the Saffir\u2013Simpson scale. (sing.) sastruga; also spelled zastrugi Also moist adiabat. Also moist adiabatic lapse rate. Also ocean surface temperature. Also arcus cloud. Sometimes stylized as SKYWARN. Also snow bale or snow donut. Often used interchangeably with winter storm. Also rocketsonde, research rocket, and suborbital rocket. Also spoondrift. Also simply Storm Data. Also plough wind, thundergust, and hurricane of the prairie. Also cauda. Also thermal column. Also electrical storm and lightning storm. Also twister, whirlwind, and cyclone. Also debris cloud or debris ball. Also extended tornado outbreak. Variously hurricane, typhoon, tropical storm, cyclonic storm, or simply cyclone. Also vertical draft. (pl.) vortices or vortexes Also murus and pedestal cloud. Also sounding balloon. Also wind vane and weathercock. Also wind chill index and wind chill factor. Sometimes used interchangeably with wind gradient. Often used interchangeably with snowstorm. Also snowspout. Also spelled dzud.",
      "timestamp": "2025-10-09 19:12:11.923290"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_meteorology",
      "title": "Glossary of meteorology - Wikipedia",
      "description": "",
      "text": "\nThis glossary of meteorology is a list of terms and concepts relevant to meteorology and atmospheric science, their sub-disciplines, and related fields.\n Also actiniform. Also adiabatic warming. Also barometric pressure. Sometimes called aerology. Also simply called an area forecast. Also baroclinicity. Also barotropicity. Also clear ice. Also blocking high and blocking anticyclone. Also standing cloud. Also castellatus. Also pilot balloon or pibal. Also climate science. Also irisation. Also cloud genus. Also saddle point and neutral point. Also cold spell and cold snap. Also vortex Crow instability. Also red adaptation goggles. Also daybreak. Also dewpoint or dew-point. Also non-adiabatic process. Also simply diffuse radiation. Also diurnal range. Also drouth. Also heat storm. Also duster or duststorm. Also atmometer. Also fetch length. Also fire devil and fire tornado. Also pyrocumulus and fire cloud. Also beaver's tail. Also white rainbow, mist bow, and cloud bow. Also foehn wind. Also front-flank downdraft. Often used interchangeably with scud. Also simply called the F scale. Also glazed frost. Also soft hail and snow pellets. Also gust front tornado. Also tropical cell. Also Lower Atmosphere Severity Index. Also apparent temperature, felt air temperature, and humiture. Also velocity diagram. Also huayco. Also the doldrums or the calms. Also simply jet. Also jet stream core or jet maximum. Also George's index. Also catabatic wind, drainage wind, or fall wind. Also chamsin, hamsin, and khamaseen. Also lee depression, orographic depression, and dynamic trough. Also rendered as LIDAR, LiDAR, or LADAR. Also maritime climate. Also moisture content or water content. Also mudslide. Also mother-of-pearl cloud. Also northeaster. Also Canterbury arch; associated with nor'wester. Also octa. Also orographic uplift. Also gust front. Also the ozone shield and ozonosphere. Also scud; often used interchangeably with fractus. Also sun dog or mock sun. Also Pascal's principle. Also Fujita-Pearson scale or F-P-P scale. Also cap cloud or scarf cloud. Also polar anticyclone. Also polar-air depression. Also total precipitable water (TPW). Also psychrometry and hygrometry. Also solarimeter. Also stratospheric oscillation. Also geostrophic approximation and pseudogeostrophic approximation. Also radio-sounding device. Also wet season and green season. Also udometer, pluviometer, and ombrometer. Often simply showers. Also retrograde motion. Also wedge. Also long wave or planetary wave. Also simply called the Saffir\u2013Simpson scale. (sing.) sastruga; also spelled zastrugi Also moist adiabat. Also moist adiabatic lapse rate. Also ocean surface temperature. Also arcus cloud. Sometimes stylized as SKYWARN. Also snow bale or snow donut. Often used interchangeably with winter storm. Also rocketsonde, research rocket, and suborbital rocket. Also spoondrift. Also simply Storm Data. Also plough wind, thundergust, and hurricane of the prairie. Also cauda. Also thermal column. Also electrical storm and lightning storm. Also twister, whirlwind, and cyclone. Also debris cloud or debris ball. Also extended tornado outbreak. Variously hurricane, typhoon, tropical storm, cyclonic storm, or simply cyclone. Also vertical draft. (pl.) vortices or vortexes Also murus and pedestal cloud. Also sounding balloon. Also wind vane and weathercock. Also wind chill index and wind chill factor. Sometimes used interchangeably with wind gradient. Often used interchangeably with snowstorm. Also snowspout. Also spelled dzud.",
      "timestamp": "2025-10-09 19:12:11.934944"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_medicine",
      "text": "\n This glossary of medicine includes definitions of medical terminology and other terms pertaining to medicine and related fields.\n \n",
      "timestamp": "2025-10-09 19:12:13.005712"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_medicine",
      "title": "Glossary of medicine - Wikipedia",
      "description": "",
      "text": "\n This glossary of medicine includes definitions of medical terminology and other terms pertaining to medicine and related fields.\n \n",
      "timestamp": "2025-10-09 19:12:13.022059"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_mechanical_engineering",
      "text": "\nMost of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself.  However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together.  You can help enhance this page by adding new terms or writing definitions for existing ones.\n This glossary of mechanical engineering terms pertains specifically to mechanical engineering and its sub-disciplines. For a broad overview of engineering, see glossary of engineering.\n",
      "timestamp": "2025-10-09 19:12:13.449664"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_mechanical_engineering",
      "title": "Glossary of mechanical engineering - Wikipedia",
      "description": "",
      "text": "\nMost of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself.  However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together.  You can help enhance this page by adding new terms or writing definitions for existing ones.\n This glossary of mechanical engineering terms pertains specifically to mechanical engineering and its sub-disciplines. For a broad overview of engineering, see glossary of engineering.\n",
      "timestamp": "2025-10-09 19:12:13.455834"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_areas_of_mathematics",
      "text": "\n Mathematics is a broad subject that is commonly divided in many areas  or branches that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.\n This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics \u00a7\u00a0Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.\n Also called infinitesimal calculus Also called absolute differential calculus.",
      "timestamp": "2025-10-09 19:12:13.860811"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_areas_of_mathematics",
      "title": "Glossary of areas of mathematics - Wikipedia",
      "description": "",
      "text": "\n Mathematics is a broad subject that is commonly divided in many areas  or branches that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.\n This glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics \u00a7\u00a0Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.\n Also called infinitesimal calculus Also called absolute differential calculus.",
      "timestamp": "2025-10-09 19:12:13.866890"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_machine_vision",
      "text": "\n The following are common definitions related to the machine vision field.\n General related fields\n \n where \n\n\n\n\nf\n\no\n\n\n\n\n{\\displaystyle f_{o}}\n\n is the resonant frequency, \n\n\n\n\n\nE\n\n\n\n\n{\\displaystyle {\\mathcal {E}}}\n\n is the stored energy in the cavity, and \n\n\n\nP\n=\n\u2212\n\n\n\nd\nE\n\n\nd\nt\n\n\n\n\n\n{\\displaystyle P=-{\\frac {dE}{dt}}}\n\n is the power dissipated. The optical Q is equal to the ratio of the resonant frequency to the bandwidth of the cavity resonance. The average lifetime of a resonant photon in the cavity is proportional to the cavity's Q. If the Q factor of a laser's cavity is abruptly changed from a low value to a high one, the laser will emit a pulse of light that is much more intense than the laser's normal continuous output. This technique is known as Q-switching.\n",
      "timestamp": "2025-10-09 19:12:14.217166"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_machine_vision",
      "title": "Glossary of machine vision - Wikipedia",
      "description": "",
      "text": "\n The following are common definitions related to the machine vision field.\n General related fields\n \n where \n\n\n\n\nf\n\no\n\n\n\n\n{\\displaystyle f_{o}}\n\n is the resonant frequency, \n\n\n\n\n\nE\n\n\n\n\n{\\displaystyle {\\mathcal {E}}}\n\n is the stored energy in the cavity, and \n\n\n\nP\n=\n\u2212\n\n\n\nd\nE\n\n\nd\nt\n\n\n\n\n\n{\\displaystyle P=-{\\frac {dE}{dt}}}\n\n is the power dissipated. The optical Q is equal to the ratio of the resonant frequency to the bandwidth of the cavity resonance. The average lifetime of a resonant photon in the cavity is proportional to the cavity's Q. If the Q factor of a laser's cavity is abruptly changed from a low value to a high one, the laser will emit a pulse of light that is much more intense than the laser's normal continuous output. This technique is known as Q-switching.\n",
      "timestamp": "2025-10-09 19:12:14.219335"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_ichthyology",
      "text": "\n This glossary of ichthyology is a list of definitions of terms and concepts used in ichthyology, the study of fishes.[1]\n",
      "timestamp": "2025-10-09 19:12:14.660778"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_ichthyology",
      "title": "Glossary of ichthyology - Wikipedia",
      "description": "",
      "text": "\n This glossary of ichthyology is a list of definitions of terms and concepts used in ichthyology, the study of fishes.[1]\n",
      "timestamp": "2025-10-09 19:12:14.663031"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_geology",
      "text": "\n This glossary of geology is a list of definitions of terms and concepts relevant to geology, its sub-disciplines, and related fields. For other terms related to the Earth sciences, see Glossary of geography terms (disambiguation).\n Also called Indianite. Also called a composite gem. Also called a coccolithophorid. Also spelled dyke. Also spelled gemmology. Also called a gem, fine gem, jewel, precious stone, or semi-precious stone. Also geologic time scale. Also spelled greywacke. Also simply called a lithic. Also spelled luster. Also simply called the Mohs scale. Also called an extensional fault. Also spelled Paleozoic. Also called a thrust fault. Since 1982, officially called titanite by the International Mineralogical Association. Also called scree. Also called glacial till. Sometimes used interchangeably with ultrabasic. Also called the Hercynian orogeny. Also called lignite or brown coal. Also urania. Also called the tensile modulus. \n",
      "timestamp": "2025-10-09 19:12:15.099309"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_geology",
      "title": "Glossary of geology - Wikipedia",
      "description": "",
      "text": "\n This glossary of geology is a list of definitions of terms and concepts relevant to geology, its sub-disciplines, and related fields. For other terms related to the Earth sciences, see Glossary of geography terms (disambiguation).\n Also called Indianite. Also called a composite gem. Also called a coccolithophorid. Also spelled dyke. Also spelled gemmology. Also called a gem, fine gem, jewel, precious stone, or semi-precious stone. Also geologic time scale. Also spelled greywacke. Also simply called a lithic. Also spelled luster. Also simply called the Mohs scale. Also called an extensional fault. Also spelled Paleozoic. Also called a thrust fault. Since 1982, officially called titanite by the International Mineralogical Association. Also called scree. Also called glacial till. Sometimes used interchangeably with ultrabasic. Also called the Hercynian orogeny. Also called lignite or brown coal. Also urania. Also called the tensile modulus. \n",
      "timestamp": "2025-10-09 19:12:15.106625"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Oikonyms_in_Western_and_South_Asia",
      "text": "Oikonyms in Western, Central, South, and Southeast Asia can be grouped according to various components, reflecting common linguistic and cultural histories.[1] Toponymic study is not as extensive as it is for placenames in Europe and Anglophone parts of the world, but the origins of many placenames can be determined with a fair degree of certainty.[2][3]\nOne complexity to the study when discussing it in English is that the Romanization of names, during British rule and otherwise, from other languages has not been consistent.[2]\n In Rajasthan, names are frequently given after rock, stone, ravine, and embankment. In the Gangetic plain, the predominant natural features are trees, grass prairies, and bodies of water. Prominent trees, visible from a long way off, would often serve as landmarks and give their name to places before there was any permanent settlement there. This was especially the case where a large tree indicated a ford across a river; for example, the name Gaigh\u0101\u1e6d indicates a ford next to an agai tree. Tree names are especially common in areas that were historically under dense forest cover until recent centuries.[4]:\u200a23\u200a\n Common affixes used in South Asian oikonyms can be grouped based on their linguistic origin (with examples from India, Bangladesh, Pakistan, Nepal, and elsewhere such as in Sanskrit-influenced Indonesia): \n Means hamlet[5] \u2014 e.g. Dombivli; Kasan Wala; Sandhilianwali; Gujranwala; Tiruchirappalli\n Means \"fort\"[6][5] \u2014 Pathankot; Sialkot\n Means \"city\", or \"city of\"[7] \u2014 e.g. Visakhapatnam'\n Means \"abode\"; from Sanskrit \u0101-laya. e.g. Meghalaya, Himalaya,  Lok\u0101-laya (settlement).\n These suffixes are very common, especially -aul\u012b. In many cases, they are probably derived from Sanskrit palli, referring to a hamlet or small village. For example, B\u0101r\u1e0dol\u012b in Gujarat is attested in a Rashtrakuta-era inscription as V\u0101ra\u1e0dapallik\u0101. Names with these suffixes may also come from Sanskrit valli, meaning \"section\" or \"part\"; either origin is plausible.[8]:\u200a72\u200a[9]:\u200a53\u20134,\u200a64\u200a\n At some point, it seems that -aul\u012b became regarded as a distinct morpheme by itself, and apparently used independently as a suffix without being derived from an earlier form. For example,  the names Shamsaul\u012b and Shekhauliy\u0101 must have coined after the Muslim conquest to simply mean something like \"Shams ud-D\u012bn's village\".[8]:\u200a67,\u200a75\u200a\n The form -aul\u012b also seems to have become standardised and absorbed similar forms by analogy. For example, Dubaul\u012b (from D\u016bbe) is a common village name in eastern Uttar Pradesh, but it is not the regular, expected form of the name. The regular form would be Dubel\u012b, which exists but is far less common. In most cases, the name was assimilated to -aul\u012b by analogy with other places with names ending in -aul\u012b.[8]:\u200a75\u200a\n Means bamboo, from Sanskrit va\u1e43\u015ba. It was historically common for villages to be surrounded by bamboo groves that were planted as a form of defence. In many cases, it can be hard to distinguish between places named with b\u0101ns from places named with b\u0101s (\"dwelling\"), since b\u0101s sometimes becomes nasalised and b\u0101ns sometimes becomes de-nasalised. Examples of places named with b\u0101ns are B\u0101nsg\u0101on and B\u0101ns\u012b.[4]:\u200a36\u20137\u200a\n The names ba\u1e5b and bargad both refer to the banyan tree, ultimately from Sanskrit va\u1e6da.[4]:\u200a25\u20136\u200a This is a very common place name element; according to Sankalia, many towns and villages may have originally started out as temporary shelters underneath the wide canopy of a banyan tree. As they grew into more permanent settlements, they kept the name.[9]:\u200a88\u200a Ba\u1e5b has the common variations ba\u1e0d and ba\u1e6d.[4]:\u200a26\u200a Another variant is va\u1e0d, as in Va\u1e0dodar\u0101.[9]:\u200a88\u200a\n A common prefix, especially in eastern Uttar Pradesh.[10]:\u200a72\u200a The Oxford Hindi-English Dictionary defines chak (\u091a\u0915) with several meanings, including \"a piece of assigned or rent-free land\"; \"the detached or unconsolidated fields of a village\"; and simply \"a sub-division of land\". It derives the term from Sanskrit chakra, meaning \"circle\".[11]:\u200a296\u200a Whalley, on the other hand, preferred a derivation from Persian chak, noting that \"Chak\" is frequently followed by a Muslim name.[10]:\u200a72\u200a\n Means village, land, country; from Sanskrit \u0926\u0947\u0936 (desa) for \"space\"[12][13] \u2014 e.g. Bangladesh. In Indonesia it becomes Desa which is another Indonesian word for \"village\". \n In many cases, this ending is probably a \"worn-down\" descendant of earlier -khe\u1e5ba (\"village\").[4]:\u200a2\u200a\n Means fortress[14] \u2014 Chandigarh, Ramgarh\n According to Whalley, Ga\u1e5bh\u012b when used as a prefix probably in most cases originally referred to a village surrounded by a ditch.[10]:\u200a74\u200a\n In many cases, the place name element Ma\u016b (or mai) may be derived from Sanskrit mary\u0101d\u0101, meaning \"shore\" or \"bank\". This name is usually given to places by a river, stream, or jhil (for example, \u1e0calma\u016b on the Ga\u1e45g\u0101). Examples of these names are Argha\u1e6dma\u016b (\"the bank or shore where the water-wheel is\"), Bhainsmai (\"shore where cattle or horses graze\"), or Pathr\u0101mai (\"stony shore\"). In other cases, ma\u016b is a contraction of mahu\u0101: the mahua tree, Madhuca longifolia.[4]:\u200a4\u20136\u200a\n Some places have Ma\u016b as a standalone name (for example, Ma\u016b, Uttar Pradesh), while in other cases -ma\u016b is a suffix or even a prefix. The name M\u0101wai is a variant of Ma\u016b.[4]:\u200a4\u20136\u200a\n Means city, land, country, village;[6] from Sanskrit \u0928\u0917\u0930 (nagara) \u2014 e.g. Ahmednagar, Biratnagar. In Indonesian, the word Negara means \"country\" and the word Nagari is a term used in West Sumatra referring to \"village\". Also used in Borneo island, e.g. Negara Brunei Darussalam\n Many modern names using nagar in full are relatively recent origin; older names with nagar have often been shortened to n\u0101r or ner.[9]:\u200a73\u200a\n At least in northern India, nagar is not used as a prefix. Instead, the forms Nagl\u0101 or, more rarely, Nagr\u0101, are used. About 100 places also have the feminine forms Nagariy\u0101 and Nagariy\u0101.[10]:\u200a71\u200a\n Apparently derived from Sanskrit niv\u0101sa, \"dwelling\", combined with the Persian name Nau\u0101b\u0101d (\"new settlement\"). Naw\u0101d\u0101, along with its feminine variant Naw\u0101diy\u0101, is a very common village name by itself, and it is also used as a prefix for other names.[10]:\u200a74\u200a\n In many cases, this ending is probably a \"worn-down\" descendant of earlier -g\u0101on (\"village\") or -ban (\"forest\").[4]:\u200a2\u200a\n The Sanskrit term padra denoted a roadside village or residence (related to pad, meaning \"foot\"). Beginning around the 5th century, a regular sound change took place where /p/ became /v/ between vowels, turning this suffix into -vadra in many place names. In many modern place names, -vadra has further morphed into -dar\u0101. For example, Va\u1e0dodar\u0101 is from an earlier attested form Va\u1e6dapadra, Talodr\u0101 is from Talapadra or Talapadraka, and L\u0101\u1e6dhodr\u0101 is from L\u0101\u1e6dhivadra (attested in Chaulukya-era epigraphy). Similar names like Sa\u1e0dodar\u0101 and Ra\u1e47odar\u0101 probably share the same origin, although their older forms are not directly attested.[9]:\u200a51\u20133,\u200a61\u20133,\u200a76\u20137\u200a\n Pah\u0101\u1e5b, with the retroflex \u1e5b, means a hill, cliff, or overhanging river bank. Pah\u0101r with a non-retroflex r is a personal name, derived from Sanskrit prah\u0101ra. It can be hard to tell these place name elements apart because they can be easily confused in other scripts.[8]:\u200a55\u200a\n From Hindi pa\u1e6d\u1e6d\u012b, meaning \"strip\", itself derived from Sanskrit pa\u1e6d\u1e6dik\u0101. As a place name element, it is used in the sense of \"a strip of land\". In some cases it refers to a share of land held in joint tenure by a pattidar (literally \"shareholder\").[15]\n These are all names for the pilkhan tree, one of several varieties of fig tree viewed as sacred in Hinduism. The forms pilkhu and pilkhan come from Sanskrit plak\u1e63\u0101, while p\u0101ka\u1e5b and p\u0101kha\u1e5b come from Sanskrit #Sanskrit parka\u1e6d\u012b. One place with this name is Pilkhuw\u0101.[4]:\u200a27\u200a\n literally \"lump\" or a small altar of sand[6]\n The p\u012bpal tree, Ficus religiosa, is a common place name element.[4]:\u200a26\u20137\u200a\n Means village, town, state, country;[6] from Sanskrit \u092a\u0941\u0930 (pura) \u2014 e.g. Jamalpur; Kanpur; Khanpur. In Southeast Asian and some south Asian countries, it is known as pura, e.g. Anuradhapura,  Singapura, and Indonesian cities such as Jayapura, Siak Sri Indrapura, etc. In Indonesia, pura also refers to a Hindu temple.[16]\n In ancient times, the word pura strictly referred to a fort, but its meaning was gradually broadened to include any town regardless of its particular function. By the early medieval period, pura was often used to denote a commercial centre \u2013 especially in southern India, where the typical form was puram.[17]:\u200a68\u20139\u200a\n In many cases, old names originally ending in -pura have become shortened to -or over the centuries. In the case of Mangrol (originally Ma\u1e45galapura), the suffix has become -rol instead.[9]:\u200a71\u20133\u200a\n The variant pur\u0101 often originally referred to a suburb, or to a Muslim colony.[9]:\u200a72\u200a\n Pur is not used as a prefix. Instead, the form Pur\u0101 is used. In west-central Uttar Pradesh, around Kanpur and Etawah, the prefix takes the form Purw\u0101. Farther east, toward Basti, it takes the form Pure. The feminine form Pur\u012b is rarely found as a prefix.[10]:\u200a71\u20132\u200a\n Many places are named after the semal tree. There are many variations of this place name. One place with this name is Sambhal, where the form sambal ended up becoming aspirated.[4]:\u200a29\u200a\n According to Sankalia, this suffix has two possible origins: from -p\u0101\u1e6daka, which originally designated \"a large, but private house, or settlement within a village\"; and -v\u0101\u1e6daka, which denoted \"a temporarily enclosed place, such as a garden, plantation, or an enclosure of a (low caste) village consisting of boundary trees\". The shortened form p\u0101\u1e0d\u0101 appears early on in Ardham\u0101gadh\u012b Prakrit, and in early Jain literature refers to a suburb of a larger town. In Gujarat, the present form -v\u0101\u1e0d\u0101 first appears in inscriptions dating to the Chaulukya period. -V\u0101\u1e0d\u0101 continued to be used productively to form new place names; it would have been originally given to private settlements \"characterised either by a personal name or a prominent physiographical feature\". Modern names ending in -v\u0101\u1e0d\u0101 are descended from either ancient names that originally ended in either -p\u0101\u1e6daka or -v\u0101\u1e6daka, or more recent names that originally ended in -v\u0101\u1e0d\u0101.[9]:\u200a56\u20137,\u200a59,\u200a66\u20137\u200a\n An example is Delv\u0101\u1e0d\u0101. This name is attested in a Maitraka inscription as Devakula-p\u0101\u1e6daka, which would have later been contracted to *Devalv\u0101\u1e0d\u0101 and then De\u00fclav\u0101\u1e0d\u0101 (which is attested in a Chaulukya inscription) before finally reaching the present form.[9]:\u200a66\u20137\u200a\n In Maharasthra, the term v\u0101\u1e0d\u0101 refers to a built-up area, with or without an enclosure, belonging to a private citizen.[9]:\u200a59\u200a\n From Sanskrit, meaning \"dwelling\" or \"residence\" (of either an individual or a group). This suffix is especially common in northern Gujarat. Some places, such as Jetalvasana, contain the entire suffix without any modification. Others, like Chadasana, Jhulasan, Lunasan, Nandasan, and Ranasan (all of which are mentioned in medieval inscriptions with the suffix -vasa\u1e47a), have had the suffix modified to -sa\u1e47(\u0101) or -san(\u0101) over time.[9]:\u200a58,\u200a69\u201370\u200a\n From Sanskrit v\u0101\u1e6dik\u0101, meaning \"orchard\" or \"garden\". Commonly paired with tree names, e.g. Sisw\u0101r\u012b. Some examples with tribal names are also found; these are probably references to an individual person; examples are Bharw\u0101r\u012b and Lodhw\u0101r\u012b.[4]:\u200a25,\u200a29,\u200a60\u200a\n (\u0622\u0628\u0627\u062f): - -abad is a Persian \"dwelling of\" or \"town of\", combined with a person's or group's name (usually the founder or primary inhabitant(s))[6][18] \u2014 e.g. Hyderabad; Islamabad; Mirza Abad; Ashgabat;  Leninabad; Vagharshapat; Sardarabad; Sardarapat . Being a generic and an ambiguous term referring to small isolated farms, village (but not city) on one hand, and towns and cities, on the other hand.[19][20][21] See also abadi (settlement).\n Means \"port\" (wikt:\u0628\u0646\u062f\u0631) \u2014 e.g. Bandar Abbas; see All pages with titles containing Bandar\n Means field, desert (wikt:\u062f\u0634\u062a) \u2014 e.g. Hulandasht; see All pages with titles containing dasht\n From Perso-Arabic i\u1e25tim\u0101l, meaning \"probability\". In historical South Asian revenue terminology, Ihtimali referred to flood-prone lands along river banks or in low-lying areas. Ghair Ihtimali meant the opposite, i.e. not liable to flooding during the rainy season. These were used in place names to distinguish two villages with the same name, such as Todarpur Ihtimali and Todarpur Ghair Ihtimali in present-day Aligarh district, India.[22]\n From Arabic kh\u0101\u1e63\u1e63, meaning \"selected\" or \"private\". In India, it was historically used to refer to a place managed directly by the government or by a jagirdar, without any intermediaries. For example, Jamal Mohd Siddiqi identifies six places with \"kh\u0101s\" in their name in present-day Aligarh district, India. All six were founded by Rajput chiefs during the Mughal period, and they all occupy a prominent position on high ground. Kh\u0101s is also sometimes used in cases where there are two villages with the same name; in this case, kh\u0101s is affixed to the older and/or larger one.[23]\n Means \"neighborhood\" (wikt:\u06a9\u0648\u06cc) \u2014 e.g. Kordkuy; see All pages with titles containing kuy\n (in various languages) shrine, grave, tomb, etc. (from wikt:\u0645\u0632\u0627\u0631), cf. \"Mazar (mausoleum)\". The placename usually refers to a grave of a saint, ruler, etc.: Mazar-i-Sharif; see All pages with titles containing Mazar\n Derived from Arabic mazra\u0295, which originally refers to a farm field. In parts of India, though, the term refers to a hamlet or cluster of houses that is separate from, but subordinate to, a larger village. (The reason for the hamlet's separation is so that farmers can be closer to their crops.) Places with Majra in their name typically originated in this manner and later became independent villages of their own.[24]\n Derived from Arabic milk, meaning \"possession\" or \"property\". Like chak, it was historically used to designate a rent-free piece of land. Milk in particular usually designated land held by Muslim zamindars.[25]\n Derived from Perso-Arabic munzabt, meaning \"confiscated\". For example, the village of Raipur Munzabtah in Aligarh district got its name because it was confiscated by the British government after its pattidar participated in the Indian Rebellion of 1857.[25]\n From Arabic mutafarriq\u0101t, literally meaning \"miscellaneous\". This was used historically to denote a fiscal or administrative unit consisting of various scattered pieces of land. Villages called \"mutafarriqat\" are so named because they belonged to such a unit.[23]\n Derived from Arabic ni\u1e63f, meaning \"half\". For example, the village of Marhauli Nisfi Ashrafabad in present-day Aligarh district was formed by taking out a half portion from Ashrafabad.[25]\n From Perso-Arabic ra'iyyat, meaning \"subjects, peasants, cultivators\". It is used, for example, in the name of Lalpur Raiyyatpur in present-day Aligarh district, which likely originated as a settlement of peasants under the zamindar of nearby Lalpur.[26]\n Means \"city\"[6] \u2014 e.g. Bulandshahr\n Means fort, fortress, castle;[6] see also \"Qalat (fortress)\" \u2014 e.g. Makhachkala, Akhalkalaki, Solzha-Ghala, Dzaudzhikau\n Persian-Urdu, taken to mean neighborhood in Indian context. For example, Daryaganj, Sunamganj\n Refers to a granted habitat, also sanctuary from the Persian suffix, bast\u012b[27]\u2014 e.g. Basti Maluk, Azam Basti\n wikt:\u0646\u0647\u0631, river, e.g., Nahr-e Mian; see All pages with titles containing Nahr-e\n Means (irrigation) canal[6]\n Means \"tent\"[28] \u2014 e.g. Dera Ghazi Khan, Dera Ismail Khan\n Examples: Darabgerd, Dastagird, Dastjerd, Khosrowjerd, Farhadgerd, Stepanakert, Tigranakert\n Means \"a place abounding in...\", \"place of...\"[29] \u2014 e.g. Afghanistan; Pakistan\n",
      "timestamp": "2025-10-09 19:12:15.525990"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Oikonyms_in_Western_and_South_Asia",
      "title": "Oikonyms in Western and South Asia - Wikipedia",
      "description": "",
      "text": "Oikonyms in Western, Central, South, and Southeast Asia can be grouped according to various components, reflecting common linguistic and cultural histories.[1] Toponymic study is not as extensive as it is for placenames in Europe and Anglophone parts of the world, but the origins of many placenames can be determined with a fair degree of certainty.[2][3]\nOne complexity to the study when discussing it in English is that the Romanization of names, during British rule and otherwise, from other languages has not been consistent.[2]\n In Rajasthan, names are frequently given after rock, stone, ravine, and embankment. In the Gangetic plain, the predominant natural features are trees, grass prairies, and bodies of water. Prominent trees, visible from a long way off, would often serve as landmarks and give their name to places before there was any permanent settlement there. This was especially the case where a large tree indicated a ford across a river; for example, the name Gaigh\u0101\u1e6d indicates a ford next to an agai tree. Tree names are especially common in areas that were historically under dense forest cover until recent centuries.[4]:\u200a23\u200a\n Common affixes used in South Asian oikonyms can be grouped based on their linguistic origin (with examples from India, Bangladesh, Pakistan, Nepal, and elsewhere such as in Sanskrit-influenced Indonesia): \n Means hamlet[5] \u2014 e.g. Dombivli; Kasan Wala; Sandhilianwali; Gujranwala; Tiruchirappalli\n Means \"fort\"[6][5] \u2014 Pathankot; Sialkot\n Means \"city\", or \"city of\"[7] \u2014 e.g. Visakhapatnam'\n Means \"abode\"; from Sanskrit \u0101-laya. e.g. Meghalaya, Himalaya,  Lok\u0101-laya (settlement).\n These suffixes are very common, especially -aul\u012b. In many cases, they are probably derived from Sanskrit palli, referring to a hamlet or small village. For example, B\u0101r\u1e0dol\u012b in Gujarat is attested in a Rashtrakuta-era inscription as V\u0101ra\u1e0dapallik\u0101. Names with these suffixes may also come from Sanskrit valli, meaning \"section\" or \"part\"; either origin is plausible.[8]:\u200a72\u200a[9]:\u200a53\u20134,\u200a64\u200a\n At some point, it seems that -aul\u012b became regarded as a distinct morpheme by itself, and apparently used independently as a suffix without being derived from an earlier form. For example,  the names Shamsaul\u012b and Shekhauliy\u0101 must have coined after the Muslim conquest to simply mean something like \"Shams ud-D\u012bn's village\".[8]:\u200a67,\u200a75\u200a\n The form -aul\u012b also seems to have become standardised and absorbed similar forms by analogy. For example, Dubaul\u012b (from D\u016bbe) is a common village name in eastern Uttar Pradesh, but it is not the regular, expected form of the name. The regular form would be Dubel\u012b, which exists but is far less common. In most cases, the name was assimilated to -aul\u012b by analogy with other places with names ending in -aul\u012b.[8]:\u200a75\u200a\n Means bamboo, from Sanskrit va\u1e43\u015ba. It was historically common for villages to be surrounded by bamboo groves that were planted as a form of defence. In many cases, it can be hard to distinguish between places named with b\u0101ns from places named with b\u0101s (\"dwelling\"), since b\u0101s sometimes becomes nasalised and b\u0101ns sometimes becomes de-nasalised. Examples of places named with b\u0101ns are B\u0101nsg\u0101on and B\u0101ns\u012b.[4]:\u200a36\u20137\u200a\n The names ba\u1e5b and bargad both refer to the banyan tree, ultimately from Sanskrit va\u1e6da.[4]:\u200a25\u20136\u200a This is a very common place name element; according to Sankalia, many towns and villages may have originally started out as temporary shelters underneath the wide canopy of a banyan tree. As they grew into more permanent settlements, they kept the name.[9]:\u200a88\u200a Ba\u1e5b has the common variations ba\u1e0d and ba\u1e6d.[4]:\u200a26\u200a Another variant is va\u1e0d, as in Va\u1e0dodar\u0101.[9]:\u200a88\u200a\n A common prefix, especially in eastern Uttar Pradesh.[10]:\u200a72\u200a The Oxford Hindi-English Dictionary defines chak (\u091a\u0915) with several meanings, including \"a piece of assigned or rent-free land\"; \"the detached or unconsolidated fields of a village\"; and simply \"a sub-division of land\". It derives the term from Sanskrit chakra, meaning \"circle\".[11]:\u200a296\u200a Whalley, on the other hand, preferred a derivation from Persian chak, noting that \"Chak\" is frequently followed by a Muslim name.[10]:\u200a72\u200a\n Means village, land, country; from Sanskrit \u0926\u0947\u0936 (desa) for \"space\"[12][13] \u2014 e.g. Bangladesh. In Indonesia it becomes Desa which is another Indonesian word for \"village\". \n In many cases, this ending is probably a \"worn-down\" descendant of earlier -khe\u1e5ba (\"village\").[4]:\u200a2\u200a\n Means fortress[14] \u2014 Chandigarh, Ramgarh\n According to Whalley, Ga\u1e5bh\u012b when used as a prefix probably in most cases originally referred to a village surrounded by a ditch.[10]:\u200a74\u200a\n In many cases, the place name element Ma\u016b (or mai) may be derived from Sanskrit mary\u0101d\u0101, meaning \"shore\" or \"bank\". This name is usually given to places by a river, stream, or jhil (for example, \u1e0calma\u016b on the Ga\u1e45g\u0101). Examples of these names are Argha\u1e6dma\u016b (\"the bank or shore where the water-wheel is\"), Bhainsmai (\"shore where cattle or horses graze\"), or Pathr\u0101mai (\"stony shore\"). In other cases, ma\u016b is a contraction of mahu\u0101: the mahua tree, Madhuca longifolia.[4]:\u200a4\u20136\u200a\n Some places have Ma\u016b as a standalone name (for example, Ma\u016b, Uttar Pradesh), while in other cases -ma\u016b is a suffix or even a prefix. The name M\u0101wai is a variant of Ma\u016b.[4]:\u200a4\u20136\u200a\n Means city, land, country, village;[6] from Sanskrit \u0928\u0917\u0930 (nagara) \u2014 e.g. Ahmednagar, Biratnagar. In Indonesian, the word Negara means \"country\" and the word Nagari is a term used in West Sumatra referring to \"village\". Also used in Borneo island, e.g. Negara Brunei Darussalam\n Many modern names using nagar in full are relatively recent origin; older names with nagar have often been shortened to n\u0101r or ner.[9]:\u200a73\u200a\n At least in northern India, nagar is not used as a prefix. Instead, the forms Nagl\u0101 or, more rarely, Nagr\u0101, are used. About 100 places also have the feminine forms Nagariy\u0101 and Nagariy\u0101.[10]:\u200a71\u200a\n Apparently derived from Sanskrit niv\u0101sa, \"dwelling\", combined with the Persian name Nau\u0101b\u0101d (\"new settlement\"). Naw\u0101d\u0101, along with its feminine variant Naw\u0101diy\u0101, is a very common village name by itself, and it is also used as a prefix for other names.[10]:\u200a74\u200a\n In many cases, this ending is probably a \"worn-down\" descendant of earlier -g\u0101on (\"village\") or -ban (\"forest\").[4]:\u200a2\u200a\n The Sanskrit term padra denoted a roadside village or residence (related to pad, meaning \"foot\"). Beginning around the 5th century, a regular sound change took place where /p/ became /v/ between vowels, turning this suffix into -vadra in many place names. In many modern place names, -vadra has further morphed into -dar\u0101. For example, Va\u1e0dodar\u0101 is from an earlier attested form Va\u1e6dapadra, Talodr\u0101 is from Talapadra or Talapadraka, and L\u0101\u1e6dhodr\u0101 is from L\u0101\u1e6dhivadra (attested in Chaulukya-era epigraphy). Similar names like Sa\u1e0dodar\u0101 and Ra\u1e47odar\u0101 probably share the same origin, although their older forms are not directly attested.[9]:\u200a51\u20133,\u200a61\u20133,\u200a76\u20137\u200a\n Pah\u0101\u1e5b, with the retroflex \u1e5b, means a hill, cliff, or overhanging river bank. Pah\u0101r with a non-retroflex r is a personal name, derived from Sanskrit prah\u0101ra. It can be hard to tell these place name elements apart because they can be easily confused in other scripts.[8]:\u200a55\u200a\n From Hindi pa\u1e6d\u1e6d\u012b, meaning \"strip\", itself derived from Sanskrit pa\u1e6d\u1e6dik\u0101. As a place name element, it is used in the sense of \"a strip of land\". In some cases it refers to a share of land held in joint tenure by a pattidar (literally \"shareholder\").[15]\n These are all names for the pilkhan tree, one of several varieties of fig tree viewed as sacred in Hinduism. The forms pilkhu and pilkhan come from Sanskrit plak\u1e63\u0101, while p\u0101ka\u1e5b and p\u0101kha\u1e5b come from Sanskrit #Sanskrit parka\u1e6d\u012b. One place with this name is Pilkhuw\u0101.[4]:\u200a27\u200a\n literally \"lump\" or a small altar of sand[6]\n The p\u012bpal tree, Ficus religiosa, is a common place name element.[4]:\u200a26\u20137\u200a\n Means village, town, state, country;[6] from Sanskrit \u092a\u0941\u0930 (pura) \u2014 e.g. Jamalpur; Kanpur; Khanpur. In Southeast Asian and some south Asian countries, it is known as pura, e.g. Anuradhapura,  Singapura, and Indonesian cities such as Jayapura, Siak Sri Indrapura, etc. In Indonesia, pura also refers to a Hindu temple.[16]\n In ancient times, the word pura strictly referred to a fort, but its meaning was gradually broadened to include any town regardless of its particular function. By the early medieval period, pura was often used to denote a commercial centre \u2013 especially in southern India, where the typical form was puram.[17]:\u200a68\u20139\u200a\n In many cases, old names originally ending in -pura have become shortened to -or over the centuries. In the case of Mangrol (originally Ma\u1e45galapura), the suffix has become -rol instead.[9]:\u200a71\u20133\u200a\n The variant pur\u0101 often originally referred to a suburb, or to a Muslim colony.[9]:\u200a72\u200a\n Pur is not used as a prefix. Instead, the form Pur\u0101 is used. In west-central Uttar Pradesh, around Kanpur and Etawah, the prefix takes the form Purw\u0101. Farther east, toward Basti, it takes the form Pure. The feminine form Pur\u012b is rarely found as a prefix.[10]:\u200a71\u20132\u200a\n Many places are named after the semal tree. There are many variations of this place name. One place with this name is Sambhal, where the form sambal ended up becoming aspirated.[4]:\u200a29\u200a\n According to Sankalia, this suffix has two possible origins: from -p\u0101\u1e6daka, which originally designated \"a large, but private house, or settlement within a village\"; and -v\u0101\u1e6daka, which denoted \"a temporarily enclosed place, such as a garden, plantation, or an enclosure of a (low caste) village consisting of boundary trees\". The shortened form p\u0101\u1e0d\u0101 appears early on in Ardham\u0101gadh\u012b Prakrit, and in early Jain literature refers to a suburb of a larger town. In Gujarat, the present form -v\u0101\u1e0d\u0101 first appears in inscriptions dating to the Chaulukya period. -V\u0101\u1e0d\u0101 continued to be used productively to form new place names; it would have been originally given to private settlements \"characterised either by a personal name or a prominent physiographical feature\". Modern names ending in -v\u0101\u1e0d\u0101 are descended from either ancient names that originally ended in either -p\u0101\u1e6daka or -v\u0101\u1e6daka, or more recent names that originally ended in -v\u0101\u1e0d\u0101.[9]:\u200a56\u20137,\u200a59,\u200a66\u20137\u200a\n An example is Delv\u0101\u1e0d\u0101. This name is attested in a Maitraka inscription as Devakula-p\u0101\u1e6daka, which would have later been contracted to *Devalv\u0101\u1e0d\u0101 and then De\u00fclav\u0101\u1e0d\u0101 (which is attested in a Chaulukya inscription) before finally reaching the present form.[9]:\u200a66\u20137\u200a\n In Maharasthra, the term v\u0101\u1e0d\u0101 refers to a built-up area, with or without an enclosure, belonging to a private citizen.[9]:\u200a59\u200a\n From Sanskrit, meaning \"dwelling\" or \"residence\" (of either an individual or a group). This suffix is especially common in northern Gujarat. Some places, such as Jetalvasana, contain the entire suffix without any modification. Others, like Chadasana, Jhulasan, Lunasan, Nandasan, and Ranasan (all of which are mentioned in medieval inscriptions with the suffix -vasa\u1e47a), have had the suffix modified to -sa\u1e47(\u0101) or -san(\u0101) over time.[9]:\u200a58,\u200a69\u201370\u200a\n From Sanskrit v\u0101\u1e6dik\u0101, meaning \"orchard\" or \"garden\". Commonly paired with tree names, e.g. Sisw\u0101r\u012b. Some examples with tribal names are also found; these are probably references to an individual person; examples are Bharw\u0101r\u012b and Lodhw\u0101r\u012b.[4]:\u200a25,\u200a29,\u200a60\u200a\n (\u0622\u0628\u0627\u062f): - -abad is a Persian \"dwelling of\" or \"town of\", combined with a person's or group's name (usually the founder or primary inhabitant(s))[6][18] \u2014 e.g. Hyderabad; Islamabad; Mirza Abad; Ashgabat;  Leninabad; Vagharshapat; Sardarabad; Sardarapat . Being a generic and an ambiguous term referring to small isolated farms, village (but not city) on one hand, and towns and cities, on the other hand.[19][20][21] See also abadi (settlement).\n Means \"port\" (wikt:\u0628\u0646\u062f\u0631) \u2014 e.g. Bandar Abbas; see All pages with titles containing Bandar\n Means field, desert (wikt:\u062f\u0634\u062a) \u2014 e.g. Hulandasht; see All pages with titles containing dasht\n From Perso-Arabic i\u1e25tim\u0101l, meaning \"probability\". In historical South Asian revenue terminology, Ihtimali referred to flood-prone lands along river banks or in low-lying areas. Ghair Ihtimali meant the opposite, i.e. not liable to flooding during the rainy season. These were used in place names to distinguish two villages with the same name, such as Todarpur Ihtimali and Todarpur Ghair Ihtimali in present-day Aligarh district, India.[22]\n From Arabic kh\u0101\u1e63\u1e63, meaning \"selected\" or \"private\". In India, it was historically used to refer to a place managed directly by the government or by a jagirdar, without any intermediaries. For example, Jamal Mohd Siddiqi identifies six places with \"kh\u0101s\" in their name in present-day Aligarh district, India. All six were founded by Rajput chiefs during the Mughal period, and they all occupy a prominent position on high ground. Kh\u0101s is also sometimes used in cases where there are two villages with the same name; in this case, kh\u0101s is affixed to the older and/or larger one.[23]\n Means \"neighborhood\" (wikt:\u06a9\u0648\u06cc) \u2014 e.g. Kordkuy; see All pages with titles containing kuy\n (in various languages) shrine, grave, tomb, etc. (from wikt:\u0645\u0632\u0627\u0631), cf. \"Mazar (mausoleum)\". The placename usually refers to a grave of a saint, ruler, etc.: Mazar-i-Sharif; see All pages with titles containing Mazar\n Derived from Arabic mazra\u0295, which originally refers to a farm field. In parts of India, though, the term refers to a hamlet or cluster of houses that is separate from, but subordinate to, a larger village. (The reason for the hamlet's separation is so that farmers can be closer to their crops.) Places with Majra in their name typically originated in this manner and later became independent villages of their own.[24]\n Derived from Arabic milk, meaning \"possession\" or \"property\". Like chak, it was historically used to designate a rent-free piece of land. Milk in particular usually designated land held by Muslim zamindars.[25]\n Derived from Perso-Arabic munzabt, meaning \"confiscated\". For example, the village of Raipur Munzabtah in Aligarh district got its name because it was confiscated by the British government after its pattidar participated in the Indian Rebellion of 1857.[25]\n From Arabic mutafarriq\u0101t, literally meaning \"miscellaneous\". This was used historically to denote a fiscal or administrative unit consisting of various scattered pieces of land. Villages called \"mutafarriqat\" are so named because they belonged to such a unit.[23]\n Derived from Arabic ni\u1e63f, meaning \"half\". For example, the village of Marhauli Nisfi Ashrafabad in present-day Aligarh district was formed by taking out a half portion from Ashrafabad.[25]\n From Perso-Arabic ra'iyyat, meaning \"subjects, peasants, cultivators\". It is used, for example, in the name of Lalpur Raiyyatpur in present-day Aligarh district, which likely originated as a settlement of peasants under the zamindar of nearby Lalpur.[26]\n Means \"city\"[6] \u2014 e.g. Bulandshahr\n Means fort, fortress, castle;[6] see also \"Qalat (fortress)\" \u2014 e.g. Makhachkala, Akhalkalaki, Solzha-Ghala, Dzaudzhikau\n Persian-Urdu, taken to mean neighborhood in Indian context. For example, Daryaganj, Sunamganj\n Refers to a granted habitat, also sanctuary from the Persian suffix, bast\u012b[27]\u2014 e.g. Basti Maluk, Azam Basti\n wikt:\u0646\u0647\u0631, river, e.g., Nahr-e Mian; see All pages with titles containing Nahr-e\n Means (irrigation) canal[6]\n Means \"tent\"[28] \u2014 e.g. Dera Ghazi Khan, Dera Ismail Khan\n Examples: Darabgerd, Dastagird, Dastjerd, Khosrowjerd, Farhadgerd, Stepanakert, Tigranakert\n Means \"a place abounding in...\", \"place of...\"[29] \u2014 e.g. Afghanistan; Pakistan\n",
      "timestamp": "2025-10-09 19:12:15.530062"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_Hebrew_toponyms",
      "text": "\n This glossary gives translations of Hebrew terms commonly found as components in Hebrew toponyms.\n",
      "timestamp": "2025-10-09 19:12:15.913899"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_Hebrew_toponyms",
      "title": "Glossary of Hebrew toponyms - Wikipedia",
      "description": "",
      "text": "\n This glossary gives translations of Hebrew terms commonly found as components in Hebrew toponyms.\n",
      "timestamp": "2025-10-09 19:12:15.916069"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_Arabic_toponyms",
      "text": "The glossary of Arabic toponyms gives translations of Arabic terms commonly found as components in Arabic toponyms. A significant number of them were put together during the PEF Survey of Palestine carried out in the second half of the 19th century.\n",
      "timestamp": "2025-10-09 19:12:16.325477"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_Arabic_toponyms",
      "title": "Glossary of Arabic toponyms - Wikipedia",
      "description": "",
      "text": "The glossary of Arabic toponyms gives translations of Arabic terms commonly found as components in Arabic toponyms. A significant number of them were put together during the PEF Survey of Palestine carried out in the second half of the 19th century.\n",
      "timestamp": "2025-10-09 19:12:16.327909"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_geography_terms_(N%E2%80%93Z)",
      "text": "\n This glossary of geography terms is a list of definitions of terms and concepts used in geography and related fields, including Earth science, oceanography, cartography, and human geography, as well as those describing spatial dimension, topographical features, natural resources, and the collection, analysis, and visualization of geographic data. It is split across two articles:\n Related terms may be found in Glossary of geology, Glossary of agriculture, Glossary of environmental science, and Glossary of astronomy.\n Also narrow. Also neighbourhood or abbreviated to hood. Also called the Geographic North Pole, Geographic North, or simply the North Pole. Also called the Geomagnetic North Pole. Also called the Magnetic North Pole or Magnetic North. Also oceanology. Also econym and oikonym. Also curvimeter, meilograph, or map measurer. Also orthophoto, orthoimage, or orthoimagery. Also outcropping. (pl.) palsen (pl.) pampas Also gnamma, weathering pit, and solution pan. Also centrocline. Also periplous. Also euphotic zone, epipelagic zone, and sunlight zone. Also zone of saturation. Also physiography or geosystems. Also foothills. Also potentiometric surface. Also chimney, finger, monument, needle, pillar, spire, and tower. Also subsidence crater or collapse crater. Also plain table. Also high plain or tableland. Also exaration. Also polar ice sheet. Also empolder. Also karst polje or karst field. Also pot, swirlhole, churn hole, evorsion, rock mill, and eddy mill. Also abbreviated quad. Also rapids or whitewater. Also impoundment. Also loxodrome or simply rhumb. Also dune and swale. Also riparian area, riparian corridor, and riparian strip. Also riparian. Also route map and street map. Also sebkha. Also panhandle, chimney (if protruding northward), or bootheel (if protruding southward). Also tidal flat, sea marsh, or salt swamp. Also salt flat. Also seawater. Also satnav. Also savannah. Also escarpment. Also scroll bar. Also sea road, seaway, or shipping lane. Also sea floor or ocean floor. Also sector model. Also s\u00e9rac. Also locality or populated place. Also r\u00f4che moutonn\u00e9e. Also sandbank, sandbar, or gravel bar. Also shoreline. Also slack tide or simply slack. Also terminus or toe. Also called the Geographic South Pole, Geographic South, or simply the South Pole. Also called the Geomagnetic South Pole. Also called the Magnetic South Pole or Magnetic South. Also coordinate reference system (CRS). Also sandspit. Also spot height. Also sea stack. Also stream stage or river stage. Also composite volcano. Also waterbody order. Also riverbed or simply bed. Also acme, apex, peak, and zenith. Also supraglacial. Also breaker zone. Also tableland. Also tails. Also corrie loch. Also end moraine. Also topographical relief or simply relief. Also talweg. Also tidal channel. Also land-tied island. Also time-space geography. Also Tissot's ellipse and ellipse of distortion. Also tollway or turnpike. Also relief map. Also autonomous height, relative height, or shoulder drop. Also castle koppie or kopje. Also timberline. Also called an affluent. Also called the tropical zone or torrid zone. Also geodetic north. Also geodetic south. Sometimes used interchangeably with highland. Also vale. Also debris avalanche. Also vulcanology. Also simply wash. Also wasteland or simply waste. Also water point mapping. Also cascade, cataract, or simply fall or falls. Also water hole. Also shore platform, wave-cut cliff, or coastal bench. Also low head dam. Also air gap. Also gypsey. Much of this material was copied from U.S. government works which are in the public domain because they are not eligible for copyright protection.[25]\n",
      "timestamp": "2025-10-09 19:12:16.872547"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_geography_terms_(N%E2%80%93Z)",
      "title": "Glossary of geography terms (N\u2013Z) - Wikipedia",
      "description": "",
      "text": "\n This glossary of geography terms is a list of definitions of terms and concepts used in geography and related fields, including Earth science, oceanography, cartography, and human geography, as well as those describing spatial dimension, topographical features, natural resources, and the collection, analysis, and visualization of geographic data. It is split across two articles:\n Related terms may be found in Glossary of geology, Glossary of agriculture, Glossary of environmental science, and Glossary of astronomy.\n Also narrow. Also neighbourhood or abbreviated to hood. Also called the Geographic North Pole, Geographic North, or simply the North Pole. Also called the Geomagnetic North Pole. Also called the Magnetic North Pole or Magnetic North. Also oceanology. Also econym and oikonym. Also curvimeter, meilograph, or map measurer. Also orthophoto, orthoimage, or orthoimagery. Also outcropping. (pl.) palsen (pl.) pampas Also gnamma, weathering pit, and solution pan. Also centrocline. Also periplous. Also euphotic zone, epipelagic zone, and sunlight zone. Also zone of saturation. Also physiography or geosystems. Also foothills. Also potentiometric surface. Also chimney, finger, monument, needle, pillar, spire, and tower. Also subsidence crater or collapse crater. Also plain table. Also high plain or tableland. Also exaration. Also polar ice sheet. Also empolder. Also karst polje or karst field. Also pot, swirlhole, churn hole, evorsion, rock mill, and eddy mill. Also abbreviated quad. Also rapids or whitewater. Also impoundment. Also loxodrome or simply rhumb. Also dune and swale. Also riparian area, riparian corridor, and riparian strip. Also riparian. Also route map and street map. Also sebkha. Also panhandle, chimney (if protruding northward), or bootheel (if protruding southward). Also tidal flat, sea marsh, or salt swamp. Also salt flat. Also seawater. Also satnav. Also savannah. Also escarpment. Also scroll bar. Also sea road, seaway, or shipping lane. Also sea floor or ocean floor. Also sector model. Also s\u00e9rac. Also locality or populated place. Also r\u00f4che moutonn\u00e9e. Also sandbank, sandbar, or gravel bar. Also shoreline. Also slack tide or simply slack. Also terminus or toe. Also called the Geographic South Pole, Geographic South, or simply the South Pole. Also called the Geomagnetic South Pole. Also called the Magnetic South Pole or Magnetic South. Also coordinate reference system (CRS). Also sandspit. Also spot height. Also sea stack. Also stream stage or river stage. Also composite volcano. Also waterbody order. Also riverbed or simply bed. Also acme, apex, peak, and zenith. Also supraglacial. Also breaker zone. Also tableland. Also tails. Also corrie loch. Also end moraine. Also topographical relief or simply relief. Also talweg. Also tidal channel. Also land-tied island. Also time-space geography. Also Tissot's ellipse and ellipse of distortion. Also tollway or turnpike. Also relief map. Also autonomous height, relative height, or shoulder drop. Also castle koppie or kopje. Also timberline. Also called an affluent. Also called the tropical zone or torrid zone. Also geodetic north. Also geodetic south. Sometimes used interchangeably with highland. Also vale. Also debris avalanche. Also vulcanology. Also simply wash. Also wasteland or simply waste. Also water point mapping. Also cascade, cataract, or simply fall or falls. Also water hole. Also shore platform, wave-cut cliff, or coastal bench. Also low head dam. Also air gap. Also gypsey. Much of this material was copied from U.S. government works which are in the public domain because they are not eligible for copyright protection.[25]\n",
      "timestamp": "2025-10-09 19:12:16.884901"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_geography_terms_(A%E2%80%93M)",
      "text": "\n This glossary of geography terms is a list of definitions of terms and concepts used in geography and related fields, including Earth science, oceanography, cartography, and human geography, as well as those describing spatial dimension, topographical features, natural resources, and the collection, analysis, and visualization of geographic data. It is split across two articles:\n Related terms may be found in Glossary of geology, Glossary of agriculture, Glossary of environmental science, and Glossary of astronomy.\n Also amphidrome and tidal node. Also anastomosed stream. Also anoecumene. Also antecedent river and antecedent drainage. Also aquafer. Also island chain. Also wash. Also exposure. Also bahada. Also corridor. Also rimaye or simply berg or schrund. Also biodiversity. Also felsenmeer. Also marine geyser. Also mire, quagmire, or muskeg. Also bourn, born, borne, and burn. Also breaker. Also lowveld, or simply bush or veld. Also cadaster. Also canebreak. Also gorge or ca\u00f1on. Also key. Also strait. Also shott and shatt. Also corrie or cwm. Also city centre. Also abrasion coast. Also coastline, seashore, and seaboard. Also index of concentration. Also gap or notch. Variously comb, coomb, coombe, and cumb. Also compass star, wind rose, or rose of the winds. Also isoline or isopleth. Also frost churning. Also impoundment. Also datum level or datum line. Also debouche. Also gentilic. Also dependent territory. Also reg, serir, gibber, sa\u00ef, and desert mosaic. Also desert patina, rock varnish, and rock rust. Also social trail. Also Atlantic-type coastline. Also vrta\u010de and shakehole. Also down and downs. Also catchment, drainage area, river basin, water basin, or watershed. Also ridgeline, watershed, water parting, water divide, or simply divide. Also re-entrant. Also brash ice. Also called the Earth sciences or geoscience. Also called an ecological region. Also oecumene. Also endoreic basin, closed basin, or terminal basin. Also entrepot or transshipment port. Also reg and hamada. Also glacial erratic. Also os, eskar, or eschar. Also inversac. Also land-fast ice and shore-fast ice. Also Tobler's First Law of Geography. Also fiord. Also bottomland. Also hypocenter. Also cross-border worker and frontier worker. Also gio. Also geolocking. Also geospatial data, georeferenced information, and geoinformation. Also orthodrome, geodesic line, and geodetic line. Also geodetics. Also geodetic network, reference network, or control point network. Also geodetic system, geodetic reference datum, or geodetic reference system. Also GIScience. Also geographical momentum. Also geospatial science. Also clearing. Also world city, power city, or alpha city. Also slope, incline, gradient, pitch, rise, or mainfall. Also orthodrome. Also orthodromic distance. Also greenway. Also grid variation or grivation. Also tablemount. Also hammada. Also heathland. Also simply hedge. Sometimes used interchangeably with upland. Also knoll. Also hog's back or hogsback. Also skyline. Also humanist geography. Also continental glacier. Also clinometer, declinometer, tilt meter, gradient meter, slope gauge, and level gauge. Also ingressed coast and depressed coast. Also monadnock. Also integrative geography, environmental geography, or human\u2013environment geography. Also intermediate directions or ordinal directions. Also intermountain. Also isle. Also isostatic equilibrium. Also jhoom cultivation or slash-and-burn agriculture. Also magmatic water. Also kettle hole or pothole. Also kil. Also nickpoint. Also colc and colk. Also Gaussian process regression and Wiener\u2013Kolmogorov prediction. Also mudflow or debris flow. Also landslip. Also dike, embankment, floodbank, and stopbank. Also longshore current and littoral drift. Also magnetic variation. Also dip angle and magnetic inclination. Also trunk. Also index map. Also marche or mark; (pl.) marches or marchlands. Also territorial sea, marine belt, and maritime belt. Also meander scarp. Also median moraine. Also snowmelt. Also equatorial cylindrical orthomorphic map projection. Also metro area or commuter belt. Also Moho discontinuity, Moho boundary, or simply Moho. Also moorland. Also mud flat and tidal flat. Much of this material was copied from U.S. government works which are in the public domain because they are not eligible for copyright protection.[24]\n",
      "timestamp": "2025-10-09 19:12:18.096627"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_geography_terms_(A%E2%80%93M)",
      "title": "Glossary of geography terms (A\u2013M) - Wikipedia",
      "description": "",
      "text": "\n This glossary of geography terms is a list of definitions of terms and concepts used in geography and related fields, including Earth science, oceanography, cartography, and human geography, as well as those describing spatial dimension, topographical features, natural resources, and the collection, analysis, and visualization of geographic data. It is split across two articles:\n Related terms may be found in Glossary of geology, Glossary of agriculture, Glossary of environmental science, and Glossary of astronomy.\n Also amphidrome and tidal node. Also anastomosed stream. Also anoecumene. Also antecedent river and antecedent drainage. Also aquafer. Also island chain. Also wash. Also exposure. Also bahada. Also corridor. Also rimaye or simply berg or schrund. Also biodiversity. Also felsenmeer. Also marine geyser. Also mire, quagmire, or muskeg. Also bourn, born, borne, and burn. Also breaker. Also lowveld, or simply bush or veld. Also cadaster. Also canebreak. Also gorge or ca\u00f1on. Also key. Also strait. Also shott and shatt. Also corrie or cwm. Also city centre. Also abrasion coast. Also coastline, seashore, and seaboard. Also index of concentration. Also gap or notch. Variously comb, coomb, coombe, and cumb. Also compass star, wind rose, or rose of the winds. Also isoline or isopleth. Also frost churning. Also impoundment. Also datum level or datum line. Also debouche. Also gentilic. Also dependent territory. Also reg, serir, gibber, sa\u00ef, and desert mosaic. Also desert patina, rock varnish, and rock rust. Also social trail. Also Atlantic-type coastline. Also vrta\u010de and shakehole. Also down and downs. Also catchment, drainage area, river basin, water basin, or watershed. Also ridgeline, watershed, water parting, water divide, or simply divide. Also re-entrant. Also brash ice. Also called the Earth sciences or geoscience. Also called an ecological region. Also oecumene. Also endoreic basin, closed basin, or terminal basin. Also entrepot or transshipment port. Also reg and hamada. Also glacial erratic. Also os, eskar, or eschar. Also inversac. Also land-fast ice and shore-fast ice. Also Tobler's First Law of Geography. Also fiord. Also bottomland. Also hypocenter. Also cross-border worker and frontier worker. Also gio. Also geolocking. Also geospatial data, georeferenced information, and geoinformation. Also orthodrome, geodesic line, and geodetic line. Also geodetics. Also geodetic network, reference network, or control point network. Also geodetic system, geodetic reference datum, or geodetic reference system. Also GIScience. Also geographical momentum. Also geospatial science. Also clearing. Also world city, power city, or alpha city. Also slope, incline, gradient, pitch, rise, or mainfall. Also orthodrome. Also orthodromic distance. Also greenway. Also grid variation or grivation. Also tablemount. Also hammada. Also heathland. Also simply hedge. Sometimes used interchangeably with upland. Also knoll. Also hog's back or hogsback. Also skyline. Also humanist geography. Also continental glacier. Also clinometer, declinometer, tilt meter, gradient meter, slope gauge, and level gauge. Also ingressed coast and depressed coast. Also monadnock. Also integrative geography, environmental geography, or human\u2013environment geography. Also intermediate directions or ordinal directions. Also intermountain. Also isle. Also isostatic equilibrium. Also jhoom cultivation or slash-and-burn agriculture. Also magmatic water. Also kettle hole or pothole. Also kil. Also nickpoint. Also colc and colk. Also Gaussian process regression and Wiener\u2013Kolmogorov prediction. Also mudflow or debris flow. Also landslip. Also dike, embankment, floodbank, and stopbank. Also longshore current and littoral drift. Also magnetic variation. Also dip angle and magnetic inclination. Also trunk. Also index map. Also marche or mark; (pl.) marches or marchlands. Also territorial sea, marine belt, and maritime belt. Also meander scarp. Also median moraine. Also snowmelt. Also equatorial cylindrical orthomorphic map projection. Also metro area or commuter belt. Also Moho discontinuity, Moho boundary, or simply Moho. Also moorland. Also mud flat and tidal flat. Much of this material was copied from U.S. government works which are in the public domain because they are not eligible for copyright protection.[24]\n",
      "timestamp": "2025-10-09 19:12:18.112993"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_genetics_and_evolutionary_biology",
      "text": "\nThis glossary of genetics and evolutionary biology is a list of definitions of terms and concepts used in the study of genetics and evolutionary biology, as well as sub-disciplines and related fields, with an emphasis on classical genetics, quantitative genetics, population biology,  phylogenetics, speciation, and systematics. It has been designed as a companion to Glossary of cellular and molecular biology, which contains many overlapping and related terms; other related glossaries include Glossary of biology and Glossary of ecology.\n Also called functionalism. Also called geographic speciation, vicariance, vicariant speciation, and dichopatric speciation. Also called an ancestral character, primitive character, or primitive trait. Also called positive assortative mating and homogamy. Also testcrossing. Also simply called the Dobzhansky\u2013Muller model. Also called a monophyletic group. Also convergence. Also crossing and outbreeding. Also Darwinian theory and Darwinian evolution. Also derived character, advanced character, and advanced trait. Denoted in shorthand with the somatic number 2n. Also positive selection. Also negative assortative mating and heterogamy. Also diversifying selection. Also divergence. Also gene amplification. Sometimes used interchangeably with genetic variation. Also called allelic drift or the Sewall Wright effect. Also genetic draft and the hitchhiking effect. Also DNA testing and genetic screening. Sometimes used interchangeably with genetic variation. Sometimes used interchangeably with genetic diversity and genetic variability. Denoted in shorthand with the somatic number n. Also inheritance. Also hybrid vigor and outbreeding enhancement. Also homologs or homologues. Also lateral gene transfer (LGT). Also incrossing. Also introgressive hybridization. Also called the last universal cellular ancestor or simply the last universal ancestor. Also pedigree. Also called lineage-branching. Plural loci. Also environmental genomics, ecogenomics, and community genomics. Also point-nonsense mutation. Also nonsynonymous substitution or replacement mutation. Also ontogenesis and morphogenesis. Also outcrossing or crossbreeding. Also maximum parsimony. Also polypheny. Also multifurcation. Also genetic bottleneck. Also prosposito for a male subject and prosposita for a female subject. Also neotype. Also purebreed. Also complex trait. Also refuge. Also called network evolution. Also reversion. Also Fisherian runaway. Also selection pressure. Denoted in shorthand with a + superscript.",
      "timestamp": "2025-10-09 19:12:18.631722"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_genetics_and_evolutionary_biology",
      "title": "Glossary of genetics and evolutionary biology - Wikipedia",
      "description": "",
      "text": "\nThis glossary of genetics and evolutionary biology is a list of definitions of terms and concepts used in the study of genetics and evolutionary biology, as well as sub-disciplines and related fields, with an emphasis on classical genetics, quantitative genetics, population biology,  phylogenetics, speciation, and systematics. It has been designed as a companion to Glossary of cellular and molecular biology, which contains many overlapping and related terms; other related glossaries include Glossary of biology and Glossary of ecology.\n Also called functionalism. Also called geographic speciation, vicariance, vicariant speciation, and dichopatric speciation. Also called an ancestral character, primitive character, or primitive trait. Also called positive assortative mating and homogamy. Also testcrossing. Also simply called the Dobzhansky\u2013Muller model. Also called a monophyletic group. Also convergence. Also crossing and outbreeding. Also Darwinian theory and Darwinian evolution. Also derived character, advanced character, and advanced trait. Denoted in shorthand with the somatic number 2n. Also positive selection. Also negative assortative mating and heterogamy. Also diversifying selection. Also divergence. Also gene amplification. Sometimes used interchangeably with genetic variation. Also called allelic drift or the Sewall Wright effect. Also genetic draft and the hitchhiking effect. Also DNA testing and genetic screening. Sometimes used interchangeably with genetic variation. Sometimes used interchangeably with genetic diversity and genetic variability. Denoted in shorthand with the somatic number n. Also inheritance. Also hybrid vigor and outbreeding enhancement. Also homologs or homologues. Also lateral gene transfer (LGT). Also incrossing. Also introgressive hybridization. Also called the last universal cellular ancestor or simply the last universal ancestor. Also pedigree. Also called lineage-branching. Plural loci. Also environmental genomics, ecogenomics, and community genomics. Also point-nonsense mutation. Also nonsynonymous substitution or replacement mutation. Also ontogenesis and morphogenesis. Also outcrossing or crossbreeding. Also maximum parsimony. Also polypheny. Also multifurcation. Also genetic bottleneck. Also prosposito for a male subject and prosposita for a female subject. Also neotype. Also purebreed. Also complex trait. Also refuge. Also called network evolution. Also reversion. Also Fisherian runaway. Also selection pressure. Denoted in shorthand with a + superscript.",
      "timestamp": "2025-10-09 19:12:18.640654"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_environmental_science",
      "text": "\nThis is a glossary of environmental science.\n Environmental science is the study of interactions among physical, chemical, and biological components of the environment. Environmental science provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.\n considered ideal for gardening and agricultural uses.\n",
      "timestamp": "2025-10-09 19:12:19.246869"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_environmental_science",
      "title": "Glossary of environmental science - Wikipedia",
      "description": "",
      "text": "\nThis is a glossary of environmental science.\n Environmental science is the study of interactions among physical, chemical, and biological components of the environment. Environmental science provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.\n considered ideal for gardening and agricultural uses.\n",
      "timestamp": "2025-10-09 19:12:19.250426"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_entomology_terms",
      "text": "\n This glossary of entomology describes terms used in the formal study of insect species by entomologists.\n \u00a0The dictionary definition of thesaurus:insect#See also at Wiktionary\n",
      "timestamp": "2025-10-09 19:12:19.659678"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_entomology_terms",
      "title": "Glossary of entomology terms - Wikipedia",
      "description": "",
      "text": "\n This glossary of entomology describes terms used in the formal study of insect species by entomologists.\n \u00a0The dictionary definition of thesaurus:insect#See also at Wiktionary\n",
      "timestamp": "2025-10-09 19:12:19.662545"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_engineering:_M%E2%80%93Z",
      "text": "\n This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.\n \n",
      "timestamp": "2025-10-09 19:12:20.473934"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_engineering:_M%E2%80%93Z",
      "title": "Glossary of engineering: M\u2013Z - Wikipedia",
      "description": "",
      "text": "\n This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.\n \n",
      "timestamp": "2025-10-09 19:12:20.489946"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_engineering:_A%E2%80%93L",
      "text": "\n This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.\n When any system at equilibrium for a long period of time is subjected to a change in concentration, temperature, volume, or pressure, (1) the system changes to a new equilibrium, and (2) this change partly counteracts the applied change. It is common to treat the principle as a more general observation of systems,[309] such as\n When a settled system is disturbed, it will adjust to diminish the change that has been made to it or, \"roughly stated\",[309]\n Any change in status quo prompts an opposing reaction in the responding system.",
      "timestamp": "2025-10-09 19:12:21.098894"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_engineering:_A%E2%80%93L",
      "title": "Glossary of engineering: A\u2013L - Wikipedia",
      "description": "",
      "text": "\n This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.\n When any system at equilibrium for a long period of time is subjected to a change in concentration, temperature, volume, or pressure, (1) the system changes to a new equilibrium, and (2) this change partly counteracts the applied change. It is common to treat the principle as a more general observation of systems,[309] such as\n When a settled system is disturbed, it will adjust to diminish the change that has been made to it or, \"roughly stated\",[309]\n Any change in status quo prompts an opposing reaction in the responding system.",
      "timestamp": "2025-10-09 19:12:21.116986"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_electrical_and_electronics_engineering",
      "text": "This glossary of electrical and electronics engineering is a list of definitions of terms and concepts related specifically to electrical engineering and electronics engineering. For terms related to engineering in general, see Glossary of engineering.\n",
      "timestamp": "2025-10-09 19:12:21.565553"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_electrical_and_electronics_engineering",
      "title": "Glossary of electrical and electronics engineering - Wikipedia",
      "description": "",
      "text": "This glossary of electrical and electronics engineering is a list of definitions of terms and concepts related specifically to electrical engineering and electronics engineering. For terms related to engineering in general, see Glossary of engineering.\n",
      "timestamp": "2025-10-09 19:12:21.573530"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_economics",
      "text": "\n This glossary of economics is a list of definitions containing terms and concepts used in economics, its sub-disciplines, and related fields.\n Empirical methods\n Prescriptive and policy\n Also called resource cost advantage. Also called advertising elasticity. Also called agrarian inflation. Also called domestic final demand (DFD) or effective demand. Also called domestic final supply (DFS). Also called the shipping the good apples out theorem, or the third law of demand. Also called uncertainty aversion. Also called the national system. Also called a competition law or anti-monopoly law. Also called the Arrow\u2013Debreu\u2013McKenzie model or ADM model. Also called a state-price security, pure security, or primitive security. Also called Arrow's disclosure paradox. Also called the general possibility theorem or Arrow's paradox. Also called exogenous consumption. Also called unit cost. Also called the Backus\u2013Kehoe\u2013Kydland consumption correlation puzzle or BKK puzzle. Also called the Backus-Smith consumption-real exchange rate puzzle or consumption \u2013 real-exchange-rate anomaly. Also called the advantage of backwardness or the latecomer's advantage. Also called the latecomer's disadvantage. Also called balance of international payments and abbreviated B.O.P. or BoP. Also called commercial balance or net exports (NX). Also called the discount rate in American English. also called direct exchange. Also called the exchange rate disconnect puzzle. Also called mixflation. Also called the Black\u2013Scholes\u2013Merton model. Also called the Sam Vimes theory of socioeconomic unfairness. Also called the break-even point (BEP). Also simply called spending. Also called an opportunity set. Also called intervention storage or the ever-normal granary. Also called the economic cycle or trade cycle. Also called the corporate sector or sometimes simply business. Also called the capital controversy or the two Cambridges debate. Also called the capital and financial account Also called a reserve bank or monetary authority. Also called circular flow model. Also called classical political economy. Also called artificially scarce goods, toll goods, collective goods or quasi-public goods. Also called cobweb theory. Also called social dilemma Also called opportunity cost advantage. Also called compensating wage differential or equalizing difference. Also called an antitrust law or anti-monopoly law. Also called applied general equilibrium (AGE). Also called the catch-up effect. Also called corporation tax or company tax. Also called cost increase or budget overrun. Sometimes called benefit costs analysis (BCA). Also called sucker rally. Also called excess burden or allocative inefficiency. Also called budget deficit or simply deficit. Also called population economics. Also called qualitative choice. Also called non-Walrasian theory, equilibrium with rationing, the non-market clearing approach, and non-t\u00e2tonnement theory. Also called Dorfman\u2013Steiner condition. Also called the Pigou\u2013Knight\u2013Downs paradox. Also called the Lewis model. Also called DDC models ordiscrete choice models of dynamic programming. Also abbreviated DGE and SDGE. Also called a democratic economy. Also called economic profits. Also called financial security. Also called excess demand. Also called excess supply. Also called an economic order.[144] Also called agglomeration effects. Also called an Edgeworth-Bowley box. Also called efficiency earnings. Also called a PEEF division. Also called efficient market theory (EMT). Also called intertemporal elasticity of substitution (IES). Also called Elliott wave theory. Also called Ellsberg's paradox. Also called economic equality. Also called economic surplus.[164] Also called deep poverty, abject poverty, absolute poverty, destitution, or penury. Often simply the Federal Reserve or the Fed. Also called the currency market or abbreviated Forex or FX. Also called Keynesianism. Also called the local premium Also called orthodox economics. German for \"method dispute.\" Also called Pareto optimality. Also called the prime lending rate. Also called private property ownership. Also called a flat tax. Also called social economics. Also called duty. Also called the general theory of second best or the second best theorem. Also called T\u00f6rnqvist-Theil index. Also called exchange. Also called transfer payment multiplier Also called the velocity of circulation of money.",
      "timestamp": "2025-10-09 19:12:22.098366"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_economics",
      "title": "Glossary of economics - Wikipedia",
      "description": "",
      "text": "\n This glossary of economics is a list of definitions containing terms and concepts used in economics, its sub-disciplines, and related fields.\n Empirical methods\n Prescriptive and policy\n Also called resource cost advantage. Also called advertising elasticity. Also called agrarian inflation. Also called domestic final demand (DFD) or effective demand. Also called domestic final supply (DFS). Also called the shipping the good apples out theorem, or the third law of demand. Also called uncertainty aversion. Also called the national system. Also called a competition law or anti-monopoly law. Also called the Arrow\u2013Debreu\u2013McKenzie model or ADM model. Also called a state-price security, pure security, or primitive security. Also called Arrow's disclosure paradox. Also called the general possibility theorem or Arrow's paradox. Also called exogenous consumption. Also called unit cost. Also called the Backus\u2013Kehoe\u2013Kydland consumption correlation puzzle or BKK puzzle. Also called the Backus-Smith consumption-real exchange rate puzzle or consumption \u2013 real-exchange-rate anomaly. Also called the advantage of backwardness or the latecomer's advantage. Also called the latecomer's disadvantage. Also called balance of international payments and abbreviated B.O.P. or BoP. Also called commercial balance or net exports (NX). Also called the discount rate in American English. also called direct exchange. Also called the exchange rate disconnect puzzle. Also called mixflation. Also called the Black\u2013Scholes\u2013Merton model. Also called the Sam Vimes theory of socioeconomic unfairness. Also called the break-even point (BEP). Also simply called spending. Also called an opportunity set. Also called intervention storage or the ever-normal granary. Also called the economic cycle or trade cycle. Also called the corporate sector or sometimes simply business. Also called the capital controversy or the two Cambridges debate. Also called the capital and financial account Also called a reserve bank or monetary authority. Also called circular flow model. Also called classical political economy. Also called artificially scarce goods, toll goods, collective goods or quasi-public goods. Also called cobweb theory. Also called social dilemma Also called opportunity cost advantage. Also called compensating wage differential or equalizing difference. Also called an antitrust law or anti-monopoly law. Also called applied general equilibrium (AGE). Also called the catch-up effect. Also called corporation tax or company tax. Also called cost increase or budget overrun. Sometimes called benefit costs analysis (BCA). Also called sucker rally. Also called excess burden or allocative inefficiency. Also called budget deficit or simply deficit. Also called population economics. Also called qualitative choice. Also called non-Walrasian theory, equilibrium with rationing, the non-market clearing approach, and non-t\u00e2tonnement theory. Also called Dorfman\u2013Steiner condition. Also called the Pigou\u2013Knight\u2013Downs paradox. Also called the Lewis model. Also called DDC models ordiscrete choice models of dynamic programming. Also abbreviated DGE and SDGE. Also called a democratic economy. Also called economic profits. Also called financial security. Also called excess demand. Also called excess supply. Also called an economic order.[144] Also called agglomeration effects. Also called an Edgeworth-Bowley box. Also called efficiency earnings. Also called a PEEF division. Also called efficient market theory (EMT). Also called intertemporal elasticity of substitution (IES). Also called Elliott wave theory. Also called Ellsberg's paradox. Also called economic equality. Also called economic surplus.[164] Also called deep poverty, abject poverty, absolute poverty, destitution, or penury. Often simply the Federal Reserve or the Fed. Also called the currency market or abbreviated Forex or FX. Also called Keynesianism. Also called the local premium Also called orthodox economics. German for \"method dispute.\" Also called Pareto optimality. Also called the prime lending rate. Also called private property ownership. Also called a flat tax. Also called social economics. Also called duty. Also called the general theory of second best or the second best theorem. Also called T\u00f6rnqvist-Theil index. Also called exchange. Also called transfer payment multiplier Also called the velocity of circulation of money.",
      "timestamp": "2025-10-09 19:12:22.111932"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_ecology",
      "text": "\n This glossary of ecology is a list of definitions of terms and concepts in ecology and related fields. For more specific definitions from other glossaries related to ecology, see Glossary of biology, Glossary of evolutionary biology, and Glossary of environmental science.\n Also Gause's law. Also ecoevolution. Also aposematism.",
      "timestamp": "2025-10-09 19:12:22.546647"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_ecology",
      "title": "Glossary of ecology - Wikipedia",
      "description": "",
      "text": "\n This glossary of ecology is a list of definitions of terms and concepts in ecology and related fields. For more specific definitions from other glossaries related to ecology, see Glossary of biology, Glossary of evolutionary biology, and Glossary of environmental science.\n Also Gause's law. Also ecoevolution. Also aposematism.",
      "timestamp": "2025-10-09 19:12:22.551993"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_developmental_biology",
      "text": "\nThis glossary of developmental biology is a list of definitions of terms and concepts commonly used in the study of developmental biology and related disciplines in biology, including embryology and reproductive biology, primarily as they pertain to vertebrate animals and particularly to humans and other mammals. The developmental biology of invertebrates, plants, fungi, and other organisms is treated in other articles; e.g terms relating to the reproduction and development of insects are listed in Glossary of entomology, and those relating to plants are listed in Glossary of botany.\n This glossary is intended as introductory material for novices; for more specific and technical detail, see the article corresponding to each term. Additional terms relevant to vertebrate reproduction and development may also be found in Glossary of biology, Glossary of cell biology, Glossary of genetics, and Glossary of evolutionary biology.\n \n Also gastrocoel. Also blastocoele, blastocele, cleavage cavity, and segmentation cavity. Also serosa and false amnion. Also diestrus. Also embryogeny. Also oestrous cycle.",
      "timestamp": "2025-10-09 19:12:22.910470"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_developmental_biology",
      "title": "Glossary of developmental biology - Wikipedia",
      "description": "",
      "text": "\nThis glossary of developmental biology is a list of definitions of terms and concepts commonly used in the study of developmental biology and related disciplines in biology, including embryology and reproductive biology, primarily as they pertain to vertebrate animals and particularly to humans and other mammals. The developmental biology of invertebrates, plants, fungi, and other organisms is treated in other articles; e.g terms relating to the reproduction and development of insects are listed in Glossary of entomology, and those relating to plants are listed in Glossary of botany.\n This glossary is intended as introductory material for novices; for more specific and technical detail, see the article corresponding to each term. Additional terms relevant to vertebrate reproduction and development may also be found in Glossary of biology, Glossary of cell biology, Glossary of genetics, and Glossary of evolutionary biology.\n \n Also gastrocoel. Also blastocoele, blastocele, cleavage cavity, and segmentation cavity. Also serosa and false amnion. Also diestrus. Also embryogeny. Also oestrous cycle.",
      "timestamp": "2025-10-09 19:12:22.912838"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_computer_science",
      "text": "\n \nThis glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.\n Also simply application or app. Also simply array. Also machine intelligence. Also simply binary search, half-interval search,[24] logarithmic search,[25] or binary chop.[26] Also bitrate. Also block list. Also bitmap image file, device independent bitmap (DIB) file format, or simply bitmap. Also cypher. Also class-orientation. Also lexical closure or function closure. Also theoretical neuroscience or mathematical neuroscience. Also scientific computing and scientific computation (SC). Also simply storage or memory. Also data network. Also cybersecurity[66] or information technology security (IT security). Also conditional statement, conditional expression, and conditional construct. Also flow of control. Also cyberharassment or online bullying. Also data centre. Also simply type. Also executable code, executable file, executable program, or simply executable. Also for-loop. Also informally io or IO. Also fetch\u2013decode\u2013execute cycle or simply fetch-execute cycle. Also web robot, robot, or simply bot. Also sequential search. Also mergesort. Portmanteau of modulator-demodulator. Also object module. Also formal argument. Also partition-exchange sort. Also base. Also rounding error.[184] Colloquially web address.[232] Also user interface engineering. Also WAVE or WAV due to its filename extension. Also spider, spiderbot, or simply crawler. Abbreviaton of eXtensible HyperText Markup Language.",
      "timestamp": "2025-10-09 19:12:24.185210"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_computer_science",
      "title": "Glossary of computer science - Wikipedia",
      "description": "",
      "text": "\n \nThis glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.\n Also simply application or app. Also simply array. Also machine intelligence. Also simply binary search, half-interval search,[24] logarithmic search,[25] or binary chop.[26] Also bitrate. Also block list. Also bitmap image file, device independent bitmap (DIB) file format, or simply bitmap. Also cypher. Also class-orientation. Also lexical closure or function closure. Also theoretical neuroscience or mathematical neuroscience. Also scientific computing and scientific computation (SC). Also simply storage or memory. Also data network. Also cybersecurity[66] or information technology security (IT security). Also conditional statement, conditional expression, and conditional construct. Also flow of control. Also cyberharassment or online bullying. Also data centre. Also simply type. Also executable code, executable file, executable program, or simply executable. Also for-loop. Also informally io or IO. Also fetch\u2013decode\u2013execute cycle or simply fetch-execute cycle. Also web robot, robot, or simply bot. Also sequential search. Also mergesort. Portmanteau of modulator-demodulator. Also object module. Also formal argument. Also partition-exchange sort. Also base. Also rounding error.[184] Colloquially web address.[232] Also user interface engineering. Also WAVE or WAV due to its filename extension. Also spider, spiderbot, or simply crawler. Abbreviaton of eXtensible HyperText Markup Language.",
      "timestamp": "2025-10-09 19:12:24.200210"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_computer_hardware_terms",
      "text": "\n This glossary of computer hardware terms is a list of definitions of terms and concepts related to computer hardware, i.e. the physical and structural components of computers, architectural issues, and peripheral devices.\n \n Also chip set. Also chassis, cabinet, box, tower, enclosure, housing, system unit, or simply case. Also simply PCI. Also Digital Versatile Disc. Also chip. Also LAN card or network card. Also solid-state disk or electronic disk. Also audio card. Also Serial AT Attachment. Also trackpad. Also graphics card.",
      "timestamp": "2025-10-09 19:12:24.603799"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_computer_hardware_terms",
      "title": "Glossary of computer hardware terms - Wikipedia",
      "description": "",
      "text": "\n This glossary of computer hardware terms is a list of definitions of terms and concepts related to computer hardware, i.e. the physical and structural components of computers, architectural issues, and peripheral devices.\n \n Also chip set. Also chassis, cabinet, box, tower, enclosure, housing, system unit, or simply case. Also simply PCI. Also Digital Versatile Disc. Also chip. Also LAN card or network card. Also solid-state disk or electronic disk. Also audio card. Also Serial AT Attachment. Also trackpad. Also graphics card.",
      "timestamp": "2025-10-09 19:12:24.607482"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_clinical_research",
      "text": "\n A glossary of terms used in clinical research.\n \n \u00a0This article incorporates public domain material from websites or documents of the United States Department of Health and Human Services.\n",
      "timestamp": "2025-10-09 19:12:25.019851"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_clinical_research",
      "title": "Glossary of clinical research - Wikipedia",
      "description": "",
      "text": "\n A glossary of terms used in clinical research.\n \n \u00a0This article incorporates public domain material from websites or documents of the United States Department of Health and Human Services.\n",
      "timestamp": "2025-10-09 19:12:25.022755"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_civil_engineering",
      "text": "This glossary of civil engineering terms is a list of definitions of terms and concepts pertaining specifically to civil engineering, its sub-disciplines, and related fields. For a more general overview of concepts within engineering as a whole, see Glossary of engineering.\n Also Abrams' water-cement ratio law.[3] Also decadic absorbance. Also paraffin. Also non-crystalline solid. Also building engineering or architecture engineering. Also statement of financial position. Also sometimes capillarity, capillary motion, capillary effect, or wicking. Also called Dalton's law of partial pressures. Also called engineering science. Also house wrap. Also ultimate strength or simply tensile strength (TS).",
      "timestamp": "2025-10-09 19:12:25.479506"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_civil_engineering",
      "title": "Glossary of civil engineering - Wikipedia",
      "description": "",
      "text": "This glossary of civil engineering terms is a list of definitions of terms and concepts pertaining specifically to civil engineering, its sub-disciplines, and related fields. For a more general overview of concepts within engineering as a whole, see Glossary of engineering.\n Also Abrams' water-cement ratio law.[3] Also decadic absorbance. Also paraffin. Also non-crystalline solid. Also building engineering or architecture engineering. Also statement of financial position. Also sometimes capillarity, capillary motion, capillary effect, or wicking. Also called Dalton's law of partial pressures. Also called engineering science. Also house wrap. Also ultimate strength or simply tensile strength (TS).",
      "timestamp": "2025-10-09 19:12:25.486664"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_chemistry_terms",
      "text": "\n This glossary of chemistry terms is a list of terms and definitions relevant to chemistry, including chemical laws, diagrams and formulae, laboratory tools, glassware, and equipment. Chemistry is a physical science concerned with the composition, structure, and properties of matter, as well as the changes it undergoes during chemical reactions; it features an extensive vocabulary and a significant amount of jargon.\n Note: All periodic table references refer to the IUPAC Style of the Periodic Table.\n Also acid ionization constant or acidity constant. Also actinoids. Also paraffin. Also olefin. Also acetylene. Also enplethy, chemical amount, or simply amount. Also amphiprotic. Also proton number. Also kindling point. Also main chain. Also Rutherford\u2013Bohr model. Also ebullition. Also Florence flask. Also vaporization point. Also simply called a buffer. Also stopper or cork. Also spelled buret. Also simply CAS Number. Also simply called a chemical. Also pure substance or simply substance. Also chromometer. Also molecular bond. Also unified atomic mass unit (u). Also drying agent. Also hydrogen-2 or heavy hydrogen, and symbolized 2H or D. Also coordinate covalent bond, coordinate bond, dative bond, and semipolar bond. Also solvation. Also malleability. Also electron magnetic moment. Also crystallization point. Also depression of freezing point. Also family. Also simply called Hess' law. Informally synonymous with proton. Also universal gas constant. Also general gas equation. Also ketoacid. Also lanthanoids. Also referred to as visible light. Also atomic mass number or nucleon number. Also liquefaction point. Also carbinyl. Also molality. Also molarity, amount concentration, or substance concentration. Also mole fraction. Sometimes used interchangeably with molecular weight and formula weight. Also inert gas. Also Lewis octet rule. Also orbital hybridization. Also osmolarity. Also oxidation number. Also oxidant, oxidizer, or electron acceptor. Also oxyacid or oxacid. Also amyl. Also simply the periodic table. Also peroxide and sometimes peroxo. Also spelled pipet. Also protogenic. (pl.) quanta Also free radical. Also radioisotope. Also called rare-earth metals or used interchangeably with lanthanides. Also rate law. Also rate-limiting step. Sometimes used interchangeably with reagent. Also simply intermediate. Also activity series. Also reductant, reducer, or electron donor. Also ultrasonication. Also massic heat capacity. Also stereocenter. Also spatial isomer. Also constitutional isomer. Also titrimetry or volumetric analysis. Also superheavy elements. Also transuranium elements. Also Dewar flask or thermos. Also equilibrium vapor pressure. Also boiling. Also water of hydration. Also bench chemistry or classical chemistry. Also inner salt and dipolar ion.",
      "timestamp": "2025-10-09 19:12:26.060097"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_chemistry_terms",
      "title": "Glossary of chemistry terms - Wikipedia",
      "description": "",
      "text": "\n This glossary of chemistry terms is a list of terms and definitions relevant to chemistry, including chemical laws, diagrams and formulae, laboratory tools, glassware, and equipment. Chemistry is a physical science concerned with the composition, structure, and properties of matter, as well as the changes it undergoes during chemical reactions; it features an extensive vocabulary and a significant amount of jargon.\n Note: All periodic table references refer to the IUPAC Style of the Periodic Table.\n Also acid ionization constant or acidity constant. Also actinoids. Also paraffin. Also olefin. Also acetylene. Also enplethy, chemical amount, or simply amount. Also amphiprotic. Also proton number. Also kindling point. Also main chain. Also Rutherford\u2013Bohr model. Also ebullition. Also Florence flask. Also vaporization point. Also simply called a buffer. Also stopper or cork. Also spelled buret. Also simply CAS Number. Also simply called a chemical. Also pure substance or simply substance. Also chromometer. Also molecular bond. Also unified atomic mass unit (u). Also drying agent. Also hydrogen-2 or heavy hydrogen, and symbolized 2H or D. Also coordinate covalent bond, coordinate bond, dative bond, and semipolar bond. Also solvation. Also malleability. Also electron magnetic moment. Also crystallization point. Also depression of freezing point. Also family. Also simply called Hess' law. Informally synonymous with proton. Also universal gas constant. Also general gas equation. Also ketoacid. Also lanthanoids. Also referred to as visible light. Also atomic mass number or nucleon number. Also liquefaction point. Also carbinyl. Also molality. Also molarity, amount concentration, or substance concentration. Also mole fraction. Sometimes used interchangeably with molecular weight and formula weight. Also inert gas. Also Lewis octet rule. Also orbital hybridization. Also osmolarity. Also oxidation number. Also oxidant, oxidizer, or electron acceptor. Also oxyacid or oxacid. Also amyl. Also simply the periodic table. Also peroxide and sometimes peroxo. Also spelled pipet. Also protogenic. (pl.) quanta Also free radical. Also radioisotope. Also called rare-earth metals or used interchangeably with lanthanides. Also rate law. Also rate-limiting step. Sometimes used interchangeably with reagent. Also simply intermediate. Also activity series. Also reductant, reducer, or electron donor. Also ultrasonication. Also massic heat capacity. Also stereocenter. Also spatial isomer. Also constitutional isomer. Also titrimetry or volumetric analysis. Also superheavy elements. Also transuranium elements. Also Dewar flask or thermos. Also equilibrium vapor pressure. Also boiling. Also water of hydration. Also bench chemistry or classical chemistry. Also inner salt and dipolar ion.",
      "timestamp": "2025-10-09 19:12:26.073726"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_cellular_and_molecular_biology_(M%E2%80%93Z)",
      "text": "\nThis glossary of cellular and molecular biology is a list of definitions of terms and concepts commonly used in the study of cell biology, molecular biology, and related disciplines, including molecular genetics, biochemistry, and microbiology.[1] It is split across two articles:\n This glossary is intended as introductory material for novices (for more specific and technical detail, see the article corresponding to each term). It has been designed as a companion to Glossary of genetics and evolutionary biology, which contains many overlapping and related terms; other related glossaries include Glossary of virology and Glossary of chemistry.\n \n Also meganucleus. Also next-generation sequencing (NGS) and second-generation sequencing. Also short tandem repeat (STR) or simple sequence repeat (SSR). (pl.) microtrabeculae Also ectosome and microparticle. Also mispairing. (pl.) mitochondria; also formerly chondriosome. Also M phase. Also somatic crossing over. Also phosphorodiamidate Morpholino oligomer. Also polylinker. \n Also negative regulation. Also nicking endonuclease and nickase. Sometimes used interchangeably with nucleobase or simply base. Also non-standard amino acid. Also point-nonsense mutation. Also nonsynonymous substitution or replacement mutation. Also amine terminus and amino terminus. Also nuclear localization sequence. Sometimes used interchangeably with nitrogenous base or simply base. Also prokaryon. Also karyoplasm. Also karyolymph or nuclear hyaloplasm. Also nucleoside monophosphate (NMP). pl. nuclei \n Also abbreviated oligo. Also one gene\u2013one protein or one gene\u2013one enzyme. Also umber. Also replication origin or simply origin. Also osmotic stress. Also electron transport-linked phosphorylation or terminal oxidation. \n Also Tumor protein P53 (TP53), transformation-related protein 53 (TRP53), and cellular tumor antigen p53. Also pachytene stage. Also palindrome. Also Pasteur-Meyerhof effect. Also extrinsic membrane protein. Also periplasm. Also phosphodiester backbone, sugar\u2013phosphate backbone, and phosphate\u2013sugar backbone. Also polyribosome or ergosome. Also map-based cloning. Also positive regulation. Also blast cell. Also peptidase. Also ingensin, macropain, prosome, multicatalytic proteinase, and multicatalytic endopeptidase complex. Also protein targeting. Abbreviated in shorthand with the letter R. Also pycnosis or karyopyknosis. Abbreviated in shorthand with the letter Y. \n Also real-time PCR (rtPCR). \n Also repetitious DNA. Also replication bubble. Also Y fork. Also restriction endonuclease, restriction exonuclease, or restrictase. Also restriction recognition site. Also ribonucleoside diphosphate reductase. Often abbreviated RNAP or RNApol. \n Also synthesis phase or synthetic phase. Also secondary messenger. Also selfish DNA or parasitic DNA. Denoted in shorthand with the symbol p. Also leader peptide, prepeptide, and presequence. Also vegetal cell or soma. Also ultrasonication. Also intergenic spacer (IGS) or non-transcribed spacer (NTS). Also termination codon. Also sumoylation. Also symplasm; pl. syncytia. Also synonymous substitution or samesense mutation. \n Also Goldberg-Hogness box. Also antisense strand, negative (-) sense strand, and noncoding strand. Also deoxythymidine. Also 5-methyluracil. Also transcription initiation site. Formerly referred to as soluble RNA (sRNA). Also transporter. Also transposon. Also triacylglycerol and triacylglyceride. Also tropic movement. Also turgidity. \n Also ubiquitylation. Also non-repetitive DNA. Also promotion. Also upstream activator sequence and upstream activation sequence. \n \n Denoted in shorthand with a + superscript. Also writhe. \n \n \n Also zygotene stage. \n",
      "timestamp": "2025-10-09 19:12:26.824030"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_cellular_and_molecular_biology_(M%E2%80%93Z)",
      "title": "Glossary of cellular and molecular biology (M\u2013Z) - Wikipedia",
      "description": "",
      "text": "\nThis glossary of cellular and molecular biology is a list of definitions of terms and concepts commonly used in the study of cell biology, molecular biology, and related disciplines, including molecular genetics, biochemistry, and microbiology.[1] It is split across two articles:\n This glossary is intended as introductory material for novices (for more specific and technical detail, see the article corresponding to each term). It has been designed as a companion to Glossary of genetics and evolutionary biology, which contains many overlapping and related terms; other related glossaries include Glossary of virology and Glossary of chemistry.\n \n Also meganucleus. Also next-generation sequencing (NGS) and second-generation sequencing. Also short tandem repeat (STR) or simple sequence repeat (SSR). (pl.) microtrabeculae Also ectosome and microparticle. Also mispairing. (pl.) mitochondria; also formerly chondriosome. Also M phase. Also somatic crossing over. Also phosphorodiamidate Morpholino oligomer. Also polylinker. \n Also negative regulation. Also nicking endonuclease and nickase. Sometimes used interchangeably with nucleobase or simply base. Also non-standard amino acid. Also point-nonsense mutation. Also nonsynonymous substitution or replacement mutation. Also amine terminus and amino terminus. Also nuclear localization sequence. Sometimes used interchangeably with nitrogenous base or simply base. Also prokaryon. Also karyoplasm. Also karyolymph or nuclear hyaloplasm. Also nucleoside monophosphate (NMP). pl. nuclei \n Also abbreviated oligo. Also one gene\u2013one protein or one gene\u2013one enzyme. Also umber. Also replication origin or simply origin. Also osmotic stress. Also electron transport-linked phosphorylation or terminal oxidation. \n Also Tumor protein P53 (TP53), transformation-related protein 53 (TRP53), and cellular tumor antigen p53. Also pachytene stage. Also palindrome. Also Pasteur-Meyerhof effect. Also extrinsic membrane protein. Also periplasm. Also phosphodiester backbone, sugar\u2013phosphate backbone, and phosphate\u2013sugar backbone. Also polyribosome or ergosome. Also map-based cloning. Also positive regulation. Also blast cell. Also peptidase. Also ingensin, macropain, prosome, multicatalytic proteinase, and multicatalytic endopeptidase complex. Also protein targeting. Abbreviated in shorthand with the letter R. Also pycnosis or karyopyknosis. Abbreviated in shorthand with the letter Y. \n Also real-time PCR (rtPCR). \n Also repetitious DNA. Also replication bubble. Also Y fork. Also restriction endonuclease, restriction exonuclease, or restrictase. Also restriction recognition site. Also ribonucleoside diphosphate reductase. Often abbreviated RNAP or RNApol. \n Also synthesis phase or synthetic phase. Also secondary messenger. Also selfish DNA or parasitic DNA. Denoted in shorthand with the symbol p. Also leader peptide, prepeptide, and presequence. Also vegetal cell or soma. Also ultrasonication. Also intergenic spacer (IGS) or non-transcribed spacer (NTS). Also termination codon. Also sumoylation. Also symplasm; pl. syncytia. Also synonymous substitution or samesense mutation. \n Also Goldberg-Hogness box. Also antisense strand, negative (-) sense strand, and noncoding strand. Also deoxythymidine. Also 5-methyluracil. Also transcription initiation site. Formerly referred to as soluble RNA (sRNA). Also transporter. Also transposon. Also triacylglycerol and triacylglyceride. Also tropic movement. Also turgidity. \n Also ubiquitylation. Also non-repetitive DNA. Also promotion. Also upstream activator sequence and upstream activation sequence. \n \n Denoted in shorthand with a + superscript. Also writhe. \n \n \n Also zygotene stage. \n",
      "timestamp": "2025-10-09 19:12:26.838467"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_cellular_and_molecular_biology_(0%E2%80%93L)",
      "text": "\nThis glossary of cellular and molecular biology is a list of definitions of terms and concepts commonly used in the study of cell biology, molecular biology, and related disciplines, including genetics, biochemistry, and microbiology.[1] It is split across two articles:\n This glossary is intended as introductory material for novices (for more specific and technical detail, see the article corresponding to each term). It has been designed as a companion to Glossary of genetics and evolutionary biology, which contains many overlapping and related terms; other related glossaries include Glossary of virology and Glossary of chemistry.\n \n Also three-prime untranslated region, 3' non-translated region (3'-NTR), and trailer sequence. Also three-prime end. Also five-prime cap. Also five-prime untranslated region, 5' non-translated region (5'-NTR), and leader sequence. Also five-prime end. \n Also binding site and catalytic site. Also sex chromosome, heterochromosome, or idiochromosome. Also differential splicing or simply splicing. Also tRNA-ligase. Also aminoacylated tRNA and charged tRNA. Also antisense transcript and antisense oligonucleotide (ASO). Also anuclear. Also compound X. Also autophagocytosis. \n Also \u03b2-oxidation. Also biological molecule. Also 5-bromodeoxyuridine. \n Also CAAT box or CAT box. Also cellular biology. Also plasma membrane, cytoplasmic membrane, and plasmalemma. Also cell communication. Also cell-mediated immunity. Also map unit (m.u.). (pl.) chiasmata Also idiomere. Also crossing over. (pl.) cilia Also cis-regulatory module (CRM). (pl.) cisternae Also sense strand, positive (+) sense strand, and nontemplate strand. Also abbreviated SHCoA and CoASH. Also copy DNA. Also confluency. Also canonical sequence. Also contact inhibition of growth or density-dependent inhibition. Also cooperative binding. Also CG island and C-G island. Also CG site and C-G site. Also CRISPR/Cas9 gene editing. (pl.) cristae Also cross-link. Also carboxyl terminus. Also C-value paradox. Also protoplasmic streaming and cyclosis. Also hyaloplasm and groundplasm. \n Denoted in shorthand with the symbol \u0394. Abbreviated in shorthand with dA. Abbreviated in shorthand with dC. Abbreviated in shorthand with dG. Also 2-deoxyribose. Denoted in shorthand with the somatic number 2n. Also diplotene stage. Also repression or suppression. \n Also modifier and modulator. Also electropermeabilization. Also extension. Also antigenic determinant. Also open chromatin. Also expression construct. Also intercellular matrix. Also extranuclear DNA and cytoplasmic DNA. \n (pl.) flagella Formerly known by the abbreviation MGED. \n Also Giemsa banding or G-banding. Also gene amplification. Also genetic modification or genetic manipulation. Also DNA testing or genetic screening. Also chromosome walking. Also chromosomal DNA. Also abbreviated GC-content. Also single guide RNA (sgRNA). \n Also hairpin loop or stem-loop. Denoted in shorthand with the somatic number n. Also inheritance. Also H-RNA. Also histone octamer and core particle. Also homeodomain responsive element. Also homologs or homologues. Also lateral gene transfer (LGT). Sometimes used interchangeably with lipophilic. \n Also ideogram. Also insertion element or simply insert. Also intrinsic membrane protein. Also transmembrane protein. Also interphase II. Also intragenic region. Also isoelectric pH. \n \n Also karyosphere. Also simply Kozak sequence. \n Also tagging. Also donor splicing junction or donor splicing site. Also leptotene stage. Also phospholipid bilayer. Also cellule, spherule, or spherulite. Plural loci. Denoted in shorthand with the symbol q. \n",
      "timestamp": "2025-10-09 19:12:27.450173"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_cellular_and_molecular_biology_(0%E2%80%93L)",
      "title": "Glossary of cellular and molecular biology (0\u2013L) - Wikipedia",
      "description": "",
      "text": "\nThis glossary of cellular and molecular biology is a list of definitions of terms and concepts commonly used in the study of cell biology, molecular biology, and related disciplines, including genetics, biochemistry, and microbiology.[1] It is split across two articles:\n This glossary is intended as introductory material for novices (for more specific and technical detail, see the article corresponding to each term). It has been designed as a companion to Glossary of genetics and evolutionary biology, which contains many overlapping and related terms; other related glossaries include Glossary of virology and Glossary of chemistry.\n \n Also three-prime untranslated region, 3' non-translated region (3'-NTR), and trailer sequence. Also three-prime end. Also five-prime cap. Also five-prime untranslated region, 5' non-translated region (5'-NTR), and leader sequence. Also five-prime end. \n Also binding site and catalytic site. Also sex chromosome, heterochromosome, or idiochromosome. Also differential splicing or simply splicing. Also tRNA-ligase. Also aminoacylated tRNA and charged tRNA. Also antisense transcript and antisense oligonucleotide (ASO). Also anuclear. Also compound X. Also autophagocytosis. \n Also \u03b2-oxidation. Also biological molecule. Also 5-bromodeoxyuridine. \n Also CAAT box or CAT box. Also cellular biology. Also plasma membrane, cytoplasmic membrane, and plasmalemma. Also cell communication. Also cell-mediated immunity. Also map unit (m.u.). (pl.) chiasmata Also idiomere. Also crossing over. (pl.) cilia Also cis-regulatory module (CRM). (pl.) cisternae Also sense strand, positive (+) sense strand, and nontemplate strand. Also abbreviated SHCoA and CoASH. Also copy DNA. Also confluency. Also canonical sequence. Also contact inhibition of growth or density-dependent inhibition. Also cooperative binding. Also CG island and C-G island. Also CG site and C-G site. Also CRISPR/Cas9 gene editing. (pl.) cristae Also cross-link. Also carboxyl terminus. Also C-value paradox. Also protoplasmic streaming and cyclosis. Also hyaloplasm and groundplasm. \n Denoted in shorthand with the symbol \u0394. Abbreviated in shorthand with dA. Abbreviated in shorthand with dC. Abbreviated in shorthand with dG. Also 2-deoxyribose. Denoted in shorthand with the somatic number 2n. Also diplotene stage. Also repression or suppression. \n Also modifier and modulator. Also electropermeabilization. Also extension. Also antigenic determinant. Also open chromatin. Also expression construct. Also intercellular matrix. Also extranuclear DNA and cytoplasmic DNA. \n (pl.) flagella Formerly known by the abbreviation MGED. \n Also Giemsa banding or G-banding. Also gene amplification. Also genetic modification or genetic manipulation. Also DNA testing or genetic screening. Also chromosome walking. Also chromosomal DNA. Also abbreviated GC-content. Also single guide RNA (sgRNA). \n Also hairpin loop or stem-loop. Denoted in shorthand with the somatic number n. Also inheritance. Also H-RNA. Also histone octamer and core particle. Also homeodomain responsive element. Also homologs or homologues. Also lateral gene transfer (LGT). Sometimes used interchangeably with lipophilic. \n Also ideogram. Also insertion element or simply insert. Also intrinsic membrane protein. Also transmembrane protein. Also interphase II. Also intragenic region. Also isoelectric pH. \n \n Also karyosphere. Also simply Kozak sequence. \n Also tagging. Also donor splicing junction or donor splicing site. Also leptotene stage. Also phospholipid bilayer. Also cellule, spherule, or spherulite. Plural loci. Denoted in shorthand with the symbol q. \n",
      "timestamp": "2025-10-09 19:12:27.465970"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_cell_biology",
      "text": "\nThis glossary of cellular and molecular biology is a list of definitions of terms and concepts commonly used in the study of cell biology, molecular biology, and related disciplines, including genetics, biochemistry, and microbiology.[1] It is split across two articles:\n This glossary is intended as introductory material for novices (for more specific and technical detail, see the article corresponding to each term). It has been designed as a companion to Glossary of genetics and evolutionary biology, which contains many overlapping and related terms; other related glossaries include Glossary of virology and Glossary of chemistry.\n \n Also three-prime untranslated region, 3' non-translated region (3'-NTR), and trailer sequence. Also three-prime end. Also five-prime cap. Also five-prime untranslated region, 5' non-translated region (5'-NTR), and leader sequence. Also five-prime end. \n Also binding site and catalytic site. Also sex chromosome, heterochromosome, or idiochromosome. Also differential splicing or simply splicing. Also tRNA-ligase. Also aminoacylated tRNA and charged tRNA. Also antisense transcript and antisense oligonucleotide (ASO). Also anuclear. Also compound X. Also autophagocytosis. \n Also \u03b2-oxidation. Also biological molecule. Also 5-bromodeoxyuridine. \n Also CAAT box or CAT box. Also cellular biology. Also plasma membrane, cytoplasmic membrane, and plasmalemma. Also cell communication. Also cell-mediated immunity. Also map unit (m.u.). (pl.) chiasmata Also idiomere. Also crossing over. (pl.) cilia Also cis-regulatory module (CRM). (pl.) cisternae Also sense strand, positive (+) sense strand, and nontemplate strand. Also abbreviated SHCoA and CoASH. Also copy DNA. Also confluency. Also canonical sequence. Also contact inhibition of growth or density-dependent inhibition. Also cooperative binding. Also CG island and C-G island. Also CG site and C-G site. Also CRISPR/Cas9 gene editing. (pl.) cristae Also cross-link. Also carboxyl terminus. Also C-value paradox. Also protoplasmic streaming and cyclosis. Also hyaloplasm and groundplasm. \n Denoted in shorthand with the symbol \u0394. Abbreviated in shorthand with dA. Abbreviated in shorthand with dC. Abbreviated in shorthand with dG. Also 2-deoxyribose. Denoted in shorthand with the somatic number 2n. Also diplotene stage. Also repression or suppression. \n Also modifier and modulator. Also electropermeabilization. Also extension. Also antigenic determinant. Also open chromatin. Also expression construct. Also intercellular matrix. Also extranuclear DNA and cytoplasmic DNA. \n (pl.) flagella Formerly known by the abbreviation MGED. \n Also Giemsa banding or G-banding. Also gene amplification. Also genetic modification or genetic manipulation. Also DNA testing or genetic screening. Also chromosome walking. Also chromosomal DNA. Also abbreviated GC-content. Also single guide RNA (sgRNA). \n Also hairpin loop or stem-loop. Denoted in shorthand with the somatic number n. Also inheritance. Also H-RNA. Also histone octamer and core particle. Also homeodomain responsive element. Also homologs or homologues. Also lateral gene transfer (LGT). Sometimes used interchangeably with lipophilic. \n Also ideogram. Also insertion element or simply insert. Also intrinsic membrane protein. Also transmembrane protein. Also interphase II. Also intragenic region. Also isoelectric pH. \n \n Also karyosphere. Also simply Kozak sequence. \n Also tagging. Also donor splicing junction or donor splicing site. Also leptotene stage. Also phospholipid bilayer. Also cellule, spherule, or spherulite. Plural loci. Denoted in shorthand with the symbol q. \n",
      "timestamp": "2025-10-09 19:12:28.114092"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_cell_biology",
      "title": "Glossary of cellular and molecular biology (0\u2013L) - Wikipedia",
      "description": "",
      "text": "\nThis glossary of cellular and molecular biology is a list of definitions of terms and concepts commonly used in the study of cell biology, molecular biology, and related disciplines, including genetics, biochemistry, and microbiology.[1] It is split across two articles:\n This glossary is intended as introductory material for novices (for more specific and technical detail, see the article corresponding to each term). It has been designed as a companion to Glossary of genetics and evolutionary biology, which contains many overlapping and related terms; other related glossaries include Glossary of virology and Glossary of chemistry.\n \n Also three-prime untranslated region, 3' non-translated region (3'-NTR), and trailer sequence. Also three-prime end. Also five-prime cap. Also five-prime untranslated region, 5' non-translated region (5'-NTR), and leader sequence. Also five-prime end. \n Also binding site and catalytic site. Also sex chromosome, heterochromosome, or idiochromosome. Also differential splicing or simply splicing. Also tRNA-ligase. Also aminoacylated tRNA and charged tRNA. Also antisense transcript and antisense oligonucleotide (ASO). Also anuclear. Also compound X. Also autophagocytosis. \n Also \u03b2-oxidation. Also biological molecule. Also 5-bromodeoxyuridine. \n Also CAAT box or CAT box. Also cellular biology. Also plasma membrane, cytoplasmic membrane, and plasmalemma. Also cell communication. Also cell-mediated immunity. Also map unit (m.u.). (pl.) chiasmata Also idiomere. Also crossing over. (pl.) cilia Also cis-regulatory module (CRM). (pl.) cisternae Also sense strand, positive (+) sense strand, and nontemplate strand. Also abbreviated SHCoA and CoASH. Also copy DNA. Also confluency. Also canonical sequence. Also contact inhibition of growth or density-dependent inhibition. Also cooperative binding. Also CG island and C-G island. Also CG site and C-G site. Also CRISPR/Cas9 gene editing. (pl.) cristae Also cross-link. Also carboxyl terminus. Also C-value paradox. Also protoplasmic streaming and cyclosis. Also hyaloplasm and groundplasm. \n Denoted in shorthand with the symbol \u0394. Abbreviated in shorthand with dA. Abbreviated in shorthand with dC. Abbreviated in shorthand with dG. Also 2-deoxyribose. Denoted in shorthand with the somatic number 2n. Also diplotene stage. Also repression or suppression. \n Also modifier and modulator. Also electropermeabilization. Also extension. Also antigenic determinant. Also open chromatin. Also expression construct. Also intercellular matrix. Also extranuclear DNA and cytoplasmic DNA. \n (pl.) flagella Formerly known by the abbreviation MGED. \n Also Giemsa banding or G-banding. Also gene amplification. Also genetic modification or genetic manipulation. Also DNA testing or genetic screening. Also chromosome walking. Also chromosomal DNA. Also abbreviated GC-content. Also single guide RNA (sgRNA). \n Also hairpin loop or stem-loop. Denoted in shorthand with the somatic number n. Also inheritance. Also H-RNA. Also histone octamer and core particle. Also homeodomain responsive element. Also homologs or homologues. Also lateral gene transfer (LGT). Sometimes used interchangeably with lipophilic. \n Also ideogram. Also insertion element or simply insert. Also intrinsic membrane protein. Also transmembrane protein. Also interphase II. Also intragenic region. Also isoelectric pH. \n \n Also karyosphere. Also simply Kozak sequence. \n Also tagging. Also donor splicing junction or donor splicing site. Also leptotene stage. Also phospholipid bilayer. Also cellule, spherule, or spherulite. Plural loci. Denoted in shorthand with the symbol q. \n",
      "timestamp": "2025-10-09 19:12:28.129057"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_calculus",
      "text": "\nMost of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\n This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.\n where b is a positive real number, and in which the argument x occurs as an exponent. For real numbers c and d, a function of the form \n\n\n\nf\n(\nx\n)\n=\na\n\nb\n\nc\nx\n+\nd\n\n\n\n\n{\\displaystyle f(x)=ab^{cx+d}}\n\n is also an exponential function, as it can be rewritten as \n",
      "timestamp": "2025-10-09 19:12:28.620464"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_calculus",
      "title": "Glossary of calculus - Wikipedia",
      "description": "",
      "text": "\nMost of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\n This glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.\n where b is a positive real number, and in which the argument x occurs as an exponent. For real numbers c and d, a function of the form \n\n\n\nf\n(\nx\n)\n=\na\n\nb\n\nc\nx\n+\nd\n\n\n\n\n{\\displaystyle f(x)=ab^{cx+d}}\n\n is also an exponential function, as it can be rewritten as \n",
      "timestamp": "2025-10-09 19:12:28.630353"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_botanical_terms",
      "text": "\n This glossary of botanical terms is a list of definitions of terms and concepts relevant to botany and plants in general. Terms of plant morphology are included here as well as at the more specific Glossary of plant morphology and Glossary of leaf morphology. For other related terms, see Glossary of phytopathology, Glossary of lichen terms, and List of Latin and Greek words commonly used in systematic names.\n pl. adelphiae Also graminology. pl. apices; adj. apical pl. aphlebiae adj. apomictic pl. arboreta Plural archegonia. \n pl. brochi \n pl. calluses pl. calyces pl. caudices adj. cauliflorous sing. cilium; adj. ciliate adj. clinal adj. cormose, cormous pl. cortexes or cortices adj. corymbose pl. cyathia adj. cymose \n Also abbreviated dicot. Also spelled disk. sing. domatium \n Also aglandular Also elliptic. \n adj. fasciculate pl. fimbriae \n pl. genera Also globular. Also gramineous \n pl. herbaria (never capitalized) \n \n \n adj. keeled \n pl. lamellae adj. lamellate \n Also midvein. dim. mucronule. pl. mycorrhizae; adj. mycorrhizal adj. mycotrophic \n adj. nectariferous \n Also spelled ochrea. Also imparipinnate pl. opera utique oppressa \n pl. paleae adj. paniculate pl. papillae; adj. papillose or papillate Also paraperigone. Also patulous. adj. pedicellate adj. pedunculate adj. penninerved adj. perulate adj. phyllodineous Also phytomelanin; adj. phytomelanous pl. pinnae adj. prickly Also puberulent. \n \n adj. racemose, pl. rachises or rachides \n adj. saprophytic adj. saprotrophic Also scabrous adj. scapose adj. sclerophyllous pl. septa pl. setae; adj. setose, setaceous pl. sori adj. spathaceous adj. spicate adj. spicate adj. spinose pl. squamulae; adj. squamulose adj. staminate Also male flower. Also stipel; pl. stipellae Also runner. pl. stomata pl. strobili Also undershrub pl. suffrutices Also sym-. \n pl. taxa Also semiterete Obsolete pl. thalli Also tomentose \n \n Often variety in common usage and abbreviated as var. Also nerve. Diminutive: virgulate pl. vittae \n \n \n \n",
      "timestamp": "2025-10-09 19:12:30.207396"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_botanical_terms",
      "title": "Glossary of botanical terms - Wikipedia",
      "description": "",
      "text": "\n This glossary of botanical terms is a list of definitions of terms and concepts relevant to botany and plants in general. Terms of plant morphology are included here as well as at the more specific Glossary of plant morphology and Glossary of leaf morphology. For other related terms, see Glossary of phytopathology, Glossary of lichen terms, and List of Latin and Greek words commonly used in systematic names.\n pl. adelphiae Also graminology. pl. apices; adj. apical pl. aphlebiae adj. apomictic pl. arboreta Plural archegonia. \n pl. brochi \n pl. calluses pl. calyces pl. caudices adj. cauliflorous sing. cilium; adj. ciliate adj. clinal adj. cormose, cormous pl. cortexes or cortices adj. corymbose pl. cyathia adj. cymose \n Also abbreviated dicot. Also spelled disk. sing. domatium \n Also aglandular Also elliptic. \n adj. fasciculate pl. fimbriae \n pl. genera Also globular. Also gramineous \n pl. herbaria (never capitalized) \n \n \n adj. keeled \n pl. lamellae adj. lamellate \n Also midvein. dim. mucronule. pl. mycorrhizae; adj. mycorrhizal adj. mycotrophic \n adj. nectariferous \n Also spelled ochrea. Also imparipinnate pl. opera utique oppressa \n pl. paleae adj. paniculate pl. papillae; adj. papillose or papillate Also paraperigone. Also patulous. adj. pedicellate adj. pedunculate adj. penninerved adj. perulate adj. phyllodineous Also phytomelanin; adj. phytomelanous pl. pinnae adj. prickly Also puberulent. \n \n adj. racemose, pl. rachises or rachides \n adj. saprophytic adj. saprotrophic Also scabrous adj. scapose adj. sclerophyllous pl. septa pl. setae; adj. setose, setaceous pl. sori adj. spathaceous adj. spicate adj. spicate adj. spinose pl. squamulae; adj. squamulose adj. staminate Also male flower. Also stipel; pl. stipellae Also runner. pl. stomata pl. strobili Also undershrub pl. suffrutices Also sym-. \n pl. taxa Also semiterete Obsolete pl. thalli Also tomentose \n \n Often variety in common usage and abbreviated as var. Also nerve. Diminutive: virgulate pl. vittae \n \n \n \n",
      "timestamp": "2025-10-09 19:12:30.229105"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_biology",
      "text": "\n This glossary of biology terms is a list of definitions of fundamental terms and concepts used in biology, the study of life and of living organisms. It is intended as introductory material for novices; for more specific and technical definitions from sub-disciplines and related fields, see Glossary of cell biology, Glossary of genetics, Glossary of evolutionary biology, Glossary of ecology, Glossary of environmental science and Glossary of scientific naming, or any of the organism-specific glossaries in Category:Glossaries of biology.\n Also called an antibacterial. Also called selective breeding. Sometimes used interchangeably with primary producer. Also called the biosynthetic phase, light-independent reactions, dark reactions, or photosynthetic carbon reduction (PCR) cycle. Also called carbon assimilation. Also called cytology. Also called the Krebs cycle and tricarboxylic acid cycle (TCA). Also called the macula adhaerens. Also called a trophic pyramid, eltonian pyramid, energy pyramid, or sometimes food pyramid. Sometimes called an ecospecies. Also called a nonspontaneous reaction or unfavorable reaction. Also called symbiogenesis. Also spelled foetus. (pl.) flagella (pl.) foramimina Also called an exotic species, foreign species, alien species, non-native species, or non-indigenous species. Also called a white blood cell. (sing.) mitochondrion Also called neuroscience. Also called autoecology. Also called behavioral neuroscience, biological psychology, and biopsychology. Also called procreation or breeding. (pl.) taxa Also called a neoplasm.",
      "timestamp": "2025-10-09 19:12:30.708197"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_biology",
      "title": "Glossary of biology - Wikipedia",
      "description": "",
      "text": "\n This glossary of biology terms is a list of definitions of fundamental terms and concepts used in biology, the study of life and of living organisms. It is intended as introductory material for novices; for more specific and technical definitions from sub-disciplines and related fields, see Glossary of cell biology, Glossary of genetics, Glossary of evolutionary biology, Glossary of ecology, Glossary of environmental science and Glossary of scientific naming, or any of the organism-specific glossaries in Category:Glossaries of biology.\n Also called an antibacterial. Also called selective breeding. Sometimes used interchangeably with primary producer. Also called the biosynthetic phase, light-independent reactions, dark reactions, or photosynthetic carbon reduction (PCR) cycle. Also called carbon assimilation. Also called cytology. Also called the Krebs cycle and tricarboxylic acid cycle (TCA). Also called the macula adhaerens. Also called a trophic pyramid, eltonian pyramid, energy pyramid, or sometimes food pyramid. Sometimes called an ecospecies. Also called a nonspontaneous reaction or unfavorable reaction. Also called symbiogenesis. Also spelled foetus. (pl.) flagella (pl.) foramimina Also called an exotic species, foreign species, alien species, non-native species, or non-indigenous species. Also called a white blood cell. (sing.) mitochondrion Also called neuroscience. Also called autoecology. Also called behavioral neuroscience, biological psychology, and biopsychology. Also called procreation or breeding. (pl.) taxa Also called a neoplasm.",
      "timestamp": "2025-10-09 19:12:30.716131"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_astronomy",
      "text": "\n\nThis glossary of astronomy is a list of definitions of terms and concepts relevant to astronomy and cosmology, their sub-disciplines, and related fields. Astronomy is concerned with the study of celestial objects and phenomena that originate outside the atmosphere of Earth. The field of astronomy features an extensive vocabulary and a significant amount of jargon.\n Also visual brightness (V). Also argument of perifocus or argument of pericenter. Also the north node. Also exobiology. Also planetary geology. Also celestial body. Also spelled astronomical catalog. Also celestial object. Also obliquity. Also critical velocity or critical rotation. Also circumstellar disk. Also compact object. Also space dust. Also cosmic microwave background radiation (CMBR). Also break-up velocity. Also meridian transit. Also the south node. Also distant detached object and extended scattered disc object. Also ecliptic plane or plane of the ecliptic. Also elliptic orbit. Also exoplanet. Also the Cusp of Aries. Also background stars. Also galactic core or galactic center. Also galactic year or cosmic year. Also group of galaxies (GrG). Also geosynchronous equatorial orbit (GEO). Also the Hill radius. Also Laplace's invariable plane or the Laplace plane. Also Keplerian orbit. Also Edgeworth\u2013Kuiper belt. Also Lagrange point, libration point, or L-point. Also the Lenakaeia Supercluster, Local Supercluster, or Local SCI. Also Moon phase. Also the Northward equinox. Also shooting star or falling star. Also normalized polar moment of inertia. Also minor moon or minor natural satellite. Also MK classification. Also rise width. Also stellar association. Also bare eye or unaided eye. Also moon. Also arc length. Also the \u00d6pik\u2013Oort cloud. Also orbital plot. Also revolution period. Also simply space. Also pericenter. Also reference plane. Also planetary object. Also planetology. Also planemo or planetary body. Also gravitational primary, primary body, or central body. Also direct motion. Also proplyd. Also quasi-stellar radio source. Also interstellar planet, nomad planet, orphan planet, and starless planet. Also twinkling. Also major semi-axis. Also southward equinox. Also positional astronomy. Also standard acceleration due to gravity. Also spelled star catalog. Also stellar system. Also stellar envelope. Also spectral classification. Also simply stellar model. Also substar. Also synodic rotation period. Also tidal acceleration. Also tide-generating force. Also Tisserand parameter. Also the Johnson system or Johnson\u2013Morgan system. Also the Local Supercluster (LSC or LC). An acronym of X-ray bright optically normal galaxy. Also Zone of Galactic Obscuration (ZGO).",
      "timestamp": "2025-10-09 19:12:31.365397"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_astronomy",
      "title": "Glossary of astronomy - Wikipedia",
      "description": "",
      "text": "\n\nThis glossary of astronomy is a list of definitions of terms and concepts relevant to astronomy and cosmology, their sub-disciplines, and related fields. Astronomy is concerned with the study of celestial objects and phenomena that originate outside the atmosphere of Earth. The field of astronomy features an extensive vocabulary and a significant amount of jargon.\n Also visual brightness (V). Also argument of perifocus or argument of pericenter. Also the north node. Also exobiology. Also planetary geology. Also celestial body. Also spelled astronomical catalog. Also celestial object. Also obliquity. Also critical velocity or critical rotation. Also circumstellar disk. Also compact object. Also space dust. Also cosmic microwave background radiation (CMBR). Also break-up velocity. Also meridian transit. Also the south node. Also distant detached object and extended scattered disc object. Also ecliptic plane or plane of the ecliptic. Also elliptic orbit. Also exoplanet. Also the Cusp of Aries. Also background stars. Also galactic core or galactic center. Also galactic year or cosmic year. Also group of galaxies (GrG). Also geosynchronous equatorial orbit (GEO). Also the Hill radius. Also Laplace's invariable plane or the Laplace plane. Also Keplerian orbit. Also Edgeworth\u2013Kuiper belt. Also Lagrange point, libration point, or L-point. Also the Lenakaeia Supercluster, Local Supercluster, or Local SCI. Also Moon phase. Also the Northward equinox. Also shooting star or falling star. Also normalized polar moment of inertia. Also minor moon or minor natural satellite. Also MK classification. Also rise width. Also stellar association. Also bare eye or unaided eye. Also moon. Also arc length. Also the \u00d6pik\u2013Oort cloud. Also orbital plot. Also revolution period. Also simply space. Also pericenter. Also reference plane. Also planetary object. Also planetology. Also planemo or planetary body. Also gravitational primary, primary body, or central body. Also direct motion. Also proplyd. Also quasi-stellar radio source. Also interstellar planet, nomad planet, orphan planet, and starless planet. Also twinkling. Also major semi-axis. Also southward equinox. Also positional astronomy. Also standard acceleration due to gravity. Also spelled star catalog. Also stellar system. Also stellar envelope. Also spectral classification. Also simply stellar model. Also substar. Also synodic rotation period. Also tidal acceleration. Also tide-generating force. Also Tisserand parameter. Also the Johnson system or Johnson\u2013Morgan system. Also the Local Supercluster (LSC or LC). An acronym of X-ray bright optically normal galaxy. Also Zone of Galactic Obscuration (ZGO).",
      "timestamp": "2025-10-09 19:12:31.376629"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence",
      "text": "\n This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, Glossary of machine vision, and Glossary of logic.\n Pronounced \"A-star\". Also abduction. Also adaptive network-based fuzzy inference system. Also artificial emotional intelligence or emotion AI. Also fuzzy string searching. Also argumentation system. Also machine intelligence. Also simply AI planning. Also self-driving car, robot car, and driverless car. Also backward reasoning. Also stochastic Hopfield network with hidden units. Also propositional satisfiability problem; abbreviated SATISFIABILITY or SAT. Also bagging or bootstrapping. Also self-learning know-how system. Also exhaustive search or generate and test. Also smartbot, talkbot, chatterbot, bot, IM bot, interactive agent, conversational interface, or artificial conversational entity. Also clustering. Also artificial creativity, mechanical creativity, creative computing, or creative computation. Also theoretical neuroscience or mathematical neuroscience. Also algorithmic number theory. Also statistical computing. Also conlang. Also recombination. Also dataset. Also enterprise data warehouse (EDW). Also theory of choice. Also epigenetic robotics. Also conversational agent (CA). Also dimension reduction. Also decentralized artificial intelligence. Also dilution. Also interface agent. Also representation learning. Also first-order predicate calculus or predicate logic. Also forward reasoning. Also friendly AI or FAI. Also graph search. Also cognitive augmentation, machine augmented intelligence, and enhanced intelligence. Also virtual assistant or personal digital assistant. Also logic tree. Also Clique Tree. \n Also mathematical programming. Also computer audition (CA). Also mechatronic engineering. Also self-organized system. Also entity identification, entity chunking, and entity extraction. Also brain\u2013computer interface (BCI), neural-control interface (NCI), mind-machine interface (MMI), direct neural interface (DNI), or brain\u2013machine interface (BMI). Also neuromorphic computing. Also non-deterministic polynomial-time hardness. Also Ockham's razor or Ocham's razor. Also ontology extraction, ontology generation, or ontology acquisition. Also pathing. Also first-order logic, predicate logic, and first-order predicate calculus. Also rationality principle. Also propositional logic, statement logic, sentential calculus, sentential logic, and zeroth-order logic. Also random decision forest. Also frame network. Also reasoning engine, rules engine, or simply reasoner. Also weak supervision. Also simply SLD resolution. Also sparse coding or SDL. Also simply the singularity. Abbreviated H+ or h+. Also tree search. Also narrow AI.",
      "timestamp": "2025-10-09 19:12:36.556870"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence",
      "title": "Glossary of artificial intelligence - Wikipedia",
      "description": "",
      "text": "\n This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, Glossary of machine vision, and Glossary of logic.\n Pronounced \"A-star\". Also abduction. Also adaptive network-based fuzzy inference system. Also artificial emotional intelligence or emotion AI. Also fuzzy string searching. Also argumentation system. Also machine intelligence. Also simply AI planning. Also self-driving car, robot car, and driverless car. Also backward reasoning. Also stochastic Hopfield network with hidden units. Also propositional satisfiability problem; abbreviated SATISFIABILITY or SAT. Also bagging or bootstrapping. Also self-learning know-how system. Also exhaustive search or generate and test. Also smartbot, talkbot, chatterbot, bot, IM bot, interactive agent, conversational interface, or artificial conversational entity. Also clustering. Also artificial creativity, mechanical creativity, creative computing, or creative computation. Also theoretical neuroscience or mathematical neuroscience. Also algorithmic number theory. Also statistical computing. Also conlang. Also recombination. Also dataset. Also enterprise data warehouse (EDW). Also theory of choice. Also epigenetic robotics. Also conversational agent (CA). Also dimension reduction. Also decentralized artificial intelligence. Also dilution. Also interface agent. Also representation learning. Also first-order predicate calculus or predicate logic. Also forward reasoning. Also friendly AI or FAI. Also graph search. Also cognitive augmentation, machine augmented intelligence, and enhanced intelligence. Also virtual assistant or personal digital assistant. Also logic tree. Also Clique Tree. \n Also mathematical programming. Also computer audition (CA). Also mechatronic engineering. Also self-organized system. Also entity identification, entity chunking, and entity extraction. Also brain\u2013computer interface (BCI), neural-control interface (NCI), mind-machine interface (MMI), direct neural interface (DNI), or brain\u2013machine interface (BMI). Also neuromorphic computing. Also non-deterministic polynomial-time hardness. Also Ockham's razor or Ocham's razor. Also ontology extraction, ontology generation, or ontology acquisition. Also pathing. Also first-order logic, predicate logic, and first-order predicate calculus. Also rationality principle. Also propositional logic, statement logic, sentential calculus, sentential logic, and zeroth-order logic. Also random decision forest. Also frame network. Also reasoning engine, rules engine, or simply reasoner. Also weak supervision. Also simply SLD resolution. Also sparse coding or SDL. Also simply the singularity. Abbreviated H+ or h+. Also tree search. Also narrow AI.",
      "timestamp": "2025-10-09 19:12:36.573220"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_architecture",
      "text": "\nThis page is a glossary of architecture.\n \n",
      "timestamp": "2025-10-09 19:12:36.969597"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_architecture",
      "title": "Glossary of architecture - Wikipedia",
      "description": "",
      "text": "\nThis page is a glossary of architecture.\n \n",
      "timestamp": "2025-10-09 19:12:36.974060"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_archaeology",
      "text": "\nThis page is a glossary of archaeology, the study of the human past from material remains.\n \n",
      "timestamp": "2025-10-09 19:12:37.334446"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_archaeology",
      "title": "Glossary of archaeology - Wikipedia",
      "description": "",
      "text": "\nThis page is a glossary of archaeology, the study of the human past from material remains.\n \n",
      "timestamp": "2025-10-09 19:12:37.337326"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_agriculture",
      "text": "This glossary of agriculture is a list of definitions of terms and concepts used in agriculture, its sub-disciplines, and related fields, including horticulture, animal husbandry, agribusiness, and agricultural policy. For other glossaries relevant to agricultural science, see Glossary of biology, Glossary of ecology, Glossary of environmental science, and Glossary of botanical terms.\n (pl.) aboiteaux Also farmers' co-op or simply a co-op. Also agriscience or ag science, and often pluralized as in agricultural sciences. Also agriculturalist, agricultural scientist, agrologist, or agronomist. Also veganic farming. Also beekeeping. Also aquafarming. Also selective breeding. Also sometimes barb wire. Also catalo. Also rendered board-foot and abbreviated as BDFT or BF. Also miller's bran. Also U-fork or grelinette. Sometimes used interchangeably with fryer. Also incubating. Also billy goat. (pl.) calves Also profit crop. Also circle irrigation or water-wheel irrigation. Often used interchangeably with fertigation. Also chisel plow. Also communal farming. Also simply combine. Also contouring. Also corn house, ambar, or h\u00f3rreo. Also single-suckler herd. Also aerial application or topdressing. Also cultigen. Also cultivated meat or lab-grown meat. Also deflowering. Also worming. Also diatomite, celite, or kieselguhr. Also dibble or dibbler. Also smart farming and e-agriculture. Also cropping or tailing. Also doddy, dody, and duddie. Also nanny goat. Also dogey, dogy, and doggie. Also dovecot and columbarium. Also draught animal. Also towbar. Also trickle irrigation. Also drop pen. Also driving. Also dry farming or arid-zone agriculture. Also dewattling. Also intact. Also extensive farming. Also farm shop. Also feed yard. Also fertiliser. Also conservation buffer or buffer strip. Also fattening. Also flower farming. Also provender, animal feed, or simply feed. Also edible landscaping. Also laminitis. Also farrowing crate or sow stall. Also cattle prod or simply prod. Also simply elevator. Also grain elevator or bucket elevator. Also pastoralist or runholder. Also called the Third Agricultural Revolution. Also glasshouse. Also plough pan. Also haystack or simply stack or cock. Also haymow. Also hayrick or simply rick. Also turnrow. Also mustering. Also ridging or earthing up. Also centum weight or quintal. Also heterosis or outbreeding enhancement. Also technical crop. Also integrated farm management (IFM) or integrated production (IP). Also intensive farming. Also factory farming. Also interculture. Also jennet. Also timber. Also linchet. Also malt barn or maltings. Also trough or feeder. Also muck spreader or honey wagon. Also mechanised agriculture. Also millrun, lade, leat, flume, or penstock. Also continuous cropping. Also fixing. Also oast house or hop kiln. Also organic agriculture, biological farming, and ecological farming. Also natural fertilizer. Plural oxen; also bullock. Also simply paddy. Also livestock farming or grazing. Also pellet press. Also simply wilting point (WP). Also molecular farming, molecular pharming, and biopharming. Also hog. Also fish farming. Also plashing. Also plow. Also plow pan. Also plow planting. Also plowing. Also plowshare. Also poddy calf. Also polyhouse, hoophouse, grow tunnel, or high tunnel. Also fruticulture. Also satellite farming and site-specific crop management. Also pseudograin. Also simply quern. Also cattleman or stockgrower. Also simply range. Also rootcrop. Also rooting depth or rhizosphere. Also seedling bed. Also woolshed. Also abattoir. Also truck row. Also slurry tank, slurry lagoon, or slurry store. Also soil improvement or soil conditioner. Also microbial inoculant and bioinoculant. Often used interchangeably with seeding and planting. Also simply staple. Also cattle station, sheep station, or run. Also shock or stack. Also pigsty, pig pen, pig parlor, or pig-cote. Also subsurface irrigation or seepage irrigation. Also flat lifter. Also summer pasture. Also sun-dried. Also southwest injury. Also windrower. Also pig or hog. Also hay tedder. Also conditioning. Also thresher. Also topdressing. Also TTC assay or tetrazolium test. Also outplanting and replanting. Also treillage. Also winegrowing. Also two-wheel tractor or single-axle tractor. Also water mill. Also weaner. Also wheat mill run (WMR), millfeed (MF), and midds. Also shelterbelt. Also agricultural output.",
      "timestamp": "2025-10-09 19:12:37.979047"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_agriculture",
      "title": "Glossary of agriculture - Wikipedia",
      "description": "",
      "text": "This glossary of agriculture is a list of definitions of terms and concepts used in agriculture, its sub-disciplines, and related fields, including horticulture, animal husbandry, agribusiness, and agricultural policy. For other glossaries relevant to agricultural science, see Glossary of biology, Glossary of ecology, Glossary of environmental science, and Glossary of botanical terms.\n (pl.) aboiteaux Also farmers' co-op or simply a co-op. Also agriscience or ag science, and often pluralized as in agricultural sciences. Also agriculturalist, agricultural scientist, agrologist, or agronomist. Also veganic farming. Also beekeeping. Also aquafarming. Also selective breeding. Also sometimes barb wire. Also catalo. Also rendered board-foot and abbreviated as BDFT or BF. Also miller's bran. Also U-fork or grelinette. Sometimes used interchangeably with fryer. Also incubating. Also billy goat. (pl.) calves Also profit crop. Also circle irrigation or water-wheel irrigation. Often used interchangeably with fertigation. Also chisel plow. Also communal farming. Also simply combine. Also contouring. Also corn house, ambar, or h\u00f3rreo. Also single-suckler herd. Also aerial application or topdressing. Also cultigen. Also cultivated meat or lab-grown meat. Also deflowering. Also worming. Also diatomite, celite, or kieselguhr. Also dibble or dibbler. Also smart farming and e-agriculture. Also cropping or tailing. Also doddy, dody, and duddie. Also nanny goat. Also dogey, dogy, and doggie. Also dovecot and columbarium. Also draught animal. Also towbar. Also trickle irrigation. Also drop pen. Also driving. Also dry farming or arid-zone agriculture. Also dewattling. Also intact. Also extensive farming. Also farm shop. Also feed yard. Also fertiliser. Also conservation buffer or buffer strip. Also fattening. Also flower farming. Also provender, animal feed, or simply feed. Also edible landscaping. Also laminitis. Also farrowing crate or sow stall. Also cattle prod or simply prod. Also simply elevator. Also grain elevator or bucket elevator. Also pastoralist or runholder. Also called the Third Agricultural Revolution. Also glasshouse. Also plough pan. Also haystack or simply stack or cock. Also haymow. Also hayrick or simply rick. Also turnrow. Also mustering. Also ridging or earthing up. Also centum weight or quintal. Also heterosis or outbreeding enhancement. Also technical crop. Also integrated farm management (IFM) or integrated production (IP). Also intensive farming. Also factory farming. Also interculture. Also jennet. Also timber. Also linchet. Also malt barn or maltings. Also trough or feeder. Also muck spreader or honey wagon. Also mechanised agriculture. Also millrun, lade, leat, flume, or penstock. Also continuous cropping. Also fixing. Also oast house or hop kiln. Also organic agriculture, biological farming, and ecological farming. Also natural fertilizer. Plural oxen; also bullock. Also simply paddy. Also livestock farming or grazing. Also pellet press. Also simply wilting point (WP). Also molecular farming, molecular pharming, and biopharming. Also hog. Also fish farming. Also plashing. Also plow. Also plow pan. Also plow planting. Also plowing. Also plowshare. Also poddy calf. Also polyhouse, hoophouse, grow tunnel, or high tunnel. Also fruticulture. Also satellite farming and site-specific crop management. Also pseudograin. Also simply quern. Also cattleman or stockgrower. Also simply range. Also rootcrop. Also rooting depth or rhizosphere. Also seedling bed. Also woolshed. Also abattoir. Also truck row. Also slurry tank, slurry lagoon, or slurry store. Also soil improvement or soil conditioner. Also microbial inoculant and bioinoculant. Often used interchangeably with seeding and planting. Also simply staple. Also cattle station, sheep station, or run. Also shock or stack. Also pigsty, pig pen, pig parlor, or pig-cote. Also subsurface irrigation or seepage irrigation. Also flat lifter. Also summer pasture. Also sun-dried. Also southwest injury. Also windrower. Also pig or hog. Also hay tedder. Also conditioning. Also thresher. Also topdressing. Also TTC assay or tetrazolium test. Also outplanting and replanting. Also treillage. Also winegrowing. Also two-wheel tractor or single-axle tractor. Also water mill. Also weaner. Also wheat mill run (WMR), millfeed (MF), and midds. Also shelterbelt. Also agricultural output.",
      "timestamp": "2025-10-09 19:12:37.999736"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_aerospace_engineering",
      "text": "This glossary of aerospace engineering terms pertains specifically to aerospace engineering, its sub-disciplines, and related fields including aviation and aeronautics. For a broad overview of engineering, see glossary of engineering.\n This stabilizes the ballute as it decelerates through different flow regimes (from supersonic to subsonic).\n",
      "timestamp": "2025-10-09 19:12:38.510984"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Glossary_of_aerospace_engineering",
      "title": "Glossary of aerospace engineering - Wikipedia",
      "description": "",
      "text": "This glossary of aerospace engineering terms pertains specifically to aerospace engineering, its sub-disciplines, and related fields including aviation and aeronautics. For a broad overview of engineering, see glossary of engineering.\n This stabilizes the ballute as it decelerates through different flow regimes (from supersonic to subsonic).\n",
      "timestamp": "2025-10-09 19:12:38.521676"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Engineering",
      "text": "\n \n Engineering is the practice of using natural science, mathematics, and the engineering design process[1] to solve problems within technology, increase efficiency and productivity, and improve systems. Modern engineering comprises many subfields which include designing and improving infrastructure, machinery, vehicles, electronics, materials, and energy systems.[2]\n The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis for applications of mathematics and science. See glossary of engineering.\n The word engineering is derived from the Latin ingenium.[3]\n The American Engineers' Council for Professional Development (the predecessor of the Accreditation Board for Engineering and Technology aka ABET)[4] has defined \"engineering\" as:\n The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property.[5][6] Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc.\n The term engineering is derived from the word engineer, which itself dates back to the 14th century when an engine'er (literally, one who builds or operates a siege engine) referred to \"a constructor of military engines\".[7] In this context, now obsolete, an \"engine\" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers.\n The word \"engine\" itself is of even older origin, ultimately deriving from the Latin ingenium (c.\u20091250), meaning \"innate quality, especially mental power, hence a clever invention.\"[8]\n Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering[6] entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.\n The pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuac\u00e1n, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.\n The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times.[9] The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC.[10] The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale,[11] and to move large objects in ancient Egyptian technology.[12] The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia c.\u20093000 BC,[11] and then in ancient Egyptian technology c.\u20092000 BC.[13] The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC,[14] and ancient Egypt during the Twelfth Dynasty (1991\u20131802 BC).[15] The screw, the last of the simple machines to be invented,[16] first appeared in Mesopotamia during the Neo-Assyrian period (911\u2013609) BC.[14] The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza.[17]\n The earliest civil engineer known by name is Imhotep.[6] As one of the officials of the Pharaoh, Djos\u00e8r, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630\u20132611 BC.[18] The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC.[19]\n Kush developed the Sakia during the 4th century BC, which relied on animal power instead of human energy.[20] Hafirs were developed as a type of reservoir in Kush to store and contain water as well as boost irrigation.[21] Sappers were employed to build causeways during military campaigns.[22] Kushite ancestors built speos during the Bronze Age between 3700 and 3250 BC.[23] Bloomeries and blast furnaces were also created during the 7th centuries BC in Kush.[24][25][26][27]\n Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, an early known mechanical analog computer,[28][29] and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are widely used in fields such as robotics and automotive engineering.[30]\n Ancient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC,[31] the trireme, the ballista and the catapult, the trebuchet by Chinese circa 6th-5th century BCE.[32]\n The earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD.[33][34][35][36] The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt.[37][38]\n The cotton gin was invented in India by the 6th century AD,[39] and the spinning wheel was invented in the Islamic world by the early 11th century,[40] both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century.[41]\n The earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their Book of Ingenious Devices, in the 9th century.[42][43] In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns.[44]\n Before the development of modern engineering, mathematics was used by artisans and craftsmen, such as millwrights, clockmakers, instrument makers and surveyors. Aside from these professions, universities were not believed to have had much practical significance to technology.[45]:\u200a32\u200a\n A standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise De re metallica (1556), which also contains sections on geology, mining, and chemistry. De re metallica was the standard chemistry reference for the next 180 years.[45]\n The science of classical mechanics, sometimes called Newtonian mechanics, formed the scientific basis of much of modern engineering.[45] With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.\n Canal building was an important engineering work during the early phases of the Industrial Revolution.[46]\n John Smeaton was the first self-proclaimed civil engineer and is often regarded as the \"father\" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbors, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Using a model water wheel, Smeaton conducted experiments for seven years, determining ways to increase efficiency.[47]:\u200a127\u200a   Smeaton introduced iron axles and gears to water wheels.[45]:\u200a69\u200a Smeaton also made mechanical improvements to the Newcomen steam engine. Smeaton designed the third Eddystone Lighthouse (1755\u201359) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain \"hydraulicity\" in lime; work which led ultimately to the invention of Portland cement.\n Applied science led to the development of the steam engine. The sequence of events began with the invention of the barometer and the measurement of atmospheric pressure by Evangelista Torricelli in 1643, demonstration of the force of atmospheric pressure by Otto von Guericke using the Magdeburg hemispheres in 1656, laboratory experiments by Denis Papin, who built experimental model steam engines and demonstrated the use of a piston, which he published in 1707. Edward Somerset, 2nd Marquess of Worcester published a book of 100 inventions containing a method for raising waters similar to a coffee percolator. Samuel Morland, a mathematician and inventor who worked on pumps, left notes at the Vauxhall Ordinance Office on a steam pump design that Thomas Savery read. In 1698 Savery built a steam pump called \"The Miner's Friend\". It employed both vacuum and pressure.[48] Iron merchant Thomas Newcomen, who built the first commercial piston steam engine in 1712, was not known to have any scientific training.[47]:\u200a32\u200a\n The application of steam-powered cast iron blowing cylinders for providing pressurized air for blast furnaces lead to a large increase in iron production in the late 18th century. The higher furnace temperatures made possible with steam-powered blast allowed for the use of more lime in blast furnaces, which enabled the transition from charcoal to coke.[49] These innovations lowered the cost of iron, making horse railways and iron bridges practical. The puddling process, patented by Henry Cort in 1784 produced large scale quantities of wrought iron. Hot blast, patented by James Beaumont Neilson in 1828, greatly lowered the amount of fuel needed to smelt iron. With the development of the high pressure steam engine, the power to weight ratio of steam engines made practical steamboats and locomotives possible.[50] New steel making processes, such as the Bessemer process and the open hearth furnace, ushered in an area of heavy engineering in the late 19th century.\n One of the most famous engineers of the mid-19th century was Isambard Kingdom Brunel, who built railroads, dockyards and steamships.\n The Industrial Revolution created a demand for machinery with metal parts, which led to the development of several machine tools. Boring cast iron cylinders with precision was not possible until John Wilkinson invented his boring machine, which is considered the first machine tool.[51] Other machine tools included the screw cutting lathe, milling machine, turret lathe and the metal planer. Precision machining techniques were developed in the first half of the 19th century. These included the use of gigs to guide the machining tool over the work and fixtures to hold the work in the proper position. Machine tools and machining techniques capable of producing interchangeable parts lead to large scale factory production by the late 19th century.[52]\n The United States Census of 1850 listed the occupation of \"engineer\" for the first time with a count of 2,000.[53] There were fewer than 50 engineering graduates in the U.S. before 1865. The first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.[54] In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.[50] There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.[55]\n The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.[6]\nChemical engineering developed in the late nineteenth century.[6] Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants.[6] The role of the chemical engineer was the design of these chemical plants and processes.[6] Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering.[56] Modern materials science evolved directly from metallurgy, which itself evolved from the use of fire. Important elements of modern materials science were products of the Space Race; the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials.\n Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.[57] Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.\n Engineering is a broad discipline that is often broken down into several sub-disciplines. Although most engineers will usually be trained in a specific discipline, some engineers become multi-disciplined through experience. Engineering is often characterized as having five main branches:[58][59][60] chemical engineering, civil engineering, electrical engineering, materials science and engineering, and mechanical engineering.\n Below is a list of recognized branches of engineering. There are additional sub-disciplines as well.\n Interdisciplinary engineering draws from more than one of the principle branches of the practice. Historically, naval engineering and mining engineering were major branches. Other engineering fields are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, automotive, information engineering, petroleum, systems, audio, software, architectural, biosystems, and textile engineering.[61] These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.\n New specialties sometimes combine with the traditional fields and form new branches \u2013 for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.\n One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer. There can also be what is called by the FAA a Designated Engineering Representative.[62]\n In the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. Engineers need proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their careers.\n If multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.\n Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.\n Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a particular problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.[63]\n More than one solution to a design problem usually exists so the different design choices have to be evaluated on their merits before the one judged most suitable is chosen. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of \"low-level\" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.[64]\n Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected but only in so far as the testing has been representative of use in service. For products, such as aircraft, that are used differently by different users failures and unexpected shortcomings (and necessary design changes) can be expected throughout the operational life of the product.[65]\n Engineers take on the responsibility of producing designs that will perform as well as expected and, except those employed in specific areas of the arms industry, will not harm people. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure.\n The study of failed products is known as forensic engineering. It attempts to identify the cause of failure to allow a redesign of the product and so prevent a re-occurrence. Careful analysis is needed to establish the cause of failure of a product. The consequences of a failure may vary in severity from the minor cost of a machine breakdown to large loss of life in the case of accidents involving aircraft and large stationary structures like buildings and dams.[66]\n As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.\n One of the most widely used design tools in the profession is computer-aided design (CAD) software. It enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.\n These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.[67]\n There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and Architecture, engineering and construction (AEC) software for civil engineering.\n In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).[68]\n The engineering profession engages in a range of activities, from collaboration at the societal level, and smaller individual projects. Almost all engineering projects are obligated to a funding source: a company, a set of investors, or a government. The types of engineering that are less constrained by such a funding source, are pro bono, and open-design engineering.\n Engineering has interconnections with society, culture and human behavior. Most products and constructions used by modern society, are influenced by engineering. Engineering activities have an impact on the environment, society, economies, and public safety.\n Engineering projects can be controversial. Examples from different engineering disciplines include: the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some engineering companies have enacted serious corporate and social responsibility policies.\n The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.[69]\n Overseas development and relief NGOs make considerable use of engineers, to apply solutions in disaster and development scenarios. Some charitable organizations use engineering directly for development:\n Engineering companies in more developed economies face challenges with regard to the number of engineers being trained, compared with those retiring. This problem is prominent in the UK where engineering has a poor image and low status.[71] There are negative economic and political issues that this can cause, as well as ethical issues.[72] It is agreed the engineering profession faces an \"image crisis\".[73] The UK holds the most engineering companies compared to other European countries, together with the United States.[74]\n Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states:\n  Engineering is an important and learned profession. As members of this profession, engineers are expected to exhibit the highest standards of honesty and integrity. Engineering has a direct and vital impact on the quality of life for all people. Accordingly, the services provided by engineers require honesty, impartiality, fairness, and equity, and must be dedicated to the protection of the public health, safety, and welfare. Engineers must perform under a standard of professional behavior that requires adherence to the highest principles of ethical conduct.[75] In Canada, engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.[76]\n Scientists study the world as it is; engineers create the world that has never been. \u2014\u200aTheodore von K\u00e1rm\u00e1n[77][78][79] There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.[citation needed]\n Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology, engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely \"engineering scientists\".[80]\n In the book What Engineers Know and How They Know It,[81] Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.\n There is a \"real and important\" difference between engineering and physics as similar to any science field has to do with technology.[82][83] Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology.[84][85][86] For technology, physics is an auxiliary and in a way technology is considered as applied physics.[87] Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training.[88] Physicists and engineers engage in different lines of work.[89] But PhD physicists who specialize in sectors of engineering physics and applied physics are titled as Technology officer, R&D Engineers and System Engineers.[90]\n An example of this is the use of numerical approximations to the Navier\u2013Stokes equations to describe aerodynamic flow over an aircraft, or the use of the finite element method to calculate the stresses in complex components. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.[91]\n As stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics:\n Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.[92] Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.[93]\n The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.\n Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers.[94][95] The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.\n Conversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.[96][97]\n Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.\n Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.[98]\n The heart for example functions much like a pump,[99] the skeleton is like a linked structure with levers,[100] the brain produces electrical signals etc.[101] These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.\n Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.[98]\n There are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).[103][104][105]\n The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design.[106] Robert Maillart's bridge design is perceived by some to have been deliberately artistic.[107] At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.[103][108]\n Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.[102][109]\n Business engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or \"Management engineering\" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical and electronics, power distribution and generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.\n In political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Marketing engineering and financial engineering have similarly borrowed the term.\n",
      "timestamp": "2025-10-09 19:12:38.968543"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Engineering",
      "title": "Engineering - Wikipedia",
      "description": "",
      "text": "\n \n Engineering is the practice of using natural science, mathematics, and the engineering design process[1] to solve problems within technology, increase efficiency and productivity, and improve systems. Modern engineering comprises many subfields which include designing and improving infrastructure, machinery, vehicles, electronics, materials, and energy systems.[2]\n The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis for applications of mathematics and science. See glossary of engineering.\n The word engineering is derived from the Latin ingenium.[3]\n The American Engineers' Council for Professional Development (the predecessor of the Accreditation Board for Engineering and Technology aka ABET)[4] has defined \"engineering\" as:\n The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property.[5][6] Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc.\n The term engineering is derived from the word engineer, which itself dates back to the 14th century when an engine'er (literally, one who builds or operates a siege engine) referred to \"a constructor of military engines\".[7] In this context, now obsolete, an \"engine\" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers.\n The word \"engine\" itself is of even older origin, ultimately deriving from the Latin ingenium (c.\u20091250), meaning \"innate quality, especially mental power, hence a clever invention.\"[8]\n Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering[6] entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.\n The pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuac\u00e1n, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.\n The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times.[9] The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC.[10] The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale,[11] and to move large objects in ancient Egyptian technology.[12] The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia c.\u20093000 BC,[11] and then in ancient Egyptian technology c.\u20092000 BC.[13] The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC,[14] and ancient Egypt during the Twelfth Dynasty (1991\u20131802 BC).[15] The screw, the last of the simple machines to be invented,[16] first appeared in Mesopotamia during the Neo-Assyrian period (911\u2013609) BC.[14] The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza.[17]\n The earliest civil engineer known by name is Imhotep.[6] As one of the officials of the Pharaoh, Djos\u00e8r, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630\u20132611 BC.[18] The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC.[19]\n Kush developed the Sakia during the 4th century BC, which relied on animal power instead of human energy.[20] Hafirs were developed as a type of reservoir in Kush to store and contain water as well as boost irrigation.[21] Sappers were employed to build causeways during military campaigns.[22] Kushite ancestors built speos during the Bronze Age between 3700 and 3250 BC.[23] Bloomeries and blast furnaces were also created during the 7th centuries BC in Kush.[24][25][26][27]\n Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, an early known mechanical analog computer,[28][29] and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are widely used in fields such as robotics and automotive engineering.[30]\n Ancient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC,[31] the trireme, the ballista and the catapult, the trebuchet by Chinese circa 6th-5th century BCE.[32]\n The earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD.[33][34][35][36] The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt.[37][38]\n The cotton gin was invented in India by the 6th century AD,[39] and the spinning wheel was invented in the Islamic world by the early 11th century,[40] both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century.[41]\n The earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their Book of Ingenious Devices, in the 9th century.[42][43] In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns.[44]\n Before the development of modern engineering, mathematics was used by artisans and craftsmen, such as millwrights, clockmakers, instrument makers and surveyors. Aside from these professions, universities were not believed to have had much practical significance to technology.[45]:\u200a32\u200a\n A standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise De re metallica (1556), which also contains sections on geology, mining, and chemistry. De re metallica was the standard chemistry reference for the next 180 years.[45]\n The science of classical mechanics, sometimes called Newtonian mechanics, formed the scientific basis of much of modern engineering.[45] With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.\n Canal building was an important engineering work during the early phases of the Industrial Revolution.[46]\n John Smeaton was the first self-proclaimed civil engineer and is often regarded as the \"father\" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbors, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Using a model water wheel, Smeaton conducted experiments for seven years, determining ways to increase efficiency.[47]:\u200a127\u200a   Smeaton introduced iron axles and gears to water wheels.[45]:\u200a69\u200a Smeaton also made mechanical improvements to the Newcomen steam engine. Smeaton designed the third Eddystone Lighthouse (1755\u201359) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain \"hydraulicity\" in lime; work which led ultimately to the invention of Portland cement.\n Applied science led to the development of the steam engine. The sequence of events began with the invention of the barometer and the measurement of atmospheric pressure by Evangelista Torricelli in 1643, demonstration of the force of atmospheric pressure by Otto von Guericke using the Magdeburg hemispheres in 1656, laboratory experiments by Denis Papin, who built experimental model steam engines and demonstrated the use of a piston, which he published in 1707. Edward Somerset, 2nd Marquess of Worcester published a book of 100 inventions containing a method for raising waters similar to a coffee percolator. Samuel Morland, a mathematician and inventor who worked on pumps, left notes at the Vauxhall Ordinance Office on a steam pump design that Thomas Savery read. In 1698 Savery built a steam pump called \"The Miner's Friend\". It employed both vacuum and pressure.[48] Iron merchant Thomas Newcomen, who built the first commercial piston steam engine in 1712, was not known to have any scientific training.[47]:\u200a32\u200a\n The application of steam-powered cast iron blowing cylinders for providing pressurized air for blast furnaces lead to a large increase in iron production in the late 18th century. The higher furnace temperatures made possible with steam-powered blast allowed for the use of more lime in blast furnaces, which enabled the transition from charcoal to coke.[49] These innovations lowered the cost of iron, making horse railways and iron bridges practical. The puddling process, patented by Henry Cort in 1784 produced large scale quantities of wrought iron. Hot blast, patented by James Beaumont Neilson in 1828, greatly lowered the amount of fuel needed to smelt iron. With the development of the high pressure steam engine, the power to weight ratio of steam engines made practical steamboats and locomotives possible.[50] New steel making processes, such as the Bessemer process and the open hearth furnace, ushered in an area of heavy engineering in the late 19th century.\n One of the most famous engineers of the mid-19th century was Isambard Kingdom Brunel, who built railroads, dockyards and steamships.\n The Industrial Revolution created a demand for machinery with metal parts, which led to the development of several machine tools. Boring cast iron cylinders with precision was not possible until John Wilkinson invented his boring machine, which is considered the first machine tool.[51] Other machine tools included the screw cutting lathe, milling machine, turret lathe and the metal planer. Precision machining techniques were developed in the first half of the 19th century. These included the use of gigs to guide the machining tool over the work and fixtures to hold the work in the proper position. Machine tools and machining techniques capable of producing interchangeable parts lead to large scale factory production by the late 19th century.[52]\n The United States Census of 1850 listed the occupation of \"engineer\" for the first time with a count of 2,000.[53] There were fewer than 50 engineering graduates in the U.S. before 1865. The first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.[54] In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.[50] There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.[55]\n The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.[6]\nChemical engineering developed in the late nineteenth century.[6] Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants.[6] The role of the chemical engineer was the design of these chemical plants and processes.[6] Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering.[56] Modern materials science evolved directly from metallurgy, which itself evolved from the use of fire. Important elements of modern materials science were products of the Space Race; the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials.\n Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.[57] Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.\n Engineering is a broad discipline that is often broken down into several sub-disciplines. Although most engineers will usually be trained in a specific discipline, some engineers become multi-disciplined through experience. Engineering is often characterized as having five main branches:[58][59][60] chemical engineering, civil engineering, electrical engineering, materials science and engineering, and mechanical engineering.\n Below is a list of recognized branches of engineering. There are additional sub-disciplines as well.\n Interdisciplinary engineering draws from more than one of the principle branches of the practice. Historically, naval engineering and mining engineering were major branches. Other engineering fields are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, automotive, information engineering, petroleum, systems, audio, software, architectural, biosystems, and textile engineering.[61] These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.\n New specialties sometimes combine with the traditional fields and form new branches \u2013 for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.\n One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer. There can also be what is called by the FAA a Designated Engineering Representative.[62]\n In the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. Engineers need proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their careers.\n If multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.\n Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.\n Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a particular problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.[63]\n More than one solution to a design problem usually exists so the different design choices have to be evaluated on their merits before the one judged most suitable is chosen. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of \"low-level\" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.[64]\n Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected but only in so far as the testing has been representative of use in service. For products, such as aircraft, that are used differently by different users failures and unexpected shortcomings (and necessary design changes) can be expected throughout the operational life of the product.[65]\n Engineers take on the responsibility of producing designs that will perform as well as expected and, except those employed in specific areas of the arms industry, will not harm people. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure.\n The study of failed products is known as forensic engineering. It attempts to identify the cause of failure to allow a redesign of the product and so prevent a re-occurrence. Careful analysis is needed to establish the cause of failure of a product. The consequences of a failure may vary in severity from the minor cost of a machine breakdown to large loss of life in the case of accidents involving aircraft and large stationary structures like buildings and dams.[66]\n As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.\n One of the most widely used design tools in the profession is computer-aided design (CAD) software. It enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.\n These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.[67]\n There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and Architecture, engineering and construction (AEC) software for civil engineering.\n In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).[68]\n The engineering profession engages in a range of activities, from collaboration at the societal level, and smaller individual projects. Almost all engineering projects are obligated to a funding source: a company, a set of investors, or a government. The types of engineering that are less constrained by such a funding source, are pro bono, and open-design engineering.\n Engineering has interconnections with society, culture and human behavior. Most products and constructions used by modern society, are influenced by engineering. Engineering activities have an impact on the environment, society, economies, and public safety.\n Engineering projects can be controversial. Examples from different engineering disciplines include: the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some engineering companies have enacted serious corporate and social responsibility policies.\n The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.[69]\n Overseas development and relief NGOs make considerable use of engineers, to apply solutions in disaster and development scenarios. Some charitable organizations use engineering directly for development:\n Engineering companies in more developed economies face challenges with regard to the number of engineers being trained, compared with those retiring. This problem is prominent in the UK where engineering has a poor image and low status.[71] There are negative economic and political issues that this can cause, as well as ethical issues.[72] It is agreed the engineering profession faces an \"image crisis\".[73] The UK holds the most engineering companies compared to other European countries, together with the United States.[74]\n Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states:\n  Engineering is an important and learned profession. As members of this profession, engineers are expected to exhibit the highest standards of honesty and integrity. Engineering has a direct and vital impact on the quality of life for all people. Accordingly, the services provided by engineers require honesty, impartiality, fairness, and equity, and must be dedicated to the protection of the public health, safety, and welfare. Engineers must perform under a standard of professional behavior that requires adherence to the highest principles of ethical conduct.[75] In Canada, engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.[76]\n Scientists study the world as it is; engineers create the world that has never been. \u2014\u200aTheodore von K\u00e1rm\u00e1n[77][78][79] There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.[citation needed]\n Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology, engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely \"engineering scientists\".[80]\n In the book What Engineers Know and How They Know It,[81] Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.\n There is a \"real and important\" difference between engineering and physics as similar to any science field has to do with technology.[82][83] Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology.[84][85][86] For technology, physics is an auxiliary and in a way technology is considered as applied physics.[87] Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training.[88] Physicists and engineers engage in different lines of work.[89] But PhD physicists who specialize in sectors of engineering physics and applied physics are titled as Technology officer, R&D Engineers and System Engineers.[90]\n An example of this is the use of numerical approximations to the Navier\u2013Stokes equations to describe aerodynamic flow over an aircraft, or the use of the finite element method to calculate the stresses in complex components. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.[91]\n As stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics:\n Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.[92] Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.[93]\n The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.\n Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers.[94][95] The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.\n Conversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.[96][97]\n Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.\n Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.[98]\n The heart for example functions much like a pump,[99] the skeleton is like a linked structure with levers,[100] the brain produces electrical signals etc.[101] These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.\n Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.[98]\n There are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).[103][104][105]\n The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design.[106] Robert Maillart's bridge design is perceived by some to have been deliberately artistic.[107] At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.[103][108]\n Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.[102][109]\n Business engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or \"Management engineering\" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical and electronics, power distribution and generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.\n In political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Marketing engineering and financial engineering have similarly borrowed the term.\n",
      "timestamp": "2025-10-09 19:12:38.976845"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Science",
      "text": "\n \n \nScience is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe.[1][2] Modern science is typically divided into two\u00a0\u2013 or three\u00a0\u2013 major branches:[3] the natural sciences, which study the physical world, and the social sciences, which study individuals and societies.[4][5] While referred to as the formal sciences, the study of logic, mathematics, and theoretical computer science are typically regarded as separate because they rely on deductive reasoning instead of the scientific method as their main methodology.[6][7][8][9] Meanwhile, applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine.[10][11][12]\n The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c.\u20093000\u20131200\u00a0BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity and later medieval scholarship, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes; while further advancements, including the introduction of the Hindu\u2013Arabic numeral system, were made during the Golden Age of India and Islamic Golden Age.[13]:\u200a12\u200a[14][15][16][13]:\u200a163\u2013192\u200a The recovery and assimilation of Greek works and Islamic inquiries into Western Europe during the Renaissance revived natural philosophy,[13]:\u200a193\u2013224,\u200a225\u2013253\u200a[17] which was later transformed by the Scientific Revolution that began in the 16th century[18] as new ideas and discoveries departed from previous Greek conceptions and traditions.[13]:\u200a357\u2013368\u200a[19] The scientific method soon played a greater role in the acquisition of knowledge, and in the 19th century, many of the institutional and professional features of science began to take shape,[20][21] along with the changing of \"natural philosophy\" to \"natural science\".[22]\n New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[23][24] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[25] government agencies,[13]:\u200a163\u2013192\u200a and companies.[26] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection.\n The word science has been used in Middle English since the 14th century in the sense of \"the state of knowing\". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning \"knowledge, awareness, understanding\", a noun derivative of sciens meaning \"knowing\", itself the present active participle of sci\u014d, \"to know\".[27]\n There are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sci\u014d may have its origin in the Proto-Italic language as *skije- or *skijo- meaning \"to know\", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io meaning \"to incise\". The Lexikon der indogermanischen Verben proposed sci\u014d is a back-formation of nesc\u012bre, meaning \"to not know, be unfamiliar with\", which may derive from Proto-Indo-European *sekH- in Latin sec\u0101re, or *skh2- from *s\u1e31\u02b0eh2(i)-meaning \"to cut\".[28]\n In the past, science was a synonym for \"knowledge\" or \"study\", in keeping with its Latin origin. A person who conducted scientific research was called a \"natural philosopher\" or \"man of science\".[29] In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences,[30] crediting it to \"some ingenious gentleman\" (possibly himself).[31]\n Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years,[32][33] taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science,[34] as did religious rituals.[35] Some scholars use the term \"protoscience\" to label activities in the past that resemble modern science in some but not all features;[36][37][38] however, this label has also been criticised as denigrating,[39] or too suggestive of presentism, thinking about those activities only in relation to modern categories.[40]\n Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia (c.\u20093000\u20131200\u00a0BCE), creating the earliest written records in the history of science.[13]:\u200a12\u201315\u200a[14] Although the words and concepts of \"science\" and \"nature\" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine.[41][13]:\u200a12\u200a From the 3rd millennium\u00a0BCE, the ancient Egyptians developed a non-positional decimal numbering system,[42] solved practical problems using geometry,[43] and developed a calendar.[44] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals.[13]:\u200a9\u200a\n The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing.[45] They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes.[46] The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur.[45][47] They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity.[45]\n In classical antiquity, there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time.[48] Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural \"way\" in which a plant grows,[49] and the \"way\" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish \"nature\" and \"convention\".[50]\n The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural.[51] The Pythagoreans developed a complex number philosophy[52]:\u200a467\u2013468\u200a and contributed significantly to the development of mathematical science.[52]:\u200a465\u200a The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus.[53][54] Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a \"canon\" (ruler, standard) which established physical criteria or standards of scientific truth.Cite error: A <ref> tag is missing the closing </ref> (see the help page).[55] and is known as \"The Father of Medicine\".[56]\n A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly held truths that shape beliefs and scrutinises them for consistency.[57] Socrates criticised the older type of study of physics as too purely speculative and lacking in self-criticism.[58]\n In the 4th century\u00a0BCE, Aristotle created a systematic programme of teleological philosophy.[59] In the 3rd century\u00a0BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the centre and all the planets orbiting it.[60] Aristarchus's model was widely rejected because it was believed to violate the laws of physics,[60] while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead.[61][62] The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus.[63] Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopaedia Natural History.[64][65][66]\n Positional notation for representing numbers likely emerged between the 3rd and 5th centuries\u00a0CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide.[67]\n Due to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe.[13]:\u200a194\u200a Latin encyclopaedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge.[68] In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning.[13]:\u200a159\u200a John Philoponus, a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus.[13]:\u200a307,\u200a311,\u200a363,\u200a402\u200a His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later.[13]:\u200a307\u2013308\u200a[69]\n During late antiquity and the Early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause.[70] Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were made by Christians, mainly Nestorians and Miaphysites. Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists.[71] By the 6th and 7th centuries, the neighbouring Sasanian Empire established the medical Academy of Gondishapur, which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world.[72]\n Islamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad, Iraq[73] and the flourished[74] until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study.[a][76][77] Avicenna's compilation of The Canon of Medicine, a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century.[78]\n By the 11th century most of Europe had become Christian,[13]:\u200a204\u200a and in 1088, the University of Bologna emerged as the first university in Europe.[79] As such, demand for Latin translation of ancient and scientific texts grew,[13]:\u200a204\u200a a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature.[80] In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi.[81]\n New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle.[75]:\u200aBook I\u200a A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.[82]\n In the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.[83]\n Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light.[82][84] Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres.[85] Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model.[86]\n The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature.[87] Francis Bacon and Ren\u00e9 Descartes published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life.[88] Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature.[89]\n At the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophi\u00e6 Naturalis Principia Mathematica greatly influencing future physicists.[90] Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes.[91]\n During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, \"the real and legitimate goal of sciences is the endowment of human life with new inventions and riches\", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond \"the fume of subtle, sublime or pleasing [speculation]\".[92]\n Science during the Enlightenment was dominated by scientific societies and academies,[93] which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularisation of science among an increasingly literate population.[94] Enlightenment philosophers turned to a few of their scientific predecessors\u00a0\u2013 Galileo, Kepler, Boyle, and Newton principally\u00a0\u2013 as the guides to every physical and social field of the day.[95][96]\n The 18th century saw significant advancements in the practice of medicine[97] and physics;[98] the development of biological taxonomy by Carl Linnaeus;[99] a new understanding of magnetism and electricity;[100] and the maturation of chemistry as a discipline.[101] Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity.[102] Modern sociology largely originated from this movement.[103] In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics.[104]\n During the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as \"biologist\", \"physicist\", and \"scientist\"; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals.[105] During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879.[106]\n During the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species, published in 1859.[107] Separately, Gregor Mendel presented his paper, \"Experiments on Plant Hybridisation\" in 1865,[108] which outlined the principles of biological inheritance, serving as the basis for modern genetics.[109]\n Early in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms.[110] The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy.[111] This realisation led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.[b]\n The electromagnetic theory was established in the 19th century by the works of Hans Christian \u00d8rsted, Andr\u00e9-Marie Amp\u00e8re, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896,[114] Marie Curie then became the first person to win two Nobel Prizes.[115] In the next year came the discovery of the first subatomic particle, the electron.[116]\n In the first half of the century the development of antibiotics and artificial fertilisers improved human living standards globally.[117][118] Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication, and climate change came to the public's attention and caused the onset of environmental studies.[119]\n During this period scientific experimentation became increasingly larger in scale and funding.[120] The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race and nuclear arms race.[121][122] Substantial international collaborations were also made, despite armed conflicts.[123]\n In the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields.[124] The discovery of the cosmic microwave background in 1964[125] led to a rejection of the steady-state model of the universe in favour of the Big Bang theory of Georges Lema\u00eetre.[126]\n The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th century when the modern synthesis reconciled Darwinian evolution with classical genetics.[127] Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity.[128][129] Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling.[130]\n The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome.[131] The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body.[132] With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found.[133] In 2015, gravitational waves, predicted by general relativity a century before, were first observed.[134][135] In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disc.[136]\n Modern science is commonly divided into three major branches: natural science, social science, and formal science.[3] Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise.[137] Both natural and social sciences are empirical sciences,[138] as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions.[139]\n Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[140] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings.[141] Today, \"natural history\" suggests observational descriptions aimed at popular audiences.[142]\n Social science is the study of human behaviour and the functioning of societies.[4][5] It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology.[4] In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programmes such as the functionalists, conflict theorists, and interactionists in sociology.[4] Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes.[4]\n Formal science is an area of study that generates knowledge using formal systems.[143][144][145] A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules.[146] It includes mathematics,[147][148] systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts.[8][149][139] The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science.[6][150] Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics.[151] Natural and social sciences that rely heavily on mathematical applications include mathematical physics,[152] chemistry,[153] biology,[154] finance,[155] and economics.[156]\n Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine.[157][12] Engineering is the use of scientific principles to invent, design and build machines, structures and technologies.[158] Science may contribute to the development of new technologies.[159] Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease.[160][161]\n The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world.[162][163]\n Computational science applies computer simulations to science, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of machine learning and artificial intelligence is becoming a central feature of computational contributions to science, for example in agent-based computational economics, random forests, topic modeling and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans.[166][167]\n Interdisciplinary science involves the combination of two or more disciplines into one,[168] such as bioinformatics, a combination of biology and computer science[169] or cognitive sciences. The concept has existed since the ancient Greek period and it became popular again in the 20th century.[170]\n Scientific research can be labelled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable.[171]\n Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way.[172] Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an objective reality shared by all rational observers; this objective reality is governed by natural laws; these laws were discovered by means of systematic observation and experimentation.[2] Mathematics is essential in the formation of hypotheses, theories, and laws, because it is used extensively in quantitative modelling, observing, and collecting measurements.[173] Statistics is used to summarise and analyse data, which allows scientists to assess the reliability of experimental results.[174]\n In the scientific method an explanatory thought experiment or hypothesis is put forward as an explanation using parsimony principles and is expected to seek consilience\u00a0\u2013 fitting with other accepted facts related to an observation or scientific question.[175] This tentative explanation is used to make falsifiable predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress.[172]:\u200a4\u20135\u200a[176] Experimentation is especially important in science to help establish causal relationships to avoid the correlation fallacy, though in some sciences such as astronomy or geology, a predicted observation might be more appropriate.[177]\n When a hypothesis proves unsatisfactory it is modified or discarded. If the hypothesis survives testing, it may become adopted into the framework of a scientific theory, a validly reasoned, self-consistent model or framework for describing the behaviour of certain natural events. A theory typically describes the behaviour of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus, a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a model, an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation, and to generate new hypotheses that can be tested by experimentation.[178]\n While performing experiments to test hypotheses, scientists may have a preference for one outcome over another.[179][180] Eliminating the bias can be achieved through transparency, careful experimental design, and a thorough peer review process of the experimental results and conclusions.[181][182] After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be.[183] Taken in its entirety, the scientific method allows for highly creative problem solving while minimising the effects of subjective and confirmation bias.[184] Intersubjective verifiability, the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge.[185]\n Scientific research is published in a range of literature.[186] Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, Journal des s\u00e7avans followed by Philosophical Transactions, began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500.[187]\n Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population.[188]\n The replication crisis is an ongoing methodological crisis that affects parts of the social and life sciences. In subsequent investigations, the results of many scientific studies have been proven to be unrepeatable.[189] The crisis has long-standing roots; the phrase was coined in the early 2010s[190] as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste.[191]\n An area of study or speculation that masquerades as science in an attempt to claim legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science.[192][193] Physicist Richard Feynman coined the term \"cargo cult science\" for cases in which researchers believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated.[194] Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as \"the most important tool\" for separating valid claims from invalid ones.[195]\n There can also be an element of political bias or ideological bias on all sides of scientific debates. Sometimes, research may be characterised as \"bad science\", research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term scientific misconduct refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person.[196]\n  There are different schools of thought in the philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation; scientific theories generalise observations.[197] Empiricism generally encompasses inductivism, a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method.[198][197]\n Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation.[199] Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation.[200]\nPopper proposed replacing verifiability with falsifiability as the landmark of scientific theories, replacing induction with falsification as the empirical method.[200] Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error,[201] covering all products of the human mind, including science, mathematics, philosophy, and art.[202]\n Another approach, instrumentalism, emphasises the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes, with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be things that should be ignored.[203] Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true.[204]\n Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent \"portrait\" of the world that is consistent with observations made from its framing. He characterised normal science as the process of observation and \"puzzle solving\", which takes place within a paradigm, whereas revolutionary science occurs when one paradigm overtakes another in a paradigm shift.[205] Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more \"portraits\" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism.[206]\n Another approach often cited in debates of scientific scepticism against controversial movements like \"creation science\" is methodological naturalism. Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations.[207] Methodological naturalism maintains that science requires strict adherence to empirical study and independent verification.[208]\n The scientific community is a network of interacting scientists who conduct scientific research. The community consists of smaller groups working in scientific fields. By having peer review, through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results.[209]\n Scientists are individuals who conduct scientific research to advance knowledge in an area of interest.[210][211] Scientists may exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of public health, nations, the environment, or industries; other motivations include recognition by peers and prestige.[citation needed] In modern times, many scientists study within specific areas of science in academic institutions, often obtaining advanced degrees in the process.[212] Many scientists pursue careers in various fields such as academia, industry, government, and nonprofit organisations.[213][214][215]\n Science has historically been a male-dominated field, with notable exceptions. Women have faced considerable discrimination in science, much as they have in other areas of male-dominated societies. For example, women were frequently passed over for job opportunities and denied credit for their work.[216] The achievements of women in science have been attributed to the defiance of their traditional role as labourers within the domestic sphere.[217]\n Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance.[218] Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines.[219] Membership may either be open to all, require possession of scientific credentials, or conferred by election.[220] Most scientific societies are nonprofit organisations,[221] and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some societies act as professional bodies, regulating the activities of their members in the public interest, or the collective interest of the membership.\n The professionalisation of science, begun in the 19th century, was partly enabled by the creation of national distinguished academies of sciences such as the Italian Accademia dei Lincei in 1603,[222] the British Royal Society in 1660,[223] the French Academy of Sciences in 1666,[224] the American National Academy of Sciences in 1863,[225] the German Kaiser Wilhelm Society in 1911,[226] and the Chinese Academy of Sciences in 1949.[227] International scientific organisations, such as the International Science Council, are devoted to international cooperation for science advancement.[228]\n Science awards are usually given to individuals or organisations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist receiving them. Since the early Renaissance, scientists have often been awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and chemistry.[229]\n Funding of science is often through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP.[230] In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10%, respectively, by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and the humanities. In less developed nations, the government provides the bulk of the funds for their basic scientific research.[231]\n Many governments have dedicated agencies to support scientific research, such as the National Science Foundation in the United States,[232] the National Scientific and Technical Research Council in Argentina,[233] Commonwealth Scientific and Industrial Research Organisation in Australia,[234] National Centre for Scientific Research in France,[235] the Max Planck Society in Germany,[236] and National Research Council in Spain.[237] In commercial research and development, all but the most research-orientated corporations focus more heavily on near-term commercialisation possibilities than research driven by curiosity.[238]\n Science policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public.[239] Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organisations that fund research.[188]\n Science education for the general public is embedded in the school curriculum, and is supplemented by online pedagogical content (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Major organisations of scientists such as the American Association for the Advancement of Science (AAAS) consider the sciences to be a part of the liberal arts traditions of learning, along with philosophy and history.[240] Scientific literacy is chiefly concerned with an understanding of the scientific method, units and methods of measurement, empiricism, a basic understanding of statistics (correlations, qualitative versus quantitative observations, aggregate statistics), and a basic understanding of core scientific fields such as physics, chemistry, biology, ecology, geology, and computation. As a student advances into higher stages of formal education, the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well.[241]\n The mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter.[242] Few journalists have real scientific knowledge, and even beat reporters who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover.[243][244]\n Science magazines such as New Scientist, Science & Vie, and Scientific American cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research.[245] The science fiction genre, primarily speculative fiction, can transmit the ideas and methods of science to the general public.[246] Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the Creative Writing Science resource developed through the Royal Literary Fund.[247]\n While the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are sceptical about science. Examples are the common notion that COVID-19 is not a major health threat to the US (held by 39% of Americans in August 2021)[248] or the belief that climate change is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020).[249] Psychologists have pointed to four factors driving rejection of scientific results:[250]\n Anti-science attitudes often seem to be caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left.[252] That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardise their social status.[253]\n Attitudes towards science are often determined by political opinions and goals. Government, business and advocacy groups have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the politicisation of science such as anti-intellectualism, perceived threats to religious beliefs, and fear for business interests.[255] Politicisation of science is usually accomplished when scientific information is presented in a way that emphasises the uncertainty associated with the scientific evidence.[256] Tactics such as shifting conversation, failing to acknowledge facts, and capitalising on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence.[257] Examples of issues that have involved the politicisation of science include the global warming controversy, health effects of pesticides, and health effects of tobacco.[257][258]\n",
      "timestamp": "2025-10-09 19:12:39.706919"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Science",
      "title": "Science - Wikipedia",
      "description": "",
      "text": "\n \n \nScience is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe.[1][2] Modern science is typically divided into two\u00a0\u2013 or three\u00a0\u2013 major branches:[3] the natural sciences, which study the physical world, and the social sciences, which study individuals and societies.[4][5] While referred to as the formal sciences, the study of logic, mathematics, and theoretical computer science are typically regarded as separate because they rely on deductive reasoning instead of the scientific method as their main methodology.[6][7][8][9] Meanwhile, applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine.[10][11][12]\n The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c.\u20093000\u20131200\u00a0BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity and later medieval scholarship, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes; while further advancements, including the introduction of the Hindu\u2013Arabic numeral system, were made during the Golden Age of India and Islamic Golden Age.[13]:\u200a12\u200a[14][15][16][13]:\u200a163\u2013192\u200a The recovery and assimilation of Greek works and Islamic inquiries into Western Europe during the Renaissance revived natural philosophy,[13]:\u200a193\u2013224,\u200a225\u2013253\u200a[17] which was later transformed by the Scientific Revolution that began in the 16th century[18] as new ideas and discoveries departed from previous Greek conceptions and traditions.[13]:\u200a357\u2013368\u200a[19] The scientific method soon played a greater role in the acquisition of knowledge, and in the 19th century, many of the institutional and professional features of science began to take shape,[20][21] along with the changing of \"natural philosophy\" to \"natural science\".[22]\n New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[23][24] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[25] government agencies,[13]:\u200a163\u2013192\u200a and companies.[26] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection.\n The word science has been used in Middle English since the 14th century in the sense of \"the state of knowing\". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning \"knowledge, awareness, understanding\", a noun derivative of sciens meaning \"knowing\", itself the present active participle of sci\u014d, \"to know\".[27]\n There are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sci\u014d may have its origin in the Proto-Italic language as *skije- or *skijo- meaning \"to know\", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io meaning \"to incise\". The Lexikon der indogermanischen Verben proposed sci\u014d is a back-formation of nesc\u012bre, meaning \"to not know, be unfamiliar with\", which may derive from Proto-Indo-European *sekH- in Latin sec\u0101re, or *skh2- from *s\u1e31\u02b0eh2(i)-meaning \"to cut\".[28]\n In the past, science was a synonym for \"knowledge\" or \"study\", in keeping with its Latin origin. A person who conducted scientific research was called a \"natural philosopher\" or \"man of science\".[29] In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences,[30] crediting it to \"some ingenious gentleman\" (possibly himself).[31]\n Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years,[32][33] taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science,[34] as did religious rituals.[35] Some scholars use the term \"protoscience\" to label activities in the past that resemble modern science in some but not all features;[36][37][38] however, this label has also been criticised as denigrating,[39] or too suggestive of presentism, thinking about those activities only in relation to modern categories.[40]\n Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia (c.\u20093000\u20131200\u00a0BCE), creating the earliest written records in the history of science.[13]:\u200a12\u201315\u200a[14] Although the words and concepts of \"science\" and \"nature\" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine.[41][13]:\u200a12\u200a From the 3rd millennium\u00a0BCE, the ancient Egyptians developed a non-positional decimal numbering system,[42] solved practical problems using geometry,[43] and developed a calendar.[44] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals.[13]:\u200a9\u200a\n The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing.[45] They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes.[46] The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur.[45][47] They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity.[45]\n In classical antiquity, there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time.[48] Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural \"way\" in which a plant grows,[49] and the \"way\" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish \"nature\" and \"convention\".[50]\n The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural.[51] The Pythagoreans developed a complex number philosophy[52]:\u200a467\u2013468\u200a and contributed significantly to the development of mathematical science.[52]:\u200a465\u200a The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus.[53][54] Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a \"canon\" (ruler, standard) which established physical criteria or standards of scientific truth.Cite error: A <ref> tag is missing the closing </ref> (see the help page).[55] and is known as \"The Father of Medicine\".[56]\n A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly held truths that shape beliefs and scrutinises them for consistency.[57] Socrates criticised the older type of study of physics as too purely speculative and lacking in self-criticism.[58]\n In the 4th century\u00a0BCE, Aristotle created a systematic programme of teleological philosophy.[59] In the 3rd century\u00a0BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the centre and all the planets orbiting it.[60] Aristarchus's model was widely rejected because it was believed to violate the laws of physics,[60] while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead.[61][62] The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus.[63] Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopaedia Natural History.[64][65][66]\n Positional notation for representing numbers likely emerged between the 3rd and 5th centuries\u00a0CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide.[67]\n Due to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe.[13]:\u200a194\u200a Latin encyclopaedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge.[68] In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning.[13]:\u200a159\u200a John Philoponus, a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus.[13]:\u200a307,\u200a311,\u200a363,\u200a402\u200a His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later.[13]:\u200a307\u2013308\u200a[69]\n During late antiquity and the Early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause.[70] Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were made by Christians, mainly Nestorians and Miaphysites. Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists.[71] By the 6th and 7th centuries, the neighbouring Sasanian Empire established the medical Academy of Gondishapur, which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world.[72]\n Islamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad, Iraq[73] and the flourished[74] until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study.[a][76][77] Avicenna's compilation of The Canon of Medicine, a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century.[78]\n By the 11th century most of Europe had become Christian,[13]:\u200a204\u200a and in 1088, the University of Bologna emerged as the first university in Europe.[79] As such, demand for Latin translation of ancient and scientific texts grew,[13]:\u200a204\u200a a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature.[80] In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi.[81]\n New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle.[75]:\u200aBook I\u200a A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.[82]\n In the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.[83]\n Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light.[82][84] Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres.[85] Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model.[86]\n The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature.[87] Francis Bacon and Ren\u00e9 Descartes published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life.[88] Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature.[89]\n At the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophi\u00e6 Naturalis Principia Mathematica greatly influencing future physicists.[90] Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes.[91]\n During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, \"the real and legitimate goal of sciences is the endowment of human life with new inventions and riches\", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond \"the fume of subtle, sublime or pleasing [speculation]\".[92]\n Science during the Enlightenment was dominated by scientific societies and academies,[93] which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularisation of science among an increasingly literate population.[94] Enlightenment philosophers turned to a few of their scientific predecessors\u00a0\u2013 Galileo, Kepler, Boyle, and Newton principally\u00a0\u2013 as the guides to every physical and social field of the day.[95][96]\n The 18th century saw significant advancements in the practice of medicine[97] and physics;[98] the development of biological taxonomy by Carl Linnaeus;[99] a new understanding of magnetism and electricity;[100] and the maturation of chemistry as a discipline.[101] Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity.[102] Modern sociology largely originated from this movement.[103] In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics.[104]\n During the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as \"biologist\", \"physicist\", and \"scientist\"; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals.[105] During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879.[106]\n During the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species, published in 1859.[107] Separately, Gregor Mendel presented his paper, \"Experiments on Plant Hybridisation\" in 1865,[108] which outlined the principles of biological inheritance, serving as the basis for modern genetics.[109]\n Early in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms.[110] The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy.[111] This realisation led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.[b]\n The electromagnetic theory was established in the 19th century by the works of Hans Christian \u00d8rsted, Andr\u00e9-Marie Amp\u00e8re, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896,[114] Marie Curie then became the first person to win two Nobel Prizes.[115] In the next year came the discovery of the first subatomic particle, the electron.[116]\n In the first half of the century the development of antibiotics and artificial fertilisers improved human living standards globally.[117][118] Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication, and climate change came to the public's attention and caused the onset of environmental studies.[119]\n During this period scientific experimentation became increasingly larger in scale and funding.[120] The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race and nuclear arms race.[121][122] Substantial international collaborations were also made, despite armed conflicts.[123]\n In the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields.[124] The discovery of the cosmic microwave background in 1964[125] led to a rejection of the steady-state model of the universe in favour of the Big Bang theory of Georges Lema\u00eetre.[126]\n The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th century when the modern synthesis reconciled Darwinian evolution with classical genetics.[127] Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity.[128][129] Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling.[130]\n The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome.[131] The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body.[132] With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found.[133] In 2015, gravitational waves, predicted by general relativity a century before, were first observed.[134][135] In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disc.[136]\n Modern science is commonly divided into three major branches: natural science, social science, and formal science.[3] Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise.[137] Both natural and social sciences are empirical sciences,[138] as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions.[139]\n Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[140] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings.[141] Today, \"natural history\" suggests observational descriptions aimed at popular audiences.[142]\n Social science is the study of human behaviour and the functioning of societies.[4][5] It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology.[4] In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programmes such as the functionalists, conflict theorists, and interactionists in sociology.[4] Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes.[4]\n Formal science is an area of study that generates knowledge using formal systems.[143][144][145] A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules.[146] It includes mathematics,[147][148] systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts.[8][149][139] The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science.[6][150] Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics.[151] Natural and social sciences that rely heavily on mathematical applications include mathematical physics,[152] chemistry,[153] biology,[154] finance,[155] and economics.[156]\n Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine.[157][12] Engineering is the use of scientific principles to invent, design and build machines, structures and technologies.[158] Science may contribute to the development of new technologies.[159] Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease.[160][161]\n The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world.[162][163]\n Computational science applies computer simulations to science, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of machine learning and artificial intelligence is becoming a central feature of computational contributions to science, for example in agent-based computational economics, random forests, topic modeling and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans.[166][167]\n Interdisciplinary science involves the combination of two or more disciplines into one,[168] such as bioinformatics, a combination of biology and computer science[169] or cognitive sciences. The concept has existed since the ancient Greek period and it became popular again in the 20th century.[170]\n Scientific research can be labelled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable.[171]\n Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way.[172] Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an objective reality shared by all rational observers; this objective reality is governed by natural laws; these laws were discovered by means of systematic observation and experimentation.[2] Mathematics is essential in the formation of hypotheses, theories, and laws, because it is used extensively in quantitative modelling, observing, and collecting measurements.[173] Statistics is used to summarise and analyse data, which allows scientists to assess the reliability of experimental results.[174]\n In the scientific method an explanatory thought experiment or hypothesis is put forward as an explanation using parsimony principles and is expected to seek consilience\u00a0\u2013 fitting with other accepted facts related to an observation or scientific question.[175] This tentative explanation is used to make falsifiable predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress.[172]:\u200a4\u20135\u200a[176] Experimentation is especially important in science to help establish causal relationships to avoid the correlation fallacy, though in some sciences such as astronomy or geology, a predicted observation might be more appropriate.[177]\n When a hypothesis proves unsatisfactory it is modified or discarded. If the hypothesis survives testing, it may become adopted into the framework of a scientific theory, a validly reasoned, self-consistent model or framework for describing the behaviour of certain natural events. A theory typically describes the behaviour of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus, a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a model, an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation, and to generate new hypotheses that can be tested by experimentation.[178]\n While performing experiments to test hypotheses, scientists may have a preference for one outcome over another.[179][180] Eliminating the bias can be achieved through transparency, careful experimental design, and a thorough peer review process of the experimental results and conclusions.[181][182] After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be.[183] Taken in its entirety, the scientific method allows for highly creative problem solving while minimising the effects of subjective and confirmation bias.[184] Intersubjective verifiability, the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge.[185]\n Scientific research is published in a range of literature.[186] Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, Journal des s\u00e7avans followed by Philosophical Transactions, began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500.[187]\n Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population.[188]\n The replication crisis is an ongoing methodological crisis that affects parts of the social and life sciences. In subsequent investigations, the results of many scientific studies have been proven to be unrepeatable.[189] The crisis has long-standing roots; the phrase was coined in the early 2010s[190] as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste.[191]\n An area of study or speculation that masquerades as science in an attempt to claim legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science.[192][193] Physicist Richard Feynman coined the term \"cargo cult science\" for cases in which researchers believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated.[194] Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as \"the most important tool\" for separating valid claims from invalid ones.[195]\n There can also be an element of political bias or ideological bias on all sides of scientific debates. Sometimes, research may be characterised as \"bad science\", research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term scientific misconduct refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person.[196]\n  There are different schools of thought in the philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation; scientific theories generalise observations.[197] Empiricism generally encompasses inductivism, a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method.[198][197]\n Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation.[199] Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation.[200]\nPopper proposed replacing verifiability with falsifiability as the landmark of scientific theories, replacing induction with falsification as the empirical method.[200] Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error,[201] covering all products of the human mind, including science, mathematics, philosophy, and art.[202]\n Another approach, instrumentalism, emphasises the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes, with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be things that should be ignored.[203] Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true.[204]\n Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent \"portrait\" of the world that is consistent with observations made from its framing. He characterised normal science as the process of observation and \"puzzle solving\", which takes place within a paradigm, whereas revolutionary science occurs when one paradigm overtakes another in a paradigm shift.[205] Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more \"portraits\" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism.[206]\n Another approach often cited in debates of scientific scepticism against controversial movements like \"creation science\" is methodological naturalism. Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations.[207] Methodological naturalism maintains that science requires strict adherence to empirical study and independent verification.[208]\n The scientific community is a network of interacting scientists who conduct scientific research. The community consists of smaller groups working in scientific fields. By having peer review, through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results.[209]\n Scientists are individuals who conduct scientific research to advance knowledge in an area of interest.[210][211] Scientists may exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of public health, nations, the environment, or industries; other motivations include recognition by peers and prestige.[citation needed] In modern times, many scientists study within specific areas of science in academic institutions, often obtaining advanced degrees in the process.[212] Many scientists pursue careers in various fields such as academia, industry, government, and nonprofit organisations.[213][214][215]\n Science has historically been a male-dominated field, with notable exceptions. Women have faced considerable discrimination in science, much as they have in other areas of male-dominated societies. For example, women were frequently passed over for job opportunities and denied credit for their work.[216] The achievements of women in science have been attributed to the defiance of their traditional role as labourers within the domestic sphere.[217]\n Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance.[218] Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines.[219] Membership may either be open to all, require possession of scientific credentials, or conferred by election.[220] Most scientific societies are nonprofit organisations,[221] and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some societies act as professional bodies, regulating the activities of their members in the public interest, or the collective interest of the membership.\n The professionalisation of science, begun in the 19th century, was partly enabled by the creation of national distinguished academies of sciences such as the Italian Accademia dei Lincei in 1603,[222] the British Royal Society in 1660,[223] the French Academy of Sciences in 1666,[224] the American National Academy of Sciences in 1863,[225] the German Kaiser Wilhelm Society in 1911,[226] and the Chinese Academy of Sciences in 1949.[227] International scientific organisations, such as the International Science Council, are devoted to international cooperation for science advancement.[228]\n Science awards are usually given to individuals or organisations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist receiving them. Since the early Renaissance, scientists have often been awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and chemistry.[229]\n Funding of science is often through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP.[230] In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10%, respectively, by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and the humanities. In less developed nations, the government provides the bulk of the funds for their basic scientific research.[231]\n Many governments have dedicated agencies to support scientific research, such as the National Science Foundation in the United States,[232] the National Scientific and Technical Research Council in Argentina,[233] Commonwealth Scientific and Industrial Research Organisation in Australia,[234] National Centre for Scientific Research in France,[235] the Max Planck Society in Germany,[236] and National Research Council in Spain.[237] In commercial research and development, all but the most research-orientated corporations focus more heavily on near-term commercialisation possibilities than research driven by curiosity.[238]\n Science policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public.[239] Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organisations that fund research.[188]\n Science education for the general public is embedded in the school curriculum, and is supplemented by online pedagogical content (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Major organisations of scientists such as the American Association for the Advancement of Science (AAAS) consider the sciences to be a part of the liberal arts traditions of learning, along with philosophy and history.[240] Scientific literacy is chiefly concerned with an understanding of the scientific method, units and methods of measurement, empiricism, a basic understanding of statistics (correlations, qualitative versus quantitative observations, aggregate statistics), and a basic understanding of core scientific fields such as physics, chemistry, biology, ecology, geology, and computation. As a student advances into higher stages of formal education, the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well.[241]\n The mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter.[242] Few journalists have real scientific knowledge, and even beat reporters who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover.[243][244]\n Science magazines such as New Scientist, Science & Vie, and Scientific American cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research.[245] The science fiction genre, primarily speculative fiction, can transmit the ideas and methods of science to the general public.[246] Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the Creative Writing Science resource developed through the Royal Literary Fund.[247]\n While the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are sceptical about science. Examples are the common notion that COVID-19 is not a major health threat to the US (held by 39% of Americans in August 2021)[248] or the belief that climate change is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020).[249] Psychologists have pointed to four factors driving rejection of scientific results:[250]\n Anti-science attitudes often seem to be caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left.[252] That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardise their social status.[253]\n Attitudes towards science are often determined by political opinions and goals. Government, business and advocacy groups have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the politicisation of science such as anti-intellectualism, perceived threats to religious beliefs, and fear for business interests.[255] Politicisation of science is usually accomplished when scientific information is presented in a way that emphasises the uncertainty associated with the scientific evidence.[256] Tactics such as shifting conversation, failing to acknowledge facts, and capitalising on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence.[257] Examples of issues that have involved the politicisation of science include the global warming controversy, health effects of pesticides, and health effects of tobacco.[257][258]\n",
      "timestamp": "2025-10-09 19:12:39.717822"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Special:EditPage/Template:Glossaries_of_science_and_engineering",
      "text": "Copy and paste: \u2013 \u2014 \u00b0 \u2032 \u2033 \u2248 \u2260 \u2264 \u2265 \u00b1 \u2212 \u00d7 \u00f7 \u2190 \u2192 \u00b7 \u00a7 \u00a0 Sign your posts on talk pages: ~~~~ \u00a0 Cite your sources: <ref></ref> \n \n{{}} \u00a0 {{{}}} \u00a0 | \u00a0 [] \u00a0 [[]] \u00a0 [[Category:]] \u00a0 #REDIRECT [[]] \u00a0 &nbsp; \u00a0 <s></s> \u00a0 <sup></sup> \u00a0 <sub></sub> \u00a0 <code></code> \u00a0 <pre></pre> \u00a0 <blockquote></blockquote> \u00a0 <ref></ref> <ref name=\"\" /> \u00a0 {{Reflist}} \u00a0 <references /> \u00a0 <includeonly></includeonly> \u00a0 <noinclude></noinclude> \u00a0 {{DEFAULTSORT:}} \u00a0 <nowiki></nowiki> \u00a0 <!-- --> \u00a0 <span class=\"plainlinks\"></span> \n\n \nSymbols: ~ | \u00a1 \u00bf \u2020 \u2021 \u2194 \u2191 \u2193 \u2022 \u00b6 \u00a0 # \u221e \u00a0 \u2039\u203a \u00ab\u00bb \u00a0 \u00a4 \u20b3 \u0e3f \u20b5 \u00a2 \u20a1 \u20a2 $ \u20ab \u20af \u20ac \u20a0 \u20a3 \u0192 \u20b4 \u20ad \u20a4 \u2133 \u20a5 \u20a6 \u20a7 \u20b0 \u00a3 \u17db \u20a8 \u20aa \u09f3 \u20ae \u20a9 \u00a5 \u00a0 \u2660 \u2663 \u2665 \u2666 \u00a0 \ud834\udd2b \u266d \u266e \u266f \ud834\udd2a \u00a0 \u00a9 \u00bc \u00bd \u00be\nLatin: A a \u00c1 \u00e1 \u00c0 \u00e0 \u00c2 \u00e2 \u00c4 \u00e4 \u01cd \u01ce \u0102 \u0103 \u0100 \u0101 \u00c3 \u00e3 \u00c5 \u00e5 \u0104 \u0105 \u00c6 \u00e6 \u01e2 \u01e3 \u00a0 B b \u00a0 C c \u0106 \u0107 \u010a \u010b \u0108 \u0109 \u010c \u010d \u00c7 \u00e7 \u00a0 D d \u010e \u010f \u0110 \u0111 \u1e0c \u1e0d \u00d0 \u00f0 \u00a0 E e \u00c9 \u00e9 \u00c8 \u00e8 \u0116 \u0117 \u00ca \u00ea \u00cb \u00eb \u011a \u011b \u0114 \u0115 \u0112 \u0113 \u1ebc \u1ebd \u0118 \u0119 \u1eb8 \u1eb9 \u0190 \u025b \u018e \u01dd \u018f \u0259 \u00a0 F f \u00a0 G g \u0120 \u0121 \u011c \u011d \u011e \u011f \u0122 \u0123 \u00a0 H h \u0124 \u0125 \u0126 \u0127 \u1e24 \u1e25 \u00a0 I i \u0130 \u0131 \u00cd \u00ed \u00cc \u00ec \u00ce \u00ee \u00cf \u00ef \u01cf \u01d0 \u012c \u012d \u012a \u012b \u0128 \u0129 \u012e \u012f \u1eca\u2009\u1ecb \u00a0 J j \u0134 \u0135 \u00a0 K k \u0136 \u0137 \u00a0 L l \u0139 \u013a \u013f \u0140 \u013d \u013e \u013b \u013c \u0141 \u0142 \u1e36 \u1e37 \u1e38 \u1e39 \u00a0 M m \u1e42 \u1e43 \u00a0 N n \u0143 \u0144 \u0147 \u0148 \u00d1 \u00f1 \u0145 \u0146 \u1e46 \u1e47 \u014a \u014b \u00a0 O o \u00d3 \u00f3 \u00d2 \u00f2 \u00d4 \u00f4 \u00d6 \u00f6 \u01d1 \u01d2 \u014e \u014f \u014c \u014d \u00d5 \u00f5 \u01ea \u01eb \u1ecc \u1ecd \u0150 \u0151 \u00d8 \u00f8 \u0152 \u0153 \u00a0 \u0186 \u0254 \u00a0 P p \u00a0 Q q \u00a0 R r \u0154 \u0155 \u0158 \u0159 \u0156 \u0157 \u1e5a \u1e5b \u1e5c \u1e5d \u00a0 S s \u015a \u015b \u015c \u015d \u0160 \u0161 \u015e \u015f \u0218 \u0219 \u1e62 \u1e63 \u00df \u00a0 T t \u0164 \u0165 \u0162 \u0163 \u021a \u021b \u1e6c \u1e6d \u00de \u00fe \u00a0 U u \u00da \u00fa \u00d9 \u00f9 \u00db \u00fb \u00dc \u00fc \u01d3 \u01d4 \u016c \u016d \u016a \u016b \u0168 \u0169 \u016e \u016f \u0172 \u0173 \u1ee4 \u1ee5 \u0170 \u0171 \u01d7 \u01d8 \u01db \u01dc \u01d9 \u01da \u01d5 \u01d6 \u00a0 V v \u00a0 W w \u0174 \u0175 \u00a0 X x \u00a0 Y y \u00dd \u00fd \u0176 \u0177 \u0178 \u00ff \u1ef8 \u1ef9 \u0232 \u0233 \u00a0 Z z \u0179 \u017a \u017b \u017c \u017d \u017e \u00a0 \u00df \u00d0 \u00f0 \u00de \u00fe \u014a \u014b \u018f \u0259 \nGreek: \u0386 \u03ac \u0388 \u03ad \u0389 \u03ae \u038a \u03af \u038c \u03cc \u038e \u03cd \u038f \u03ce \u00a0 \u0391 \u03b1 \u0392 \u03b2 \u0393 \u03b3 \u0394 \u03b4 \u00a0 \u0395 \u03b5 \u0396 \u03b6 \u0397 \u03b7 \u0398 \u03b8 \u00a0 \u0399 \u03b9 \u039a \u03ba \u039b \u03bb \u039c \u03bc \u00a0 \u039d \u03bd \u039e \u03be \u039f \u03bf \u03a0 \u03c0 \u00a0 \u03a1 \u03c1 \u03a3 \u03c3 \u03c2 \u03a4 \u03c4 \u03a5 \u03c5 \u00a0 \u03a6 \u03c6 \u03a7 \u03c7 \u03a8 \u03c8 \u03a9 \u03c9 \u00a0 {{Polytonic|}} \nCyrillic: \u0410 \u0430 \u0411 \u0431 \u0412 \u0432 \u0413 \u0433 \u00a0 \u0490 \u0491 \u0403 \u0453 \u0414 \u0434 \u0402 \u0452 \u00a0 \u0415 \u0435 \u0401 \u0451 \u0404 \u0454 \u0416 \u0436 \u00a0 \u0417 \u0437 \u0405 \u0455 \u0418 \u0438 \u0406 \u0456 \u00a0 \u0407 \u0457 \u0419 \u0439 \u0408 \u0458 \u041a \u043a \u00a0 \u040c \u045c \u041b \u043b \u0409 \u0459 \u041c \u043c \u00a0 \u041d \u043d \u040a \u045a \u041e \u043e \u041f \u043f \u00a0 \u0420 \u0440 \u0421 \u0441 \u0422 \u0442 \u040b \u045b \u00a0 \u0423 \u0443 \u040e \u045e \u0424 \u0444 \u0425 \u0445 \u00a0 \u0426 \u0446 \u0427 \u0447 \u040f \u045f \u0428 \u0448 \u00a0 \u0429 \u0449 \u042a \u044a \u042b \u044b \u042c \u044c \u00a0 \u042d \u044d \u042e \u044e \u042f \u044f \u00a0 \u0301 \nIPA: t\u032a d\u032a \u0288 \u0256 \u025f \u0261 \u0262 \u02a1 \u0294 \u00a0 \u0278 \u03b2 \u03b8 \u00f0 \u0283 \u0292 \u0255 \u0291 \u0282 \u0290 \u00e7 \u029d \u0263 \u03c7 \u0281 \u0127 \u0295 \u029c \u02a2 \u0266 \u00a0 \u0271 \u0273 \u0272 \u014b \u0274 \u00a0 \u028b \u0279 \u027b \u0270 \u00a0 \u0299 \u2c71 \u0280 \u027e \u027d \u00a0 \u026b \u026c \u026e \u027a \u026d \u028e \u029f \u00a0 \u0265 \u028d \u0267 \u00a0 \u02bc \u00a0 \u0253 \u0257 \u0284 \u0260 \u029b \u00a0 \u0298 \u01c0 \u01c3 \u01c2 \u01c1 \u00a0 \u0268 \u0289 \u026f \u00a0 \u026a \u028f \u028a \u00a0 \u00f8 \u0258 \u0275 \u0264 \u00a0 \u0259 \u025a \u00a0 \u025b \u0153 \u025c \u025d \u025e \u028c \u0254 \u00a0 \u00e6 \u00a0 \u0250 \u0276 \u0251 \u0252 \u00a0 \u02b0 \u02b1 \u02b7 \u02b2 \u02e0 \u02e4 \u207f \u02e1 \u00a0 \u02c8 \u02cc \u02d0 \u02d1 \u032a \u00a0 {{IPA|}}\n\n Wikidata entities used in this page\n Pages transcluded onto the current version of this page (help):\n",
      "timestamp": "2025-10-09 19:12:40.380615"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Special:EditPage/Template:Glossaries_of_science_and_engineering",
      "title": "Editing Template:Glossaries of science and engineering - Wikipedia",
      "description": "",
      "text": "Copy and paste: \u2013 \u2014 \u00b0 \u2032 \u2033 \u2248 \u2260 \u2264 \u2265 \u00b1 \u2212 \u00d7 \u00f7 \u2190 \u2192 \u00b7 \u00a7 \u00a0 Sign your posts on talk pages: ~~~~ \u00a0 Cite your sources: <ref></ref> \n \n{{}} \u00a0 {{{}}} \u00a0 | \u00a0 [] \u00a0 [[]] \u00a0 [[Category:]] \u00a0 #REDIRECT [[]] \u00a0 &nbsp; \u00a0 <s></s> \u00a0 <sup></sup> \u00a0 <sub></sub> \u00a0 <code></code> \u00a0 <pre></pre> \u00a0 <blockquote></blockquote> \u00a0 <ref></ref> <ref name=\"\" /> \u00a0 {{Reflist}} \u00a0 <references /> \u00a0 <includeonly></includeonly> \u00a0 <noinclude></noinclude> \u00a0 {{DEFAULTSORT:}} \u00a0 <nowiki></nowiki> \u00a0 <!-- --> \u00a0 <span class=\"plainlinks\"></span> \n\n \nSymbols: ~ | \u00a1 \u00bf \u2020 \u2021 \u2194 \u2191 \u2193 \u2022 \u00b6 \u00a0 # \u221e \u00a0 \u2039\u203a \u00ab\u00bb \u00a0 \u00a4 \u20b3 \u0e3f \u20b5 \u00a2 \u20a1 \u20a2 $ \u20ab \u20af \u20ac \u20a0 \u20a3 \u0192 \u20b4 \u20ad \u20a4 \u2133 \u20a5 \u20a6 \u20a7 \u20b0 \u00a3 \u17db \u20a8 \u20aa \u09f3 \u20ae \u20a9 \u00a5 \u00a0 \u2660 \u2663 \u2665 \u2666 \u00a0 \ud834\udd2b \u266d \u266e \u266f \ud834\udd2a \u00a0 \u00a9 \u00bc \u00bd \u00be\nLatin: A a \u00c1 \u00e1 \u00c0 \u00e0 \u00c2 \u00e2 \u00c4 \u00e4 \u01cd \u01ce \u0102 \u0103 \u0100 \u0101 \u00c3 \u00e3 \u00c5 \u00e5 \u0104 \u0105 \u00c6 \u00e6 \u01e2 \u01e3 \u00a0 B b \u00a0 C c \u0106 \u0107 \u010a \u010b \u0108 \u0109 \u010c \u010d \u00c7 \u00e7 \u00a0 D d \u010e \u010f \u0110 \u0111 \u1e0c \u1e0d \u00d0 \u00f0 \u00a0 E e \u00c9 \u00e9 \u00c8 \u00e8 \u0116 \u0117 \u00ca \u00ea \u00cb \u00eb \u011a \u011b \u0114 \u0115 \u0112 \u0113 \u1ebc \u1ebd \u0118 \u0119 \u1eb8 \u1eb9 \u0190 \u025b \u018e \u01dd \u018f \u0259 \u00a0 F f \u00a0 G g \u0120 \u0121 \u011c \u011d \u011e \u011f \u0122 \u0123 \u00a0 H h \u0124 \u0125 \u0126 \u0127 \u1e24 \u1e25 \u00a0 I i \u0130 \u0131 \u00cd \u00ed \u00cc \u00ec \u00ce \u00ee \u00cf \u00ef \u01cf \u01d0 \u012c \u012d \u012a \u012b \u0128 \u0129 \u012e \u012f \u1eca\u2009\u1ecb \u00a0 J j \u0134 \u0135 \u00a0 K k \u0136 \u0137 \u00a0 L l \u0139 \u013a \u013f \u0140 \u013d \u013e \u013b \u013c \u0141 \u0142 \u1e36 \u1e37 \u1e38 \u1e39 \u00a0 M m \u1e42 \u1e43 \u00a0 N n \u0143 \u0144 \u0147 \u0148 \u00d1 \u00f1 \u0145 \u0146 \u1e46 \u1e47 \u014a \u014b \u00a0 O o \u00d3 \u00f3 \u00d2 \u00f2 \u00d4 \u00f4 \u00d6 \u00f6 \u01d1 \u01d2 \u014e \u014f \u014c \u014d \u00d5 \u00f5 \u01ea \u01eb \u1ecc \u1ecd \u0150 \u0151 \u00d8 \u00f8 \u0152 \u0153 \u00a0 \u0186 \u0254 \u00a0 P p \u00a0 Q q \u00a0 R r \u0154 \u0155 \u0158 \u0159 \u0156 \u0157 \u1e5a \u1e5b \u1e5c \u1e5d \u00a0 S s \u015a \u015b \u015c \u015d \u0160 \u0161 \u015e \u015f \u0218 \u0219 \u1e62 \u1e63 \u00df \u00a0 T t \u0164 \u0165 \u0162 \u0163 \u021a \u021b \u1e6c \u1e6d \u00de \u00fe \u00a0 U u \u00da \u00fa \u00d9 \u00f9 \u00db \u00fb \u00dc \u00fc \u01d3 \u01d4 \u016c \u016d \u016a \u016b \u0168 \u0169 \u016e \u016f \u0172 \u0173 \u1ee4 \u1ee5 \u0170 \u0171 \u01d7 \u01d8 \u01db \u01dc \u01d9 \u01da \u01d5 \u01d6 \u00a0 V v \u00a0 W w \u0174 \u0175 \u00a0 X x \u00a0 Y y \u00dd \u00fd \u0176 \u0177 \u0178 \u00ff \u1ef8 \u1ef9 \u0232 \u0233 \u00a0 Z z \u0179 \u017a \u017b \u017c \u017d \u017e \u00a0 \u00df \u00d0 \u00f0 \u00de \u00fe \u014a \u014b \u018f \u0259 \nGreek: \u0386 \u03ac \u0388 \u03ad \u0389 \u03ae \u038a \u03af \u038c \u03cc \u038e \u03cd \u038f \u03ce \u00a0 \u0391 \u03b1 \u0392 \u03b2 \u0393 \u03b3 \u0394 \u03b4 \u00a0 \u0395 \u03b5 \u0396 \u03b6 \u0397 \u03b7 \u0398 \u03b8 \u00a0 \u0399 \u03b9 \u039a \u03ba \u039b \u03bb \u039c \u03bc \u00a0 \u039d \u03bd \u039e \u03be \u039f \u03bf \u03a0 \u03c0 \u00a0 \u03a1 \u03c1 \u03a3 \u03c3 \u03c2 \u03a4 \u03c4 \u03a5 \u03c5 \u00a0 \u03a6 \u03c6 \u03a7 \u03c7 \u03a8 \u03c8 \u03a9 \u03c9 \u00a0 {{Polytonic|}} \nCyrillic: \u0410 \u0430 \u0411 \u0431 \u0412 \u0432 \u0413 \u0433 \u00a0 \u0490 \u0491 \u0403 \u0453 \u0414 \u0434 \u0402 \u0452 \u00a0 \u0415 \u0435 \u0401 \u0451 \u0404 \u0454 \u0416 \u0436 \u00a0 \u0417 \u0437 \u0405 \u0455 \u0418 \u0438 \u0406 \u0456 \u00a0 \u0407 \u0457 \u0419 \u0439 \u0408 \u0458 \u041a \u043a \u00a0 \u040c \u045c \u041b \u043b \u0409 \u0459 \u041c \u043c \u00a0 \u041d \u043d \u040a \u045a \u041e \u043e \u041f \u043f \u00a0 \u0420 \u0440 \u0421 \u0441 \u0422 \u0442 \u040b \u045b \u00a0 \u0423 \u0443 \u040e \u045e \u0424 \u0444 \u0425 \u0445 \u00a0 \u0426 \u0446 \u0427 \u0447 \u040f \u045f \u0428 \u0448 \u00a0 \u0429 \u0449 \u042a \u044a \u042b \u044b \u042c \u044c \u00a0 \u042d \u044d \u042e \u044e \u042f \u044f \u00a0 \u0301 \nIPA: t\u032a d\u032a \u0288 \u0256 \u025f \u0261 \u0262 \u02a1 \u0294 \u00a0 \u0278 \u03b2 \u03b8 \u00f0 \u0283 \u0292 \u0255 \u0291 \u0282 \u0290 \u00e7 \u029d \u0263 \u03c7 \u0281 \u0127 \u0295 \u029c \u02a2 \u0266 \u00a0 \u0271 \u0273 \u0272 \u014b \u0274 \u00a0 \u028b \u0279 \u027b \u0270 \u00a0 \u0299 \u2c71 \u0280 \u027e \u027d \u00a0 \u026b \u026c \u026e \u027a \u026d \u028e \u029f \u00a0 \u0265 \u028d \u0267 \u00a0 \u02bc \u00a0 \u0253 \u0257 \u0284 \u0260 \u029b \u00a0 \u0298 \u01c0 \u01c3 \u01c2 \u01c1 \u00a0 \u0268 \u0289 \u026f \u00a0 \u026a \u028f \u028a \u00a0 \u00f8 \u0258 \u0275 \u0264 \u00a0 \u0259 \u025a \u00a0 \u025b \u0153 \u025c \u025d \u025e \u028c \u0254 \u00a0 \u00e6 \u00a0 \u0250 \u0276 \u0251 \u0252 \u00a0 \u02b0 \u02b1 \u02b7 \u02b2 \u02e0 \u02e4 \u207f \u02e1 \u00a0 \u02c8 \u02cc \u02d0 \u02d1 \u032a \u00a0 {{IPA|}}\n\n Wikidata entities used in this page\n Pages transcluded onto the current version of this page (help):\n",
      "timestamp": "2025-10-09 19:12:40.381755"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Template_talk:Glossaries_of_science_and_engineering",
      "text": "Talk pages are where people discuss how to make content on Wikipedia the best that it can be. You can use this page to start a discussion with others about how to improve the \"Template:Glossaries of science and engineering\" page.\n",
      "timestamp": "2025-10-09 19:12:40.833159"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Template_talk:Glossaries_of_science_and_engineering",
      "title": "Template talk:Glossaries of science and engineering - Wikipedia",
      "description": "",
      "text": "Talk pages are where people discuss how to make content on Wikipedia the best that it can be. You can use this page to start a discussion with others about how to improve the \"Template:Glossaries of science and engineering\" page.\n",
      "timestamp": "2025-10-09 19:12:40.834064"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Template:Glossaries_of_science_and_engineering",
      "text": "This template's initial visibility currently defaults to autocollapse, meaning that if there is another collapsible item on the page (a navbox, sidebar, or table with the collapsible attribute), it is hidden apart from its title bar; if not, it is fully visible.\n To change this template's initial visibility, the |state= parameter may be used:\n",
      "timestamp": "2025-10-09 19:12:41.260960"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Template:Glossaries_of_science_and_engineering",
      "title": "Template:Glossaries of science and engineering - Wikipedia",
      "description": "",
      "text": "This template's initial visibility currently defaults to autocollapse, meaning that if there is another collapsible item on the page (a navbox, sidebar, or table with the collapsible attribute), it is hidden apart from its title bar; if not, it is fully visible.\n To change this template's initial visibility, the |state= parameter may be used:\n",
      "timestamp": "2025-10-09 19:12:41.262215"
    },
    {
      "url": "https://en.wikipedia.org/wiki/William_Grey_Walter",
      "text": "\n William Grey Walter (February 19, 1910 \u2013 May 6, 1977) was an American-born British neurophysiologist, cybernetician and robotician.\n Walter was born in Kansas City, Missouri, United States, on 19 February 1910, the only child of Minerva Lucrezia (Margaret) Hardy (1879\u20131953), an American journalist, and Karl Wilhelm Walter (1880\u20131965), a British journalist who was working on the Kansas City Star at the time. His parents had met and married in Italy, and during the First World War the family moved  to Britain. Walter's ancestry was German/British on his father's side, and American/British on his mother's side. He was brought to England in 1915, educated at Westminster School with an interest in classics and science, and entered King's College, Cambridge, in 1928. He achieved a third class in part one (1930) and a first class in physiology in part two of the natural sciences tripos (1931).[1]\n He failed to obtain a research fellowship in Cambridge and so turned to doing basic and applied neurophysiological research in hospitals, in London, from 1935 to 1939 and then at the Burden Neurological Institute in Bristol, from 1939 to 1970. He also carried out research work in the United States, in the Soviet Union and in various other places in Europe. He married twice, having two sons from his first marriage, and one from the second. According to his eldest son, Nicolas Walter, \"he was politically on the left, a communist fellow-traveller before the Second World War and an anarchist sympathiser after it.\"[2]:\u200a34\u201342\u200a Throughout his life he was a pioneer in the field of cybernetics. In 1970, he suffered a brain injury in a motor scooter accident.[3] He never fully recovered and died seven years later, on May 6, 1977.\n As a young man, Walter was greatly influenced by the work of the Russian physiologist Ivan Pavlov.[citation needed] He visited the lab of Hans Berger, who invented the electroencephalograph, or EEG machine, for measuring electrical activity in the brain. Walter produced his own versions of Berger's machine with improved capabilities, which allowed it to detect a variety of brain wave types ranging from the high speed alpha waves to the slow delta waves observed during sleep.\n In the 1930s, Walter made a number of discoveries using his EEG machines at the Burden Neurological Institute in Bristol.[4]:\u200a60\u201383\u200a He was the first to determine by triangulation the surface location of the strongest alpha waves within the occipital lobe (alpha waves originate from the thalamus deep within the brain). Walter demonstrated the use of delta waves to locate brain tumours or lesions responsible for epilepsy. He developed the first brain topography machine based on EEG, using an array of spiral-scan CRTs connected to high-gain amplifiers.\n During the Second World War, Walter worked on scanning radar technology and guided missiles, which may have influenced his subsequent alpha wave scanning hypothesis of brain activity.[5]\n In the 1960s, Walter also went on to discover the contingent negative variation (CNV) effect whereby a negative spike of electrical activity appears in the brain half a second prior to a person being consciously aware of movements they were about to make.[6] Intriguingly, this effect brings into question the very notion of consciousness or free will, and should be considered as part of a person's overall reaction time to events.\n Walter's experiments with stroboscopic light, described in The Living Brain,[7] inspired the development of the Dreamachine by the artist Brion Gysin and technician Ian Sommerville, a device that has evolved into electronic devices known as mind machines.\n Grey Walter's best-known work was his construction of some of the first electronic autonomous robots.[8] He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors - essentially that the secret of how the brain worked lay in how it was wired up. His first robots, which he used to call Machina speculatrix[9] and named Elmer and Elsie, were constructed between 1948 and 1949 and were often described as tortoises[10] due to their shape and slow rate of movement - and because they \"taught us\" about the secrets of organisation and life. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.\n In one experiment, Walter placed a light on the \"nose\" of a tortoise and watched as the robot observed itself in a mirror. \"It began flickering,\" he wrote. \"Twittering, and jigging like a clumsy Narcissus.\"[11] Walter argued that if it were seen in an animal it \"might be accepted as evidence of some degree of self-awareness.\"\n One of the tortoises was modified, (given the pretend scientific name Machina docilis) and added to its simple single celled \"brain\" one, then two conditional reflex circuits in which they could be taught simple behaviors similar to Ivan Pavlov's dogs.[citation needed] This tortoise was called CORA. One of these included that being hit meant food whilst whistling means food, and when conditioned such a whistle by itself means being hit. When he added another circuit tuned to a whistle of another pitch, this could become whistle means being hit, whistle means food, and this would make the animal become \"afraid\" whenever food was presented. Walter remedied this behaviour by severing the two additional circuits, and the tortoise reverted to being a Machina speculatrix. The conditioned reflex behaviour was later placed into a static desktop model, also known as CORA.\n Later versions of Machina speculatrix were exhibited at the Festival of Britain in 1951.[12]\nWalter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers, including Rodney Brooks, Hans Moravec and Mark Tilden.[citation needed] Modern incarnations of Walter's turtles may be found in the form of BEAM robotics.\n In 2000, an original tortoise went on display in London, UK, in the Science Museum.[13] In 1995, this robot was also replicated by Dr. Owen Holland and Ian Horsfield[14] of the Bristol Robotics Laboratory using some of the original parts.[15] An original tortoise as seen at the Festival of Britain is in the collection of the Smithsonian Institution.\n Walter's papers including his letters, photographs and press cuttings form part of the Burden archive held at the Science Museum Library & Archives in Wroughton Science Museum at Wroughton.\n Walter married twice. His first wife was Katherine Monica Ratcliffe (1911-2012), daughter of Samuel Kerkham Ratcliffe (1868-1958), a former member of the executive of the Fabian Society. They had two sons Nicolas Hardy Walter (1934\u20132000) and Jeremy Walter, who became a physicist.[1] After the couple separated in 1945, and divorced in 1946, their children were brought up by their mother Monica and her second husband Cambridge University scientist Arnold Beck.\n Walter's second wife was the radiographer Vivian Dovey (1915-1980). They married in Bristol 1947[17] and had one child, Timothy Walter (1949-1976) before separating in 1960, and divorcing in 1973.[18] It has been noted that Walter and his institution gave a male biased view of their work. Vivian Dovey was a significant collaborator, yet she was depicted as a wife or assistant who cared for him.[16]\n From 1960 to 1972, Walter lived with Lorraine Josephine Aldridge (n\u00e9e Donn), former wife of Keith Aldridge.[1] Vivian Dovey lived with Keith Aldridge and later took his name after her divorce.[18]\n",
      "timestamp": "2025-10-09 19:12:41.659100"
    },
    {
      "url": "https://en.wikipedia.org/wiki/William_Grey_Walter",
      "title": "William Grey Walter - Wikipedia",
      "description": "",
      "text": "\n William Grey Walter (February 19, 1910 \u2013 May 6, 1977) was an American-born British neurophysiologist, cybernetician and robotician.\n Walter was born in Kansas City, Missouri, United States, on 19 February 1910, the only child of Minerva Lucrezia (Margaret) Hardy (1879\u20131953), an American journalist, and Karl Wilhelm Walter (1880\u20131965), a British journalist who was working on the Kansas City Star at the time. His parents had met and married in Italy, and during the First World War the family moved  to Britain. Walter's ancestry was German/British on his father's side, and American/British on his mother's side. He was brought to England in 1915, educated at Westminster School with an interest in classics and science, and entered King's College, Cambridge, in 1928. He achieved a third class in part one (1930) and a first class in physiology in part two of the natural sciences tripos (1931).[1]\n He failed to obtain a research fellowship in Cambridge and so turned to doing basic and applied neurophysiological research in hospitals, in London, from 1935 to 1939 and then at the Burden Neurological Institute in Bristol, from 1939 to 1970. He also carried out research work in the United States, in the Soviet Union and in various other places in Europe. He married twice, having two sons from his first marriage, and one from the second. According to his eldest son, Nicolas Walter, \"he was politically on the left, a communist fellow-traveller before the Second World War and an anarchist sympathiser after it.\"[2]:\u200a34\u201342\u200a Throughout his life he was a pioneer in the field of cybernetics. In 1970, he suffered a brain injury in a motor scooter accident.[3] He never fully recovered and died seven years later, on May 6, 1977.\n As a young man, Walter was greatly influenced by the work of the Russian physiologist Ivan Pavlov.[citation needed] He visited the lab of Hans Berger, who invented the electroencephalograph, or EEG machine, for measuring electrical activity in the brain. Walter produced his own versions of Berger's machine with improved capabilities, which allowed it to detect a variety of brain wave types ranging from the high speed alpha waves to the slow delta waves observed during sleep.\n In the 1930s, Walter made a number of discoveries using his EEG machines at the Burden Neurological Institute in Bristol.[4]:\u200a60\u201383\u200a He was the first to determine by triangulation the surface location of the strongest alpha waves within the occipital lobe (alpha waves originate from the thalamus deep within the brain). Walter demonstrated the use of delta waves to locate brain tumours or lesions responsible for epilepsy. He developed the first brain topography machine based on EEG, using an array of spiral-scan CRTs connected to high-gain amplifiers.\n During the Second World War, Walter worked on scanning radar technology and guided missiles, which may have influenced his subsequent alpha wave scanning hypothesis of brain activity.[5]\n In the 1960s, Walter also went on to discover the contingent negative variation (CNV) effect whereby a negative spike of electrical activity appears in the brain half a second prior to a person being consciously aware of movements they were about to make.[6] Intriguingly, this effect brings into question the very notion of consciousness or free will, and should be considered as part of a person's overall reaction time to events.\n Walter's experiments with stroboscopic light, described in The Living Brain,[7] inspired the development of the Dreamachine by the artist Brion Gysin and technician Ian Sommerville, a device that has evolved into electronic devices known as mind machines.\n Grey Walter's best-known work was his construction of some of the first electronic autonomous robots.[8] He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors - essentially that the secret of how the brain worked lay in how it was wired up. His first robots, which he used to call Machina speculatrix[9] and named Elmer and Elsie, were constructed between 1948 and 1949 and were often described as tortoises[10] due to their shape and slow rate of movement - and because they \"taught us\" about the secrets of organisation and life. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.\n In one experiment, Walter placed a light on the \"nose\" of a tortoise and watched as the robot observed itself in a mirror. \"It began flickering,\" he wrote. \"Twittering, and jigging like a clumsy Narcissus.\"[11] Walter argued that if it were seen in an animal it \"might be accepted as evidence of some degree of self-awareness.\"\n One of the tortoises was modified, (given the pretend scientific name Machina docilis) and added to its simple single celled \"brain\" one, then two conditional reflex circuits in which they could be taught simple behaviors similar to Ivan Pavlov's dogs.[citation needed] This tortoise was called CORA. One of these included that being hit meant food whilst whistling means food, and when conditioned such a whistle by itself means being hit. When he added another circuit tuned to a whistle of another pitch, this could become whistle means being hit, whistle means food, and this would make the animal become \"afraid\" whenever food was presented. Walter remedied this behaviour by severing the two additional circuits, and the tortoise reverted to being a Machina speculatrix. The conditioned reflex behaviour was later placed into a static desktop model, also known as CORA.\n Later versions of Machina speculatrix were exhibited at the Festival of Britain in 1951.[12]\nWalter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers, including Rodney Brooks, Hans Moravec and Mark Tilden.[citation needed] Modern incarnations of Walter's turtles may be found in the form of BEAM robotics.\n In 2000, an original tortoise went on display in London, UK, in the Science Museum.[13] In 1995, this robot was also replicated by Dr. Owen Holland and Ian Horsfield[14] of the Bristol Robotics Laboratory using some of the original parts.[15] An original tortoise as seen at the Festival of Britain is in the collection of the Smithsonian Institution.\n Walter's papers including his letters, photographs and press cuttings form part of the Burden archive held at the Science Museum Library & Archives in Wroughton Science Museum at Wroughton.\n Walter married twice. His first wife was Katherine Monica Ratcliffe (1911-2012), daughter of Samuel Kerkham Ratcliffe (1868-1958), a former member of the executive of the Fabian Society. They had two sons Nicolas Hardy Walter (1934\u20132000) and Jeremy Walter, who became a physicist.[1] After the couple separated in 1945, and divorced in 1946, their children were brought up by their mother Monica and her second husband Cambridge University scientist Arnold Beck.\n Walter's second wife was the radiographer Vivian Dovey (1915-1980). They married in Bristol 1947[17] and had one child, Timothy Walter (1949-1976) before separating in 1960, and divorcing in 1973.[18] It has been noted that Walter and his institution gave a male biased view of their work. Vivian Dovey was a significant collaborator, yet she was depicted as a wife or assistant who cared for him.[16]\n From 1960 to 1972, Walter lived with Lorraine Josephine Aldridge (n\u00e9e Donn), former wife of Keith Aldridge.[1] Vivian Dovey lived with Keith Aldridge and later took his name after her divorce.[18]\n",
      "timestamp": "2025-10-09 19:12:41.661444"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch",
      "text": "Warren Sturgis McCulloch (November 16, 1898 \u2013 September 24, 1969) was an American neurophysiologist and cybernetician known for his work on the foundation for certain brain theories and his contribution to the cybernetics movement.[1] Along with Walter Pitts, McCulloch created computational models based on mathematical algorithms called threshold logic which split the inquiry into two distinct approaches, one approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.[2]\n Warren Sturgis McCulloch was born in Orange, New Jersey, in 1898. His brother was a chemical engineer and Warren was originally planning to join the Christian ministry. As a teenager he was associated with the theologians Henry Sloane Coffin, Harry Emerson Fosdick, Herman Karl Wilhelm Kumm and Julian F. Hecker. He was also mentored by the Quaker Rufus Jones.[3] He attended Haverford College then studied philosophy and psychology at Yale University, where he received a Bachelor of Arts degree in 1921.  He continued to study psychology at Columbia and received a Master of Arts degree in 1923. Receiving his MD in 1927 from the Columbia University College of Physicians and Surgeons in New York, he undertook an internship at Bellevue Hospital, New York. Then he worked under Eilhard von Domarus at the Rockland State Hospital for the Insane.[4] He returned to academia in 1934.  He worked at the Laboratory for Neurophysiology at Yale University from 1934 to 1941.\n In 1941 he moved to Chicago and joined the Department of Psychiatry at the University of Illinois at Chicago, where he was a professor of psychiatry, as well as the director of the Illinois Neuropsychiatric Institute until 1951.[5] From 1952 he worked at the Massachusetts Institute of Technology in Cambridge, Massachusetts with Norbert Wiener. He was a founding member of the American Society for Cybernetics and its second president during 1967\u20131968.  He was a mentor to the British operations research pioneer Stafford Beer.\n McCulloch had a range of interests and talents. In addition to his scientific contributions he wrote poetry (sonnets), and he designed and engineered buildings and a dam at his farm in Old Lyme, Connecticut.\n McCulloch married Ruth Metzger, known as 'Rook', in 1924 and they had three children.[6] He died in Cambridge in 1969.\n He is remembered for his work with Joannes Gregorius Dusser de Barenne from Yale[7] and later with Walter Pitts from the University of Chicago. He provided the foundation for certain brain theories in a number of classic papers, including \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" (1943) and \"How We Know Universals:  The Perception of Auditory and Visual Forms\" (1947), both published in the Bulletin of Mathematical Biophysics. The former is \"widely credited with being a seminal contribution to neural network theory, the theory of automata, the theory of computation, and cybernetics\".[1]\n McCulloch was the chair of the set of Macy conferences dedicated to Cybernetics. These, greatly due to the diversity of the backgrounds of the participants McCulloch brought in, became the foundation for the field.\n In Wiener's Cybernetics (1948), he recounted an event in the spring of 1947, when McCulloch designed a machine to allow the blind to read, by converting printed letters to tones. He designed it so that the tone is invariant for the same letter viewed under different angles. Gerhardt von Bonin saw the design, and immediately asked, \" Is this a diagram of the fourth layer of the visual cortex of the brain?\".[8]:\u200a22,\u200a140\u200a\n In his last days in 1960s, he worked on loops, oscillations and triadic relations with Moreno-D\u00edaz; the reticular formation with Kilmer and dynamic models of memory with Da Fonseca.[9] His work in the 1960s was summarized in a 1968 paper.[10]\n He studied the excitation of the brain by strychnine neuronography, which was a method to map brain connections. Applying strychnine in one point of the brain causes excitations in different points of the brain.[11] Bailey, Bonin, and McCulloch conducted a series of studies in the 1940s that identified connections in the brains of macaque and chimpanzee that are consistent with modern understanding of VOF.[12][13]\n In 1919 he began to work mainly on mathematical logic, and by 1923 he attempted to make a logic of transitive verbs. His goal in psychology is to invent a \"psychon\" or \"least psychic event\" that are binary atomic events with necessary causes, such that they can be combined to create complex logical propositions concerning their antecedents. He noticed in 1929 that these may correspond to the all-or-nothing firings of neurons in the brain.[3]\n In the 1943 paper, they described how memories can be formed by a neural network with loops in it, or alterable synapses. These then encodes for sentences like \"There was some x such that x was a \u03c8\" or \n\n\n\n(\n\u2203\nx\n)\n(\n\u03c8\nx\n)\n\n\n{\\displaystyle (\\exists x)(\\psi x)}\n\n, and showed that looped neural networks can encode all first-order logic with equality and conversely, any looped neural networks is equivalent to a sentence in first-order logic with equality, thus showing that they are equivalent in logical expressiveness.[3]\n The 1943 paper describes neural networks operating over time, and logical universals -- \"there exists\" and \"for all\"\u2014for spatial objects, such as geometric figures, was further developed in their 1947 paper.[14]\n He worked with Manuel Blum in studying how a neural network can be \"logically stable\", that is, can implement a boolean function even if the activation thresholds of individual neurons are varied.[15]:\u200a64\u200a They were inspired by the problem of how the brain can perform the same functions, such as breathing, under influence of caffeine or alcohol, which shifts the activation threshold over the entire brain.[3]\n He worked on triadic relations, an extension of the calculus of relations to handle relations that relates 3 objects, such as \"A gives B to C\" or \"A perceives B to be C\". He was convinced that such a logic is necessary for understanding brain activity.[10][16]\n In the 1947 paper How we know universals, they studied the problem of recognizing objects despite changes in representation. For example, recognizing a square under different viewing angles and lighting conditions, or recognizing a phoneme under different loudness and tones. That is, recognizing objects invariant under the action of some symmetry group. This problem was partly inspired by a practical problem in designing a machine for the blind to read (recounted in Wiener's Cybernetics, see before).[17]\n The paper proposed two solutions. The first is in computing an invariant by averaging over the symmetry group. Let the symmetry group be \n\n\n\nG\n\n\n{\\displaystyle G}\n\n and the object to be recognized be \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. Let a neural network implement a function \n\n\n\nT\n\n\n{\\displaystyle T}\n\n. Then, the group-invariant representation would be \n\n\n\n\n\n1\n\n\n|\n\nG\n\n|\n\n\n\n\n\n\u2211\n\ng\n\u2208\nG\n\n\nT\n(\ng\nx\n)\n\n\n{\\displaystyle {\\frac {1}{|G|}}\\sum _{g\\in G}T(gx)}\n\n, the group-action average. The second solution is in a negative feedback circuit that drives a canonical representation. Consider the problem of recognizing whether an object is a square. The circuit moves the eye so that the \"center of gravity of brightness\" of the object is moved to the middle of the visual field. This then effectively converts each object into a canonical representation, which can then be compared with a representation in the brain.[18][19]\n In the 1943 paper McCulloch and Pitts attempted to demonstrate that a Turing machine program could be implemented in a finite network of formal neurons (in the event, the Turing Machine contains their model of the brain, but the converse is not true[20]), that the neuron was the base logic unit of the brain. In the 1947 paper they offered approaches to designing \"nervous nets\" to recognize visual inputs despite changes in orientation or size.\n From 1952 McCulloch worked at the Research Laboratory of Electronics at MIT, working primarily on neural network modelling. His team examined the visual system of the frog in consideration of McCulloch's 1947 paper, discovering that the eye provides the brain with information that is already, to a degree, organized and interpreted, instead of simply transmitting an image.\n With Roberto Moreno-D\u00edaz, he studied a formalized problem of memory. Given that neural networks can store memory by a pattern of oscillations in a circle, they studied the number of possible oscillation patterns that can be sustained by some neural network with \n\n\n\nN\n\n\n{\\displaystyle N}\n\n neurons. This came out to be \n\n\n\nK\n(\nN\n)\n=\n\n\n\n(\n\n\n\n2\n\nN\n\n\nk\n\n\n)\n\n\n\n\n\u2211\n\nk\n=\n1\n\n\n\n2\n\nN\n\n\n\u2212\n1\n\n\nk\n!\n\n\n{\\displaystyle K(N)={\\binom {2^{N}}{k}}\\sum _{k=1}^{2^{N}-1}k!}\n\n (Schnabel, 1966).[21] Also, they proved a universality theorem, in that for each \n\n\n\nN\n\n\n{\\displaystyle N}\n\n, there exists a neural network (possibly with more than \n\n\n\nN\n\n\n{\\displaystyle N}\n\n neurons) with \n\n\n\n\nlog\n\n2\n\n\n\u2061\nK\n(\nN\n)\n\n\n{\\displaystyle \\log _{2}K(N)}\n\n binary inputs, such that, for any oscillation pattern realizable by some neural network with \n\n\n\nN\n\n\n{\\displaystyle N}\n\n neurons, there exists a binary input for this universal network such that it exhibits the same pattern.[22][23][24]\n McCulloch considered the problem of contradictory information and motives, which he called a \"heterarchy\" of motives, meaning that the motives are not linearly ordered, but can be ordered like \n\n\n\nA\n>\nB\n>\nC\n>\nA\n\n\n{\\displaystyle A>B>C>A}\n\n.[25] He posited the concept of \"poker chip\" reticular formations as to how the brain deals with contradictory information in a democratic, somatotopical neural network. Specifically, how the brain can commit the animal to a single course of action when the situation is ambiguous.\n They designed a prototypic example neural network \"RETIC\", with \"12 anastomatically coupled modules stacked in columnar array\", which can switch between unambiguous stable modes based on ambiguous inputs.[26][16]\n His principle of  \"Redundancy of Potential Command\"[26] was developed by von Foerster and Pask in their study of self-organization[27] and by Pask in his Conversation Theory and Interactions of Actors Theory.[28]\n McCulloch wrote a book and several articles:[29]\n Articles, a selection:\n Papers published by the Chicago Literary Club:\n",
      "timestamp": "2025-10-09 19:12:42.030685"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch",
      "title": "Warren Sturgis McCulloch - Wikipedia",
      "description": "",
      "text": "Warren Sturgis McCulloch (November 16, 1898 \u2013 September 24, 1969) was an American neurophysiologist and cybernetician known for his work on the foundation for certain brain theories and his contribution to the cybernetics movement.[1] Along with Walter Pitts, McCulloch created computational models based on mathematical algorithms called threshold logic which split the inquiry into two distinct approaches, one approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.[2]\n Warren Sturgis McCulloch was born in Orange, New Jersey, in 1898. His brother was a chemical engineer and Warren was originally planning to join the Christian ministry. As a teenager he was associated with the theologians Henry Sloane Coffin, Harry Emerson Fosdick, Herman Karl Wilhelm Kumm and Julian F. Hecker. He was also mentored by the Quaker Rufus Jones.[3] He attended Haverford College then studied philosophy and psychology at Yale University, where he received a Bachelor of Arts degree in 1921.  He continued to study psychology at Columbia and received a Master of Arts degree in 1923. Receiving his MD in 1927 from the Columbia University College of Physicians and Surgeons in New York, he undertook an internship at Bellevue Hospital, New York. Then he worked under Eilhard von Domarus at the Rockland State Hospital for the Insane.[4] He returned to academia in 1934.  He worked at the Laboratory for Neurophysiology at Yale University from 1934 to 1941.\n In 1941 he moved to Chicago and joined the Department of Psychiatry at the University of Illinois at Chicago, where he was a professor of psychiatry, as well as the director of the Illinois Neuropsychiatric Institute until 1951.[5] From 1952 he worked at the Massachusetts Institute of Technology in Cambridge, Massachusetts with Norbert Wiener. He was a founding member of the American Society for Cybernetics and its second president during 1967\u20131968.  He was a mentor to the British operations research pioneer Stafford Beer.\n McCulloch had a range of interests and talents. In addition to his scientific contributions he wrote poetry (sonnets), and he designed and engineered buildings and a dam at his farm in Old Lyme, Connecticut.\n McCulloch married Ruth Metzger, known as 'Rook', in 1924 and they had three children.[6] He died in Cambridge in 1969.\n He is remembered for his work with Joannes Gregorius Dusser de Barenne from Yale[7] and later with Walter Pitts from the University of Chicago. He provided the foundation for certain brain theories in a number of classic papers, including \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" (1943) and \"How We Know Universals:  The Perception of Auditory and Visual Forms\" (1947), both published in the Bulletin of Mathematical Biophysics. The former is \"widely credited with being a seminal contribution to neural network theory, the theory of automata, the theory of computation, and cybernetics\".[1]\n McCulloch was the chair of the set of Macy conferences dedicated to Cybernetics. These, greatly due to the diversity of the backgrounds of the participants McCulloch brought in, became the foundation for the field.\n In Wiener's Cybernetics (1948), he recounted an event in the spring of 1947, when McCulloch designed a machine to allow the blind to read, by converting printed letters to tones. He designed it so that the tone is invariant for the same letter viewed under different angles. Gerhardt von Bonin saw the design, and immediately asked, \" Is this a diagram of the fourth layer of the visual cortex of the brain?\".[8]:\u200a22,\u200a140\u200a\n In his last days in 1960s, he worked on loops, oscillations and triadic relations with Moreno-D\u00edaz; the reticular formation with Kilmer and dynamic models of memory with Da Fonseca.[9] His work in the 1960s was summarized in a 1968 paper.[10]\n He studied the excitation of the brain by strychnine neuronography, which was a method to map brain connections. Applying strychnine in one point of the brain causes excitations in different points of the brain.[11] Bailey, Bonin, and McCulloch conducted a series of studies in the 1940s that identified connections in the brains of macaque and chimpanzee that are consistent with modern understanding of VOF.[12][13]\n In 1919 he began to work mainly on mathematical logic, and by 1923 he attempted to make a logic of transitive verbs. His goal in psychology is to invent a \"psychon\" or \"least psychic event\" that are binary atomic events with necessary causes, such that they can be combined to create complex logical propositions concerning their antecedents. He noticed in 1929 that these may correspond to the all-or-nothing firings of neurons in the brain.[3]\n In the 1943 paper, they described how memories can be formed by a neural network with loops in it, or alterable synapses. These then encodes for sentences like \"There was some x such that x was a \u03c8\" or \n\n\n\n(\n\u2203\nx\n)\n(\n\u03c8\nx\n)\n\n\n{\\displaystyle (\\exists x)(\\psi x)}\n\n, and showed that looped neural networks can encode all first-order logic with equality and conversely, any looped neural networks is equivalent to a sentence in first-order logic with equality, thus showing that they are equivalent in logical expressiveness.[3]\n The 1943 paper describes neural networks operating over time, and logical universals -- \"there exists\" and \"for all\"\u2014for spatial objects, such as geometric figures, was further developed in their 1947 paper.[14]\n He worked with Manuel Blum in studying how a neural network can be \"logically stable\", that is, can implement a boolean function even if the activation thresholds of individual neurons are varied.[15]:\u200a64\u200a They were inspired by the problem of how the brain can perform the same functions, such as breathing, under influence of caffeine or alcohol, which shifts the activation threshold over the entire brain.[3]\n He worked on triadic relations, an extension of the calculus of relations to handle relations that relates 3 objects, such as \"A gives B to C\" or \"A perceives B to be C\". He was convinced that such a logic is necessary for understanding brain activity.[10][16]\n In the 1947 paper How we know universals, they studied the problem of recognizing objects despite changes in representation. For example, recognizing a square under different viewing angles and lighting conditions, or recognizing a phoneme under different loudness and tones. That is, recognizing objects invariant under the action of some symmetry group. This problem was partly inspired by a practical problem in designing a machine for the blind to read (recounted in Wiener's Cybernetics, see before).[17]\n The paper proposed two solutions. The first is in computing an invariant by averaging over the symmetry group. Let the symmetry group be \n\n\n\nG\n\n\n{\\displaystyle G}\n\n and the object to be recognized be \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. Let a neural network implement a function \n\n\n\nT\n\n\n{\\displaystyle T}\n\n. Then, the group-invariant representation would be \n\n\n\n\n\n1\n\n\n|\n\nG\n\n|\n\n\n\n\n\n\u2211\n\ng\n\u2208\nG\n\n\nT\n(\ng\nx\n)\n\n\n{\\displaystyle {\\frac {1}{|G|}}\\sum _{g\\in G}T(gx)}\n\n, the group-action average. The second solution is in a negative feedback circuit that drives a canonical representation. Consider the problem of recognizing whether an object is a square. The circuit moves the eye so that the \"center of gravity of brightness\" of the object is moved to the middle of the visual field. This then effectively converts each object into a canonical representation, which can then be compared with a representation in the brain.[18][19]\n In the 1943 paper McCulloch and Pitts attempted to demonstrate that a Turing machine program could be implemented in a finite network of formal neurons (in the event, the Turing Machine contains their model of the brain, but the converse is not true[20]), that the neuron was the base logic unit of the brain. In the 1947 paper they offered approaches to designing \"nervous nets\" to recognize visual inputs despite changes in orientation or size.\n From 1952 McCulloch worked at the Research Laboratory of Electronics at MIT, working primarily on neural network modelling. His team examined the visual system of the frog in consideration of McCulloch's 1947 paper, discovering that the eye provides the brain with information that is already, to a degree, organized and interpreted, instead of simply transmitting an image.\n With Roberto Moreno-D\u00edaz, he studied a formalized problem of memory. Given that neural networks can store memory by a pattern of oscillations in a circle, they studied the number of possible oscillation patterns that can be sustained by some neural network with \n\n\n\nN\n\n\n{\\displaystyle N}\n\n neurons. This came out to be \n\n\n\nK\n(\nN\n)\n=\n\n\n\n(\n\n\n\n2\n\nN\n\n\nk\n\n\n)\n\n\n\n\n\u2211\n\nk\n=\n1\n\n\n\n2\n\nN\n\n\n\u2212\n1\n\n\nk\n!\n\n\n{\\displaystyle K(N)={\\binom {2^{N}}{k}}\\sum _{k=1}^{2^{N}-1}k!}\n\n (Schnabel, 1966).[21] Also, they proved a universality theorem, in that for each \n\n\n\nN\n\n\n{\\displaystyle N}\n\n, there exists a neural network (possibly with more than \n\n\n\nN\n\n\n{\\displaystyle N}\n\n neurons) with \n\n\n\n\nlog\n\n2\n\n\n\u2061\nK\n(\nN\n)\n\n\n{\\displaystyle \\log _{2}K(N)}\n\n binary inputs, such that, for any oscillation pattern realizable by some neural network with \n\n\n\nN\n\n\n{\\displaystyle N}\n\n neurons, there exists a binary input for this universal network such that it exhibits the same pattern.[22][23][24]\n McCulloch considered the problem of contradictory information and motives, which he called a \"heterarchy\" of motives, meaning that the motives are not linearly ordered, but can be ordered like \n\n\n\nA\n>\nB\n>\nC\n>\nA\n\n\n{\\displaystyle A>B>C>A}\n\n.[25] He posited the concept of \"poker chip\" reticular formations as to how the brain deals with contradictory information in a democratic, somatotopical neural network. Specifically, how the brain can commit the animal to a single course of action when the situation is ambiguous.\n They designed a prototypic example neural network \"RETIC\", with \"12 anastomatically coupled modules stacked in columnar array\", which can switch between unambiguous stable modes based on ambiguous inputs.[26][16]\n His principle of  \"Redundancy of Potential Command\"[26] was developed by von Foerster and Pask in their study of self-organization[27] and by Pask in his Conversation Theory and Interactions of Actors Theory.[28]\n McCulloch wrote a book and several articles:[29]\n Articles, a selection:\n Papers published by the Chicago Literary Club:\n",
      "timestamp": "2025-10-09 19:12:42.033597"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Walter_Pitts",
      "text": "\n Walter Harry Pitts, Jr. (April 23, 1923\u00a0\u2013 May 14, 1969) was an American logician who worked in the field of computational neuroscience.[1]  He proposed landmark theoretical formulations of neural activity and generative processes that influenced diverse fields such as cognitive sciences and psychology, philosophy, neurosciences, computer science, artificial neural networks, cybernetics and artificial intelligence, together with what has come to be known as the generative sciences. He is best remembered for having written along with Warren Sturgis McCulloch, a seminal paper in scientific history, titled A Logical Calculus of Ideas Immanent in Nervous Activity (1943). This paper proposed the first mathematical model of a neural network. The unit of this model, a simple formalized neuron, is still the standard of reference in the field of neural networks. It is often called a McCulloch\u2013Pitts neuron.  Prior to that paper, he formalized his ideas regarding the fundamental steps to building a Turing machine in \"The Bulletin of Mathematical Biophysics\" in an essay titled \"Some observations on the simple neuron circuit\".\n Walter Pitts was born in Detroit, Michigan on April 23, 1923, the son of Walter and Marie (n\u00e9e Welsia). An autodidact, he taught himself logic and mathematics as a child and became a proficient reader in several languages, including Greek and Latin. He is widely remembered for having spent three days in a library, at the age of 12, reading Principia Mathematica and sent a letter to Bertrand Russell pointing out what he considered serious problems with the first half of the first volume. Russell was appreciative and invited him to study at Cambridge University at age 12. The offer was not taken up; however, Pitts did decide to become a logician. At age 15 he left home to study.\n Pitts probably continued to correspond with Bertrand Russell; and at the age of 15 he attended Russell's lectures at the University of Chicago.[1][2] He stayed there, without registering as a student. While there, in 1938 he met Jerome Lettvin, a pre-medical student, and the two became close friends.[3] Russell was a visiting professor at the University of Chicago in the fall of 1938, and he directed Pitts to study with the logician Rudolf Carnap.[3] Pitts met Carnap at Chicago by walking into his office during office hours, and presenting him with an annotated version of Carnap's recent book on logic, The Logical Syntax of Language.[4]  Since Pitts did not introduce himself, Carnap spent months searching for him, and, when he found him, he obtained for him a menial job at the university and had Pitts study with him. Pitts at the time was homeless and without income.[5] He mastered Carnap's abstract logic, then met with and was intrigued by the work of the Ukrainian mathematical physicist Nicolas Rashevsky, who was also at Chicago and was the founder of mathematical biophysics, remodeling biology on the structure of the physical sciences and mathematical logic.[6] Pitts also worked closely with the mathematician Alston Scott Householder, who was a member of Rashevsky's group.[7][8][9] During his studies under Carnap, Pitts was also a regular attendant at Nicolas Rashevsky\u2019s seminars in theoretical biology, which included Frank Offner, Herbert Landahl, Alston Householder, and the neuroanatomist Gerhardt von Bonin from the University of Illinois at Chicago. In 1940, Von Bonin introduced Lettvin to Warren McCulloch, who would become a professor of psychiatry at Illinois.\n In 1941 Warren McCulloch took a position as professor of psychiatry at the University of Illinois at Chicago, and in early 1942 he invited Pitts, who was still homeless, together with Lettvin to live with his family.[10] In the evenings, McCulloch and Pitts collaborated. Pitts was familiar with the work of Gottfried Leibniz on computing and they considered the question of whether the nervous system could be considered a kind of universal computing device as described by Leibniz. This led to their seminal neural networks paper \"A Logical Calculus of Ideas Immanent in Nervous Activity\". After five years of unofficial studies, the University of Chicago awarded Pitts an Associate of Arts (his only earned degree) for his work on the paper.[11]\n In 1943, Lettvin introduced Pitts to Norbert Wiener at the Massachusetts Institute of Technology. Their first meeting, where they discussed Wiener's proof of the ergodic theorem, went so well that Pitts moved to Greater Boston to work with Wiener. While Pitts was an unofficial student under the aegis of Wiener at MIT until their acrimonious parting in 1952, he formally enrolled as a graduate student in the physics department during the 1943\u20131944 academic year and in the electrical engineering department from 1956\u20131958.[11][12]\n In 1944, Pitts was hired by Kellex Corporation (later acquired in 1950 by Vitro Corporation) in New York City, part of the Atomic Energy Project.[13]\n From 1946, Pitts was a core member of the Macy conferences, whose principal purpose was to set the foundations for a general science of the workings of the human mind.\n In 1951, Wiener convinced Jerome Wiesner to hire some physiologists of the nervous system. A group was established with Pitts, Lettvin, McCulloch, and Pat Wall. Pitts wrote a large dissertation on the properties of neural nets connected in three dimensions. Lettvin described him as \"in no uncertain sense the genius of the group \u2026 when you asked him a question, you would get back a whole textbook.\"[14] Pitts never married.[1] Pitts was also described as an eccentric, refusing to allow his name to be made publicly available. He continued to refuse all offers of advanced degrees or positions of authority at MIT, in part as he would have to sign his name.\n In 1952, Wiener suddenly turned against McCulloch\u2014his wife, Margaret Wiener, hated McCulloch[15]\u2014and broke off relations with anyone connected to him, including Pitts.[15]\n Although he remained employed as a research associate in the Research Laboratory of Electronics at MIT \"as little more than a technicality\"[16] for the rest of his life, Pitts became increasingly socially isolated. In 1959, the paradigmatic \"What the Frog\u2019s Eye Tells the Frog\u2019s Brain\" (credited to Humberto Maturana, Lettvin, McCulloch and Pitts) conclusively demonstrated that \"analog processes in the eye were doing at least part of the interpretive work\" in image processing as opposed to \"the brain computing information digital neuron by digital neuron using the exacting implement of mathematical logic\", leading Pitts to burn his unpublished doctoral dissertation on probabilistic three-dimensional neural networks and years of unpublished research. He took little further interest in work, excepting only a collaboration with Lettvin and Robert Gesteland which produced a paper on olfaction in 1965.\n Pitts died in 1969 of bleeding esophageal varices, a condition usually associated with cirrhosis and alcoholism.[1][2][15]\n",
      "timestamp": "2025-10-09 19:12:42.400711"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Walter_Pitts",
      "title": "Walter Pitts - Wikipedia",
      "description": "",
      "text": "\n Walter Harry Pitts, Jr. (April 23, 1923\u00a0\u2013 May 14, 1969) was an American logician who worked in the field of computational neuroscience.[1]  He proposed landmark theoretical formulations of neural activity and generative processes that influenced diverse fields such as cognitive sciences and psychology, philosophy, neurosciences, computer science, artificial neural networks, cybernetics and artificial intelligence, together with what has come to be known as the generative sciences. He is best remembered for having written along with Warren Sturgis McCulloch, a seminal paper in scientific history, titled A Logical Calculus of Ideas Immanent in Nervous Activity (1943). This paper proposed the first mathematical model of a neural network. The unit of this model, a simple formalized neuron, is still the standard of reference in the field of neural networks. It is often called a McCulloch\u2013Pitts neuron.  Prior to that paper, he formalized his ideas regarding the fundamental steps to building a Turing machine in \"The Bulletin of Mathematical Biophysics\" in an essay titled \"Some observations on the simple neuron circuit\".\n Walter Pitts was born in Detroit, Michigan on April 23, 1923, the son of Walter and Marie (n\u00e9e Welsia). An autodidact, he taught himself logic and mathematics as a child and became a proficient reader in several languages, including Greek and Latin. He is widely remembered for having spent three days in a library, at the age of 12, reading Principia Mathematica and sent a letter to Bertrand Russell pointing out what he considered serious problems with the first half of the first volume. Russell was appreciative and invited him to study at Cambridge University at age 12. The offer was not taken up; however, Pitts did decide to become a logician. At age 15 he left home to study.\n Pitts probably continued to correspond with Bertrand Russell; and at the age of 15 he attended Russell's lectures at the University of Chicago.[1][2] He stayed there, without registering as a student. While there, in 1938 he met Jerome Lettvin, a pre-medical student, and the two became close friends.[3] Russell was a visiting professor at the University of Chicago in the fall of 1938, and he directed Pitts to study with the logician Rudolf Carnap.[3] Pitts met Carnap at Chicago by walking into his office during office hours, and presenting him with an annotated version of Carnap's recent book on logic, The Logical Syntax of Language.[4]  Since Pitts did not introduce himself, Carnap spent months searching for him, and, when he found him, he obtained for him a menial job at the university and had Pitts study with him. Pitts at the time was homeless and without income.[5] He mastered Carnap's abstract logic, then met with and was intrigued by the work of the Ukrainian mathematical physicist Nicolas Rashevsky, who was also at Chicago and was the founder of mathematical biophysics, remodeling biology on the structure of the physical sciences and mathematical logic.[6] Pitts also worked closely with the mathematician Alston Scott Householder, who was a member of Rashevsky's group.[7][8][9] During his studies under Carnap, Pitts was also a regular attendant at Nicolas Rashevsky\u2019s seminars in theoretical biology, which included Frank Offner, Herbert Landahl, Alston Householder, and the neuroanatomist Gerhardt von Bonin from the University of Illinois at Chicago. In 1940, Von Bonin introduced Lettvin to Warren McCulloch, who would become a professor of psychiatry at Illinois.\n In 1941 Warren McCulloch took a position as professor of psychiatry at the University of Illinois at Chicago, and in early 1942 he invited Pitts, who was still homeless, together with Lettvin to live with his family.[10] In the evenings, McCulloch and Pitts collaborated. Pitts was familiar with the work of Gottfried Leibniz on computing and they considered the question of whether the nervous system could be considered a kind of universal computing device as described by Leibniz. This led to their seminal neural networks paper \"A Logical Calculus of Ideas Immanent in Nervous Activity\". After five years of unofficial studies, the University of Chicago awarded Pitts an Associate of Arts (his only earned degree) for his work on the paper.[11]\n In 1943, Lettvin introduced Pitts to Norbert Wiener at the Massachusetts Institute of Technology. Their first meeting, where they discussed Wiener's proof of the ergodic theorem, went so well that Pitts moved to Greater Boston to work with Wiener. While Pitts was an unofficial student under the aegis of Wiener at MIT until their acrimonious parting in 1952, he formally enrolled as a graduate student in the physics department during the 1943\u20131944 academic year and in the electrical engineering department from 1956\u20131958.[11][12]\n In 1944, Pitts was hired by Kellex Corporation (later acquired in 1950 by Vitro Corporation) in New York City, part of the Atomic Energy Project.[13]\n From 1946, Pitts was a core member of the Macy conferences, whose principal purpose was to set the foundations for a general science of the workings of the human mind.\n In 1951, Wiener convinced Jerome Wiesner to hire some physiologists of the nervous system. A group was established with Pitts, Lettvin, McCulloch, and Pat Wall. Pitts wrote a large dissertation on the properties of neural nets connected in three dimensions. Lettvin described him as \"in no uncertain sense the genius of the group \u2026 when you asked him a question, you would get back a whole textbook.\"[14] Pitts never married.[1] Pitts was also described as an eccentric, refusing to allow his name to be made publicly available. He continued to refuse all offers of advanced degrees or positions of authority at MIT, in part as he would have to sign his name.\n In 1952, Wiener suddenly turned against McCulloch\u2014his wife, Margaret Wiener, hated McCulloch[15]\u2014and broke off relations with anyone connected to him, including Pitts.[15]\n Although he remained employed as a research associate in the Research Laboratory of Electronics at MIT \"as little more than a technicality\"[16] for the rest of his life, Pitts became increasingly socially isolated. In 1959, the paradigmatic \"What the Frog\u2019s Eye Tells the Frog\u2019s Brain\" (credited to Humberto Maturana, Lettvin, McCulloch and Pitts) conclusively demonstrated that \"analog processes in the eye were doing at least part of the interpretive work\" in image processing as opposed to \"the brain computing information digital neuron by digital neuron using the exacting implement of mathematical logic\", leading Pitts to burn his unpublished doctoral dissertation on probabilistic three-dimensional neural networks and years of unpublished research. He took little further interest in work, excepting only a collaboration with Lettvin and Robert Gesteland which produced a paper on olfaction in 1965.\n Pitts died in 1969 of bleeding esophageal varices, a condition usually associated with cirrhosis and alcoholism.[1][2][15]\n",
      "timestamp": "2025-10-09 19:12:42.403001"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Walter_Bradford_Cannon",
      "text": "\n Walter Bradford Cannon (October 19, 1871 \u2013 October 1, 1945) was an American physiologist, professor and chairman of the Department of Physiology at Harvard Medical School. He coined the term \"fight or flight response\", and developed the theory of homeostasis. He popularized his theories in his book The Wisdom of the Body,[1][2] first published in 1932.\n Cannon was born on October 19, 1871, in Prairie du Chien, Wisconsin, the son of Colbert Hanchett Cannon and his wife Wilma Denio.[3] His sister Ida Maud Cannon (1877-1960) became a noted hospital social worker at Massachusetts General Hospital.\n In his autobiography The Way of an Investigator, Cannon counts himself among the descendants of Jacques de Noyon, a French Canadian explorer and coureur des bois. His Calvinist family was intellectually active, including readings from James Martineau, John Fiske (philosopher), and James Freeman Clarke. Cannon's curiosity also led him to Thomas Henry Huxley, John Tyndall, George Henry Lewes, and William Kingdon Clifford.[4] A high school teacher, Mary Jeannette Newson, became his mentor. \"Miss May\" Newson motivated him and helped him take his academic skills into Harvard University in 1892.[5]\n Upon finishing his undergraduate studies in 1896, he entered Harvard Medical School. He started using X-rays to study the physiology of digestion while working with Henry P. Bowditch. In 1900 he received his medical degree.\n After graduation, Cannon was hired by William Townsend Porter at Harvard as an instructor in the Department of Physiology while continuing his digestion study.[6] Cannon was promoted to an assistant professor of physiology in 1902. He was a close friend of the physicist, G. W. Pierce, and together they founded the Wicht Club with other young instructors for social and professional purposes. In 1906, Cannon succeeded Bowditch as the Higginson Professor and chairman of the Department of Physiology at Harvard Medical School until 1942. From 1914 to 1916, Cannon was also President of the American Physiological Society.[7]\n He was married to Cornelia James Cannon, a best-selling author and feminist reformer. On July 19, 1901, during their honeymoon in Montana, they were the first people to reach the summit of the unclimbed southwest peak (2657 m or 8716\u00a0ft) of Goat Mountain, between Lake McDonald and Logan Pass. That area is now Glacier National Park. The peak was subsequently named, Mount Cannon, by the United States Geological Survey[8] The couple had five children; A son, Dr. Bradford Cannon, a military plastic surgeon and radiation researcher. The daughters were Wilma Cannon Fairbank (who was married to John K. Fairbank), Linda Cannon Burgess, Helen Cannon Bond, and Marian Cannon Schlesinger, a painter and author living in Cambridge, Massachusetts.\n His actions and his statements may infer his philosophy of life. Born into a Calvinistic family, he broke away from religious authoritarianism and became independent from his prior dogma. Later in life, he states that naturally occurring events are what makes for a useful end. He took on the role of a naturalist where believed that the body and mind are inseparable as an organismic unit. The explanations of his work should enable man to live more wisely, happily, and intelligently without the interjection of supernatural interference.[9]\n E. Digby Baltzell said that Dr. Cannon was once offered a job at the Mayo Clinic for twice his Harvard salary. Cannon declined, saying \"I don't need twice as much money. All I need is fifty cents for a haircut once a month, and fifty cents a day to get lunch.\"[10]\n Cannon was elected to the American Academy of Arts and Sciences in 1906, the American Philosophical Society in 1908, and the United States National Academy of Sciences in 1914.[11][12][13]\n Cannon supported animal experimentation and opposed the arguments of anti-vivisectionists. In 1911, he authored a booklet for the American Medical Association criticizing the arguments of anti-vivisectionists.[14]\n Walter Cannon died on October 1, 1945, in Franklin, New Hampshire.[15]\n Walter Cannon began his career in science as a Harvard undergraduate in the year 1892. Henry Pickering Bowditch, who had worked with Claude Bernard, directed the laboratory in physiology at Harvard. Here Cannon began his research: he used the newly discovered x-rays to study the mechanism of swallowing and the motility of the stomach. Withi his first experiments, he was able to watch the course of a button down a dog's esophagus.[16] He said in his autobiography, The Way of an Investigator, \"The whole purpose of my effort was to see the peristaltic waves to learn their effects. Only after some time did I note that the absence of activity was accompanied by signs of perturbation, and when serenity was restored the waves promptly reappeared.\"[17]\n He demonstrated deglutition in a goose at the APS meeting in December 1896 and published his first paper on this research in the first issue of the American Journal of Physiology in January 1898.[7]\n In 1945 Cannon summarized his career in physiology by describing his focus at different ages:[18]\n As per Cannon, adrenaline exerts several important effects on different body organs, all of which maintain homeostasis in fight-or-flight situations.[20] For example, in the skeletal muscle of the limbs, adrenaline relaxes blood vessels which increases local blood flow. Adrenaline constricts blood vessels in the skin and minimizes blood loss from physical trauma. Adrenaline also releases the key metabolic fuel, glucose, from the liver into the bloodstream. \nHowever, the fact that aggressive attack and fearful escape both involve adrenaline release into the bloodstream does not imply an equivalence of \u201cfight\u201d with \u201cflight\u201d from a physiological or biochemical point of view.\n Cannon proposed the existence and functional unity of the sympathoadrenal (or \u201csympathoadrenomedullary\u201d or \u201csympathico-adrenal\u201d) system. He theorized that the sympathetic nervous system and the adrenal gland work together as a unit to maintain homeostasis in emergencies.[22] To identify and quantify adrenaline release during stress, beginning in about 1919 Cannon exploited an ingenious experimental setup. He would surgically excise the nerves supplying the heart of a laboratory animal such as a dog or cat. Then he would subject the animal to a stressor and record the heart rate response. With the nerves to the heart removed, he could deduce that if the heart rate increased in response to the perturbation, then the increase in heart rate must have resulted from the actions of a hormone. Finally, he would compare the results of an animal with intact adrenal glands with those in an animal from which he had removed the adrenal glands. From the difference in the heart rate between the two animals, he could further infer that the hormone responsible for the increase in heart rate came from the adrenal glands. Moreover, the amount of increase in the heart rate provided a measure of the amount of hormone released. Cannon became so convinced that the sympathetic nervous system and adrenal gland functioned as a unit that in the 1930s that he formally proposed that the sympathetic nervous system uses the same chemical messenger\u2014adrenaline\u2014as does the adrenal gland.  Cannon\u2019s notion of a unitary sympathoadrenal system persists to this day. Researchers in the area have come to question the validity of the notion of a unitary sympathoadrenal system, although clinicians often continue to lump together the two components.\n Cannon wrote several books and articles.\n",
      "timestamp": "2025-10-09 19:12:42.809684"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Walter_Bradford_Cannon",
      "title": "Walter Bradford Cannon - Wikipedia",
      "description": "",
      "text": "\n Walter Bradford Cannon (October 19, 1871 \u2013 October 1, 1945) was an American physiologist, professor and chairman of the Department of Physiology at Harvard Medical School. He coined the term \"fight or flight response\", and developed the theory of homeostasis. He popularized his theories in his book The Wisdom of the Body,[1][2] first published in 1932.\n Cannon was born on October 19, 1871, in Prairie du Chien, Wisconsin, the son of Colbert Hanchett Cannon and his wife Wilma Denio.[3] His sister Ida Maud Cannon (1877-1960) became a noted hospital social worker at Massachusetts General Hospital.\n In his autobiography The Way of an Investigator, Cannon counts himself among the descendants of Jacques de Noyon, a French Canadian explorer and coureur des bois. His Calvinist family was intellectually active, including readings from James Martineau, John Fiske (philosopher), and James Freeman Clarke. Cannon's curiosity also led him to Thomas Henry Huxley, John Tyndall, George Henry Lewes, and William Kingdon Clifford.[4] A high school teacher, Mary Jeannette Newson, became his mentor. \"Miss May\" Newson motivated him and helped him take his academic skills into Harvard University in 1892.[5]\n Upon finishing his undergraduate studies in 1896, he entered Harvard Medical School. He started using X-rays to study the physiology of digestion while working with Henry P. Bowditch. In 1900 he received his medical degree.\n After graduation, Cannon was hired by William Townsend Porter at Harvard as an instructor in the Department of Physiology while continuing his digestion study.[6] Cannon was promoted to an assistant professor of physiology in 1902. He was a close friend of the physicist, G. W. Pierce, and together they founded the Wicht Club with other young instructors for social and professional purposes. In 1906, Cannon succeeded Bowditch as the Higginson Professor and chairman of the Department of Physiology at Harvard Medical School until 1942. From 1914 to 1916, Cannon was also President of the American Physiological Society.[7]\n He was married to Cornelia James Cannon, a best-selling author and feminist reformer. On July 19, 1901, during their honeymoon in Montana, they were the first people to reach the summit of the unclimbed southwest peak (2657 m or 8716\u00a0ft) of Goat Mountain, between Lake McDonald and Logan Pass. That area is now Glacier National Park. The peak was subsequently named, Mount Cannon, by the United States Geological Survey[8] The couple had five children; A son, Dr. Bradford Cannon, a military plastic surgeon and radiation researcher. The daughters were Wilma Cannon Fairbank (who was married to John K. Fairbank), Linda Cannon Burgess, Helen Cannon Bond, and Marian Cannon Schlesinger, a painter and author living in Cambridge, Massachusetts.\n His actions and his statements may infer his philosophy of life. Born into a Calvinistic family, he broke away from religious authoritarianism and became independent from his prior dogma. Later in life, he states that naturally occurring events are what makes for a useful end. He took on the role of a naturalist where believed that the body and mind are inseparable as an organismic unit. The explanations of his work should enable man to live more wisely, happily, and intelligently without the interjection of supernatural interference.[9]\n E. Digby Baltzell said that Dr. Cannon was once offered a job at the Mayo Clinic for twice his Harvard salary. Cannon declined, saying \"I don't need twice as much money. All I need is fifty cents for a haircut once a month, and fifty cents a day to get lunch.\"[10]\n Cannon was elected to the American Academy of Arts and Sciences in 1906, the American Philosophical Society in 1908, and the United States National Academy of Sciences in 1914.[11][12][13]\n Cannon supported animal experimentation and opposed the arguments of anti-vivisectionists. In 1911, he authored a booklet for the American Medical Association criticizing the arguments of anti-vivisectionists.[14]\n Walter Cannon died on October 1, 1945, in Franklin, New Hampshire.[15]\n Walter Cannon began his career in science as a Harvard undergraduate in the year 1892. Henry Pickering Bowditch, who had worked with Claude Bernard, directed the laboratory in physiology at Harvard. Here Cannon began his research: he used the newly discovered x-rays to study the mechanism of swallowing and the motility of the stomach. Withi his first experiments, he was able to watch the course of a button down a dog's esophagus.[16] He said in his autobiography, The Way of an Investigator, \"The whole purpose of my effort was to see the peristaltic waves to learn their effects. Only after some time did I note that the absence of activity was accompanied by signs of perturbation, and when serenity was restored the waves promptly reappeared.\"[17]\n He demonstrated deglutition in a goose at the APS meeting in December 1896 and published his first paper on this research in the first issue of the American Journal of Physiology in January 1898.[7]\n In 1945 Cannon summarized his career in physiology by describing his focus at different ages:[18]\n As per Cannon, adrenaline exerts several important effects on different body organs, all of which maintain homeostasis in fight-or-flight situations.[20] For example, in the skeletal muscle of the limbs, adrenaline relaxes blood vessels which increases local blood flow. Adrenaline constricts blood vessels in the skin and minimizes blood loss from physical trauma. Adrenaline also releases the key metabolic fuel, glucose, from the liver into the bloodstream. \nHowever, the fact that aggressive attack and fearful escape both involve adrenaline release into the bloodstream does not imply an equivalence of \u201cfight\u201d with \u201cflight\u201d from a physiological or biochemical point of view.\n Cannon proposed the existence and functional unity of the sympathoadrenal (or \u201csympathoadrenomedullary\u201d or \u201csympathico-adrenal\u201d) system. He theorized that the sympathetic nervous system and the adrenal gland work together as a unit to maintain homeostasis in emergencies.[22] To identify and quantify adrenaline release during stress, beginning in about 1919 Cannon exploited an ingenious experimental setup. He would surgically excise the nerves supplying the heart of a laboratory animal such as a dog or cat. Then he would subject the animal to a stressor and record the heart rate response. With the nerves to the heart removed, he could deduce that if the heart rate increased in response to the perturbation, then the increase in heart rate must have resulted from the actions of a hormone. Finally, he would compare the results of an animal with intact adrenal glands with those in an animal from which he had removed the adrenal glands. From the difference in the heart rate between the two animals, he could further infer that the hormone responsible for the increase in heart rate came from the adrenal glands. Moreover, the amount of increase in the heart rate provided a measure of the amount of hormone released. Cannon became so convinced that the sympathetic nervous system and adrenal gland functioned as a unit that in the 1930s that he formally proposed that the sympathetic nervous system uses the same chemical messenger\u2014adrenaline\u2014as does the adrenal gland.  Cannon\u2019s notion of a unitary sympathoadrenal system persists to this day. Researchers in the area have come to question the validity of the notion of a unitary sympathoadrenal system, although clinicians often continue to lump together the two components.\n Cannon wrote several books and articles.\n",
      "timestamp": "2025-10-09 19:12:42.812640"
    },
    {
      "url": "https://en.wikipedia.org/wiki/W._Ross_Ashby",
      "text": "\n William Ross Ashby (6 September 1903 \u2013 15 November 1972) was an English psychiatrist and a pioneer in cybernetics, the study of the science of communications and automatic control systems in both machines and living things. His first name was not used: he was known as Ross Ashby.[1]:\u200a91\u200a\n His two books, Design for a Brain and An Introduction to Cybernetics, introduced exact and logical thinking into the brand new discipline of cybernetics and were highly influential.[1]:\u200a93\u200a These \"missionary works\" along with his technical contributions made Ashby \"the major theoretician of cybernetics after Wiener\".[2][3]:\u200a28\u200a\n William Ross Ashby was born in 1903 in London, where his father was working at an advertising agency.[4] From  1921 he studied at Sidney Sussex College, Cambridge, where he received his B.A. in 1924 and his M.B. and B.Ch. in 1928. From 1924 to 1928 he worked at St. Bartholomew's Hospital in London.  Later on he also received a Diploma in Psychological Medicine in 1931, and an M.A. 1930 and M.D. from Cambridge in 1935.[1]:\u200a91\u200a\n Ashby started working in 1930 as a Clinical Psychiatrist at the London County Council. From 1936 until 1947 he was a Research Pathologist at St Andrew's Hospital in Northampton in England. From 1945 to 1947 he served in India where he was a Major in the Royal Army Medical Corps.[1]:\u200a92\u200a\n When he returned to England, he served as Director of Research of the Barnwood House Hospital in Gloucester from 1947 until 1959. For a year, he was Director of the Burden Neurological Institute in Bristol. In 1960, he went to the United States and became Professor, Depts. of Biophysics and Electrical Engineering, University of Illinois at Urbana\u2013Champaign, until his retirement in 1970.[5]\n Ashby was president of the Society for General Systems Research from 1962 to 1964. After retiring in August 1970, he became an Honorary Professorial Fellow at the University of Wales in 1970 and a fellow of the Royal College of Psychiatrists in 1971. In June 1972 he was diagnosed with an inoperable brain tumor, and he died on 15 November.[4]\n Despite being widely influential within cybernetics, systems theory and, more recently, complex systems, Ashby is not as well known as many of the notable scientists his work influenced, including Herbert A. Simon, Norbert Wiener, Ludwig von Bertalanffy, Stafford Beer, Stanley Milgram, and Stuart Kauffman.[6]\n \nAshby kept a journal for over 44 years in which he recorded his ideas about new theories. He started May 1928, when he was medical student at St. Bartholomew's Hospital in London. Over the years, he wrote down a series of 25 volumes totaling 7,189 pages. In 2003, these journals were given to The British Library, London, and in 2008, they were made available online as The W. Ross Ashby Digital Archive.[7] Ashby initially considered his theorizing a private hobby, and his later decision to begin publishing his work caused him some distress. He wrote:  My fear is now that I may become conspicuous, for a book of mine is in the press. For this sort of success I have no liking. My ambitions are vague\u2014someday to produce something faultless.[1]:\u200a97\u200a\n Ashby found writing so difficult that he took correspondence courses in \"Effective English and Personal Efficiency\" to prepare to write his first book.[4]\n Ashby was interested in mechanistic explanations for adaptive behavior, especially in the brain. By 1941, he had developed a coherent theory and written a 197-page booklet, titled \"The Origin of Adaptation\".[1]:\u200a99\u200a This hand-written monograph was made publicly available in January 2021.[8] In it, he expressed his opinion that \"there is an abstract science of organisation, in the sense that there are laws, theories and discoveries to be made about organisation as such without asking what it is that is organised.\"[8]:\u200a35\u200a\n In 1948 Ashby built a machine, the homeostat, to demonstrate his theories.[1]:\u200a98\u200a The machine used a simple mechanical process to return to equilibrium states after disturbances at its input. Earlier, in 1946, Alan Turing had written a letter[9] to Ashby suggesting that Ashby use Turing's Automatic Computing Engine (ACE) for his experiments instead of building a special machine. Norbert Wiener, describing the appearance of purposeful behavior in the Homeostat's random search for equilibrium, called it \"one of the great philosophical contributions of the present day\".[10] Ashby's first book, Design for a Brain, was published in 1952 and recapitulated this line of research.\n Ashby was one of the original members of the Ratio Club, a small informal dining club of young psychologists, physiologists, mathematicians and engineers who met to discuss issues in cybernetics. The club was founded in 1949 by the neurologist John Bates and continued to meet until 1958.\n The title of his book An Introduction to Cybernetics popularised the usage of the term 'cybernetics' to refer to self-regulating systems, originally coined by Norbert Wiener in Cybernetics. The book gave accounts of homeostasis, adaptation, memory and foresight in living organisms in Ashby's determinist, mechanist terms.[2]\n Ashby's 1964 paper Constraint Analysis of Many-Dimensional Relations began the study of reconstructability analysis, a multivariate systems modeling methodology based on set theory and information theory, which would later be developed by Klaus Krippendorff, George Klir, and others.[11][3]:\u200a287\u2013288\u200a\n In 1970, Ashby collaborated on simulation experiments regarding the stability of large interconnected systems.[12] This work inspired Robert May's studies of stability and complexity in model ecosystems.[13]\n In An Introduction to Cybernetics, Ashby used set cardinality, or variety, as a measure of information. With this he formulated his Law of Requisite Variety. Mathematically, the law is a statement about how \"in a two-person game the variety possible is determined by the number of possible choices open to the two players\".[14] When regulation is seen as a game between a regulator \n\n\n\nR\n\n\n{\\displaystyle R}\n\n and source of disturbances \n\n\n\nD\n\n\n{\\displaystyle D}\n\n, \"only variety in \n\n\n\nR\n\n\n{\\displaystyle R}\n\n can force down the variety due to \n\n\n\nD\n\n\n{\\displaystyle D}\n\n; only variety can destroy variety.\"[15]:\u200a207\u200a\n In work with Ashby, Conant augmented this with the \"Good Regulator theorem\" stating that \"every good regulator of a system must be a model of that system\".[16] Stafford Beer applied the law of variety to the practice of management, founding management cybernetics and developing the Viable System Model.[17]\n A popular paraphrasing of the law is \"only complexity absorbs complexity\". However, while a web search reveals many attributions to Ashby, it appears such attribution is in error. The phrase is not listed by the Cybernetics Society.[18]\n The Papers of William Ross Ashby are housed at the British Library. The papers can be accessed through the British Library catalogue.[19]\n From 4 to 6 March 2004, a W. Ross Ashby centenary conference was held at the University of Illinois at Urbana\u2013Champaign to mark the 100th anniversary of his birth. Presenters at the conference included Stuart Kauffman, Stephen Wolfram and George Klir.[20] In February 2009, a special issue of the International Journal of General Systems was specifically devoted to Ashby and his work, containing papers from leading scholars such as Klaus Krippendorff, Stuart Umpleby and Kevin Warwick.\n Ashby's work on the law of requisite variety has influenced scholars within the field of management studies.[17]\n",
      "timestamp": "2025-10-09 19:12:43.358397"
    },
    {
      "url": "https://en.wikipedia.org/wiki/W._Ross_Ashby",
      "title": "W. Ross Ashby - Wikipedia",
      "description": "",
      "text": "\n William Ross Ashby (6 September 1903 \u2013 15 November 1972) was an English psychiatrist and a pioneer in cybernetics, the study of the science of communications and automatic control systems in both machines and living things. His first name was not used: he was known as Ross Ashby.[1]:\u200a91\u200a\n His two books, Design for a Brain and An Introduction to Cybernetics, introduced exact and logical thinking into the brand new discipline of cybernetics and were highly influential.[1]:\u200a93\u200a These \"missionary works\" along with his technical contributions made Ashby \"the major theoretician of cybernetics after Wiener\".[2][3]:\u200a28\u200a\n William Ross Ashby was born in 1903 in London, where his father was working at an advertising agency.[4] From  1921 he studied at Sidney Sussex College, Cambridge, where he received his B.A. in 1924 and his M.B. and B.Ch. in 1928. From 1924 to 1928 he worked at St. Bartholomew's Hospital in London.  Later on he also received a Diploma in Psychological Medicine in 1931, and an M.A. 1930 and M.D. from Cambridge in 1935.[1]:\u200a91\u200a\n Ashby started working in 1930 as a Clinical Psychiatrist at the London County Council. From 1936 until 1947 he was a Research Pathologist at St Andrew's Hospital in Northampton in England. From 1945 to 1947 he served in India where he was a Major in the Royal Army Medical Corps.[1]:\u200a92\u200a\n When he returned to England, he served as Director of Research of the Barnwood House Hospital in Gloucester from 1947 until 1959. For a year, he was Director of the Burden Neurological Institute in Bristol. In 1960, he went to the United States and became Professor, Depts. of Biophysics and Electrical Engineering, University of Illinois at Urbana\u2013Champaign, until his retirement in 1970.[5]\n Ashby was president of the Society for General Systems Research from 1962 to 1964. After retiring in August 1970, he became an Honorary Professorial Fellow at the University of Wales in 1970 and a fellow of the Royal College of Psychiatrists in 1971. In June 1972 he was diagnosed with an inoperable brain tumor, and he died on 15 November.[4]\n Despite being widely influential within cybernetics, systems theory and, more recently, complex systems, Ashby is not as well known as many of the notable scientists his work influenced, including Herbert A. Simon, Norbert Wiener, Ludwig von Bertalanffy, Stafford Beer, Stanley Milgram, and Stuart Kauffman.[6]\n \nAshby kept a journal for over 44 years in which he recorded his ideas about new theories. He started May 1928, when he was medical student at St. Bartholomew's Hospital in London. Over the years, he wrote down a series of 25 volumes totaling 7,189 pages. In 2003, these journals were given to The British Library, London, and in 2008, they were made available online as The W. Ross Ashby Digital Archive.[7] Ashby initially considered his theorizing a private hobby, and his later decision to begin publishing his work caused him some distress. He wrote:  My fear is now that I may become conspicuous, for a book of mine is in the press. For this sort of success I have no liking. My ambitions are vague\u2014someday to produce something faultless.[1]:\u200a97\u200a\n Ashby found writing so difficult that he took correspondence courses in \"Effective English and Personal Efficiency\" to prepare to write his first book.[4]\n Ashby was interested in mechanistic explanations for adaptive behavior, especially in the brain. By 1941, he had developed a coherent theory and written a 197-page booklet, titled \"The Origin of Adaptation\".[1]:\u200a99\u200a This hand-written monograph was made publicly available in January 2021.[8] In it, he expressed his opinion that \"there is an abstract science of organisation, in the sense that there are laws, theories and discoveries to be made about organisation as such without asking what it is that is organised.\"[8]:\u200a35\u200a\n In 1948 Ashby built a machine, the homeostat, to demonstrate his theories.[1]:\u200a98\u200a The machine used a simple mechanical process to return to equilibrium states after disturbances at its input. Earlier, in 1946, Alan Turing had written a letter[9] to Ashby suggesting that Ashby use Turing's Automatic Computing Engine (ACE) for his experiments instead of building a special machine. Norbert Wiener, describing the appearance of purposeful behavior in the Homeostat's random search for equilibrium, called it \"one of the great philosophical contributions of the present day\".[10] Ashby's first book, Design for a Brain, was published in 1952 and recapitulated this line of research.\n Ashby was one of the original members of the Ratio Club, a small informal dining club of young psychologists, physiologists, mathematicians and engineers who met to discuss issues in cybernetics. The club was founded in 1949 by the neurologist John Bates and continued to meet until 1958.\n The title of his book An Introduction to Cybernetics popularised the usage of the term 'cybernetics' to refer to self-regulating systems, originally coined by Norbert Wiener in Cybernetics. The book gave accounts of homeostasis, adaptation, memory and foresight in living organisms in Ashby's determinist, mechanist terms.[2]\n Ashby's 1964 paper Constraint Analysis of Many-Dimensional Relations began the study of reconstructability analysis, a multivariate systems modeling methodology based on set theory and information theory, which would later be developed by Klaus Krippendorff, George Klir, and others.[11][3]:\u200a287\u2013288\u200a\n In 1970, Ashby collaborated on simulation experiments regarding the stability of large interconnected systems.[12] This work inspired Robert May's studies of stability and complexity in model ecosystems.[13]\n In An Introduction to Cybernetics, Ashby used set cardinality, or variety, as a measure of information. With this he formulated his Law of Requisite Variety. Mathematically, the law is a statement about how \"in a two-person game the variety possible is determined by the number of possible choices open to the two players\".[14] When regulation is seen as a game between a regulator \n\n\n\nR\n\n\n{\\displaystyle R}\n\n and source of disturbances \n\n\n\nD\n\n\n{\\displaystyle D}\n\n, \"only variety in \n\n\n\nR\n\n\n{\\displaystyle R}\n\n can force down the variety due to \n\n\n\nD\n\n\n{\\displaystyle D}\n\n; only variety can destroy variety.\"[15]:\u200a207\u200a\n In work with Ashby, Conant augmented this with the \"Good Regulator theorem\" stating that \"every good regulator of a system must be a model of that system\".[16] Stafford Beer applied the law of variety to the practice of management, founding management cybernetics and developing the Viable System Model.[17]\n A popular paraphrasing of the law is \"only complexity absorbs complexity\". However, while a web search reveals many attributions to Ashby, it appears such attribution is in error. The phrase is not listed by the Cybernetics Society.[18]\n The Papers of William Ross Ashby are housed at the British Library. The papers can be accessed through the British Library catalogue.[19]\n From 4 to 6 March 2004, a W. Ross Ashby centenary conference was held at the University of Illinois at Urbana\u2013Champaign to mark the 100th anniversary of his birth. Presenters at the conference included Stuart Kauffman, Stephen Wolfram and George Klir.[20] In February 2009, a special issue of the International Journal of General Systems was specifically devoted to Ashby and his work, containing papers from leading scholars such as Klaus Krippendorff, Stuart Umpleby and Kevin Warwick.\n Ashby's work on the law of requisite variety has influenced scholars within the field of management studies.[17]\n",
      "timestamp": "2025-10-09 19:12:43.361383"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Valentino_Braitenberg",
      "text": "Valentino Braitenberg (or Valentin von Braitenberg; 18 June 1926 \u2013 9 September 2011) was an Italian neuroscientist and cyberneticist. He was a former director at the Max Planck Institute for Biological Cybernetics in T\u00fcbingen, Germany.\n His book Vehicles: Experiments in Synthetic Psychology became famous in Robotics and among Psychologists, in which he described how hypothetical analog vehicles (a combination of sensors, actuators and their interconnections), though simple in design, can exhibit behaviors akin to aggression, love, foresight, and optimism.[1] These have come to be known as Braitenberg vehicles. His pioneering scientific work was concerned with the relationship between structures and functions of the brain.\n Valentino Braitenberg grew up in the province of South Tyrol. Braitenberg's father was Senator Carl von Braitenberg\u00a0[de],[2] a member of the South Tyrolean nobility.\n Since the age of 6, Braitenberg grew up bilingual in the two languages Italian and German. German was spoken at home and all schooling was Italian, conforming to the historic context. The humanistic Lyceum-Gymnasium (High school) in Bolzano gave him an excellent classic education including Italian literature. The German literary education was based on the classical writers he found in his extensive home library. In addition, he trained as a violinist at the Conservatorio Claudio Monteverdi\u00a0[it] in Bolzano and became a talented violinist and violist.\n Braitenberg studied Medicine and Psychiatry at the University of Innsbruck and the University of Rome between 1945 and 1954. He accompanied his studies with chamber music performances with his viola and violin, where he developed a repertoire of violin-piano duos with a colleague. He completed his medical training with an internship at the psychiatric clinic in Rome, where he decided to prefer a scientific career dedicated to the understanding of brain functions. He spent a few years at Yale University in New Haven (USA) when he was invited by Prof. Eduardo Caianiello in 1958 to set up a biocybernetics research group at the Physics Institute of the University of Naples Federico II, the \u201cLaboratorio di Cibernetica\u201d, as part of the National Research Council in Italy (CNR). Between 1958 and 1968 he was adjunct Professor of Cybernetics at the Physics Institute of the University of Naples. In 1963 Braitenberg earned the Libera docenza in Cybernetics and Information Theory, the title that used to grant access to Professorship at Italian Universities. From 1968 until his retirement in 1994 he was co-founder and co-director of the Max Planck Institute for Biological Cybernetics in T\u00fcbingen and Honorary Professor at the University of T\u00fcbingen and University of Freiburg. After 1994 he was appointed Professor at the Specialization School in Scienze Motorie (Motoric Sciences) at the Rovereto branch of the University of Trento. From 1998 to 2001 he was president of the Laboratorio di Scienze Cognitive at the University of Trento in Rovereto.\n Braitenberg received an honorary doctorate from the University of Salzburg in 1995.\n Braitenberg was married to the painter Elisabeth Hanna. They had three children, Margareta, Carla, and Zeno.\n According to Maier (2012),[3] Braitenberg's interest in understanding the brain began in 1948, when he looked for the first time at some human brain tissue under a microscope. He said that although the connections seemed unbelievably complex, Braitenberg eventually realised that computers could serve as a useful model for understanding the brain. She said that he made seminal contributions to understanding the neuroanatomy of the cerebellum, the wiring of the eye of the fly, and the organisation of the human cerebrum.\n Braitenberg published more than 180 scientific works during his lifetime, not including abstracts, reprints, translations into different languages, and different editions of some of his works.[4]\nAccording to a search of Google Scholar in September 2014, Braitenberg's book, Vehicles: Experiments in synthetic psychology, had received at least 2622 citations.\n Books published by Braitenberg include:\n",
      "timestamp": "2025-10-09 19:12:43.842447"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Valentino_Braitenberg",
      "title": "Valentino Braitenberg - Wikipedia",
      "description": "",
      "text": "Valentino Braitenberg (or Valentin von Braitenberg; 18 June 1926 \u2013 9 September 2011) was an Italian neuroscientist and cyberneticist. He was a former director at the Max Planck Institute for Biological Cybernetics in T\u00fcbingen, Germany.\n His book Vehicles: Experiments in Synthetic Psychology became famous in Robotics and among Psychologists, in which he described how hypothetical analog vehicles (a combination of sensors, actuators and their interconnections), though simple in design, can exhibit behaviors akin to aggression, love, foresight, and optimism.[1] These have come to be known as Braitenberg vehicles. His pioneering scientific work was concerned with the relationship between structures and functions of the brain.\n Valentino Braitenberg grew up in the province of South Tyrol. Braitenberg's father was Senator Carl von Braitenberg\u00a0[de],[2] a member of the South Tyrolean nobility.\n Since the age of 6, Braitenberg grew up bilingual in the two languages Italian and German. German was spoken at home and all schooling was Italian, conforming to the historic context. The humanistic Lyceum-Gymnasium (High school) in Bolzano gave him an excellent classic education including Italian literature. The German literary education was based on the classical writers he found in his extensive home library. In addition, he trained as a violinist at the Conservatorio Claudio Monteverdi\u00a0[it] in Bolzano and became a talented violinist and violist.\n Braitenberg studied Medicine and Psychiatry at the University of Innsbruck and the University of Rome between 1945 and 1954. He accompanied his studies with chamber music performances with his viola and violin, where he developed a repertoire of violin-piano duos with a colleague. He completed his medical training with an internship at the psychiatric clinic in Rome, where he decided to prefer a scientific career dedicated to the understanding of brain functions. He spent a few years at Yale University in New Haven (USA) when he was invited by Prof. Eduardo Caianiello in 1958 to set up a biocybernetics research group at the Physics Institute of the University of Naples Federico II, the \u201cLaboratorio di Cibernetica\u201d, as part of the National Research Council in Italy (CNR). Between 1958 and 1968 he was adjunct Professor of Cybernetics at the Physics Institute of the University of Naples. In 1963 Braitenberg earned the Libera docenza in Cybernetics and Information Theory, the title that used to grant access to Professorship at Italian Universities. From 1968 until his retirement in 1994 he was co-founder and co-director of the Max Planck Institute for Biological Cybernetics in T\u00fcbingen and Honorary Professor at the University of T\u00fcbingen and University of Freiburg. After 1994 he was appointed Professor at the Specialization School in Scienze Motorie (Motoric Sciences) at the Rovereto branch of the University of Trento. From 1998 to 2001 he was president of the Laboratorio di Scienze Cognitive at the University of Trento in Rovereto.\n Braitenberg received an honorary doctorate from the University of Salzburg in 1995.\n Braitenberg was married to the painter Elisabeth Hanna. They had three children, Margareta, Carla, and Zeno.\n According to Maier (2012),[3] Braitenberg's interest in understanding the brain began in 1948, when he looked for the first time at some human brain tissue under a microscope. He said that although the connections seemed unbelievably complex, Braitenberg eventually realised that computers could serve as a useful model for understanding the brain. She said that he made seminal contributions to understanding the neuroanatomy of the cerebellum, the wiring of the eye of the fly, and the organisation of the human cerebrum.\n Braitenberg published more than 180 scientific works during his lifetime, not including abstracts, reprints, translations into different languages, and different editions of some of his works.[4]\nAccording to a search of Google Scholar in September 2014, Braitenberg's book, Vehicles: Experiments in synthetic psychology, had received at least 2622 citations.\n Books published by Braitenberg include:\n",
      "timestamp": "2025-10-09 19:12:43.844397"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Valentin_Turchin",
      "text": "Valentin Fyodorovich Turchin (Russian: \u0412\u0430\u043b\u0435\u043d\u0442\u0438\u0301\u043d \u0424\u0451\u0434\u043e\u0440\u043e\u0432\u0438\u0447 \u0422\u0443\u0440\u0447\u0438\u0301\u043d, 14 February 1931 \u2013 7 April 2010) was a Soviet and American physicist, cybernetician, and computer scientist. He developed the Refal programming language, the theory of metasystem transitions and the notion of supercompilation. He was a pioneer in artificial intelligence and a proponent of the global brain hypothesis.\n Turchin was born in 1931 in Podolsk, Soviet Union. In 1952, he graduated from Moscow University with a degree in Theoretical Physics and got his Ph.D. in 1957. After working on neutron and solid-state physics at the Institute for Physics of Energy in Obninsk, in 1964 he accepted a position at the Keldysh Institute of Applied Mathematics in Moscow. There he worked on statistical regularization methods and authored REFAL, one of the first AI languages and the AI language of choice in the Soviet Union.\n In the 1960s, Turchin became politically active. In the Fall of 1968, he wrote the pamphlet The Inertia of Fear, which was quite widely circulated in samizdat, the writing began to be circulated under the title The Inertia of Fear: Socialism and Totalitarianism in Moscow in 1976.[2] Following its publication in the underground press, he lost his research laboratory.[3] In 1970 he authored \"The Phenomenon of Science\",[4] a grand cybernetic meta-theory of universal evolution, which broadened and deepened the earlier book. By 1973, Turchin had founded the Moscow chapter of Amnesty International with Andrey Tverdokhlebov and was working closely with the well-known physicist and Soviet dissident Andrei Sakharov. In 1974 he lost his position at the Institute and was persecuted by the KGB. Facing almost certain imprisonment, he and his family were forced to emigrate from the Soviet Union in 1977.\n He went to New York, where he joined the faculty of the City College of New York in 1979. In 1990, together with Cliff Joslyn and Francis Heylighen, he founded the Principia Cybernetica Project, a worldwide organization devoted to the collaborative development of an evolutionary-cybernetic philosophy. In 1998, he co-founded the software start-up SuperCompilers, LLC. He retired from his post as Professor of Computer Science at City College in 1999. A resident of Oakland, New Jersey,[5] he died there on 7 April 2010.[1]\n He has two sons named Peter Turchin (a specialist in population dynamics and the mathematical modeling of historical dynamics) and Dimitri Turchin.\n The philosophical core of Turchin's scientific work is the concept of the metasystem transition, which denotes the evolutionary process through which higher levels of control emerge in system structure and function.\n Turchin uses this concept to provide a global theory of evolution and a coherent social systems theory, to develop a complete cybernetics philosophical and ethical system, and to build a constructivist foundation for mathematics.\n Using the REFAL language he has implemented Supercompiler, a unified method for program transformation and optimization based on a metasystem transition.[6]\n Most cited publications according to Google Scholar\n",
      "timestamp": "2025-10-09 19:12:44.339642"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Valentin_Turchin",
      "title": "Valentin Turchin - Wikipedia",
      "description": "",
      "text": "Valentin Fyodorovich Turchin (Russian: \u0412\u0430\u043b\u0435\u043d\u0442\u0438\u0301\u043d \u0424\u0451\u0434\u043e\u0440\u043e\u0432\u0438\u0447 \u0422\u0443\u0440\u0447\u0438\u0301\u043d, 14 February 1931 \u2013 7 April 2010) was a Soviet and American physicist, cybernetician, and computer scientist. He developed the Refal programming language, the theory of metasystem transitions and the notion of supercompilation. He was a pioneer in artificial intelligence and a proponent of the global brain hypothesis.\n Turchin was born in 1931 in Podolsk, Soviet Union. In 1952, he graduated from Moscow University with a degree in Theoretical Physics and got his Ph.D. in 1957. After working on neutron and solid-state physics at the Institute for Physics of Energy in Obninsk, in 1964 he accepted a position at the Keldysh Institute of Applied Mathematics in Moscow. There he worked on statistical regularization methods and authored REFAL, one of the first AI languages and the AI language of choice in the Soviet Union.\n In the 1960s, Turchin became politically active. In the Fall of 1968, he wrote the pamphlet The Inertia of Fear, which was quite widely circulated in samizdat, the writing began to be circulated under the title The Inertia of Fear: Socialism and Totalitarianism in Moscow in 1976.[2] Following its publication in the underground press, he lost his research laboratory.[3] In 1970 he authored \"The Phenomenon of Science\",[4] a grand cybernetic meta-theory of universal evolution, which broadened and deepened the earlier book. By 1973, Turchin had founded the Moscow chapter of Amnesty International with Andrey Tverdokhlebov and was working closely with the well-known physicist and Soviet dissident Andrei Sakharov. In 1974 he lost his position at the Institute and was persecuted by the KGB. Facing almost certain imprisonment, he and his family were forced to emigrate from the Soviet Union in 1977.\n He went to New York, where he joined the faculty of the City College of New York in 1979. In 1990, together with Cliff Joslyn and Francis Heylighen, he founded the Principia Cybernetica Project, a worldwide organization devoted to the collaborative development of an evolutionary-cybernetic philosophy. In 1998, he co-founded the software start-up SuperCompilers, LLC. He retired from his post as Professor of Computer Science at City College in 1999. A resident of Oakland, New Jersey,[5] he died there on 7 April 2010.[1]\n He has two sons named Peter Turchin (a specialist in population dynamics and the mathematical modeling of historical dynamics) and Dimitri Turchin.\n The philosophical core of Turchin's scientific work is the concept of the metasystem transition, which denotes the evolutionary process through which higher levels of control emerge in system structure and function.\n Turchin uses this concept to provide a global theory of evolution and a coherent social systems theory, to develop a complete cybernetics philosophical and ethical system, and to build a constructivist foundation for mathematics.\n Using the REFAL language he has implemented Supercompiler, a unified method for program transformation and optimization based on a metasystem transition.[6]\n Most cited publications according to Google Scholar\n",
      "timestamp": "2025-10-09 19:12:44.341850"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Ulla_Mitzdorf",
      "text": "Ulla Mitzdorf (15 March 1944\u00a0\u2013 19 July 2013) was a German scientist. She contributed to diverse areas including physics, chemistry, psychology, physiology, medicine and gender studies.\n Mitzdorf gained her doctorate in theoretical chemistry in 1974 at the Technical University of Munich.[2] Subsequently, she worked as a scholar at the Max-Planck Institute of Psychiatry in Munich. In 1983 she habilitated in physiology,[3] and in 1984 in medical psychology and neurobiology at the Ludwig Maximilian University of Munich.\n From 1988 to 2009 she was the Fiebiger Professor for medical psychology at the Ludwig Maximilan University.[3] Simultaneously, from 2000 to 2006, she was women's affairs officer and spokeswoman for the state conference of women and gender equality officers in Bavarian universities.\n Ulla Mitzdorf significantly contributed to the understanding of local field potentials (LFPs) in the central nervous system. By implementing the technique of current source density (CSD)[4] she provided additional evidence for the theory that cortical LFPs result from the synaptic activity[4] in the brain.\n Mitzdorf died after a short illness on 19 July 2013, aged 69.[5]\n",
      "timestamp": "2025-10-09 19:12:44.828317"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Ulla_Mitzdorf",
      "title": "Ulla Mitzdorf - Wikipedia",
      "description": "",
      "text": "Ulla Mitzdorf (15 March 1944\u00a0\u2013 19 July 2013) was a German scientist. She contributed to diverse areas including physics, chemistry, psychology, physiology, medicine and gender studies.\n Mitzdorf gained her doctorate in theoretical chemistry in 1974 at the Technical University of Munich.[2] Subsequently, she worked as a scholar at the Max-Planck Institute of Psychiatry in Munich. In 1983 she habilitated in physiology,[3] and in 1984 in medical psychology and neurobiology at the Ludwig Maximilian University of Munich.\n From 1988 to 2009 she was the Fiebiger Professor for medical psychology at the Ludwig Maximilan University.[3] Simultaneously, from 2000 to 2006, she was women's affairs officer and spokeswoman for the state conference of women and gender equality officers in Bavarian universities.\n Ulla Mitzdorf significantly contributed to the understanding of local field potentials (LFPs) in the central nervous system. By implementing the technique of current source density (CSD)[4] she provided additional evidence for the theory that cortical LFPs result from the synaptic activity[4] in the brain.\n Mitzdorf died after a short illness on 19 July 2013, aged 69.[5]\n",
      "timestamp": "2025-10-09 19:12:44.829641"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Talcott_Parsons",
      "text": "\n Talcott Parsons (December 13, 1902 \u2013 May 8, 1979) was an American sociologist of the classical tradition, best known for his social action theory and structural functionalism. Parsons is considered one of the most influential figures in sociology in the 20th century.[17] After earning a PhD in economics, he served on the faculty at Harvard University from 1927 to 1973. In 1930, he was among the first professors in its new sociology department.[18] Later, he was instrumental in the establishment of the Department of Social Relations at Harvard.\n Based on empirical data, Parsons' social action theory was the first broad, systematic, and generalizable theory of social systems developed in the United States and Europe.[19] Some of Parsons' largest contributions to sociology in the English-speaking world were his translations of Max Weber's work and his analyses of works by Weber, \u00c9mile Durkheim, and Vilfredo Pareto. Their work heavily influenced Parsons' view and was the foundation for his social action theory. Parsons viewed voluntaristic action through the lens of the cultural values and social structures that constrain choices and ultimately determine all social actions, as opposed to actions that are determined based on internal psychological processes.[19] Although Parsons is generally considered a structural functionalist, towards the end of his career, in 1975, he published an article that stated that \"functional\" and \"structural functionalist\" were inappropriate ways to describe the character of his theory.[20]\n From the 1970s on, a new generation of sociologists criticized Parsons' theories as socially conservative and his writings as unnecessarily complex. Sociology courses have placed less emphasis on his theories than at the peak of his popularity (from the 1940s to the 1970s). However, there has been a recent resurgence of interest in his ideas.[18]\n Parsons was a strong advocate for the professionalization of sociology and its expansion in American academia. He was elected president of the American Sociological Association in 1949 and served as its secretary from 1960 to 1965.\n Parsons was born on December 13, 1902, in Colorado Springs, Colorado, to Edward Smith Parsons and Mary Augusta Ingersoll. His father had attended Yale Divinity School, was ordained as a Congregationalist minister, and served first as a minister for a pioneer community in Greeley, Colorado. At the time of Parsons' birth, his father was a professor in English and vice-president at Colorado College. During his Congregational ministry in Greeley, Edward had become sympathetic to the Social Gospel movement but tended to view it from a higher theological position and was hostile to the ideology of socialism.[21]\n As an undergraduate, Parsons studied biology and philosophy at Amherst College and received his BA in 1924. Amherst College had become the Parsons' family college by tradition; his father and his uncle Frank had attended it, as had his elder brother, Charles Edward. Initially, Parsons was attracted to a career in medicine, as he was inspired by his elder brother[22]:\u200a826\u200a so he studied a great deal of biology and spent a summer working at the Oceanographic Institution at Woods Hole, Massachusetts.\n Parsons' biology professors at Amherst were Otto C. Glaser and Henry Plough. Gently mocked as \"Little Talcott, the gilded cherub,\" Parsons became one of the student leaders at Amherst. Parsons also took courses with Walton Hale Hamilton and the philosopher Clarence Edwin Ayres, both known as \"institutional economists\". Hamilton, in particular, drew Parsons toward social science.[22]:\u200a826\u200a They exposed him to literature by authors such as Thorstein Veblen, John Dewey, and William Graham Sumner. Parsons also took a course with George Brown in the philosophy of Immanuel Kant and a course in modern German philosophy with Otto Manthey-Zorn, who was a great interpreter of Kant. Parsons showed from early on, a great interest in the topic of philosophy.\n Two term papers that Parsons wrote as a student for Clarence E. Ayres at Amherst have survived. They are referred to as the Amherst Papers and have been of strong interest to Parsons scholars. The first was written on December 19, 1922, \"The Theory of Human Behavior in its Individual and Social Aspects.\"[23] The second was written on March 27, 1923, \"A Behavioristic Conception of the Nature of Morals\".[24] The papers reveal Parsons' early interest in social evolution.[25] The Amherst Papers also reveal that Parsons did not agree with his professors since he wrote in his Amherst papers that technological development and moral progress are two structurally-independent empirical processes.\n After Amherst, he studied at the London School of Economics for a year, where he was exposed to the work of Bronis\u0142aw Malinowski, R. H. Tawney, L. T. Hobhouse, and Harold Laski.[22]:\u200a826\u200a During his days at LSE, he made friends with E. E. Evans-Pritchard, Meyer Fortes, and Raymond Firth, who all participated in the Malinowski seminar. Also, he made a close personal friendship with Arthur and Eveline M. Burns.\n At LSE he met Helen Bancroft Walker, a young American, and they married on April 30, 1927. The couple had three children: Anne, Charles, and Susan.\n In June, Parsons went on to the University of Heidelberg, where he received his PhD in sociology and economics in 1927. At Heidelberg, he worked with Alfred Weber, Max Weber's brother; Edgar Salin, his dissertation adviser; Emil Lederer; and Karl Mannheim. He was examined on Kant's Critique of Pure Reason by the philosopher Karl Jaspers.[26] At Heidelberg, Parsons was also examined by Willy Andreas on the French Revolution. Parsons wrote his Dr. Phil. thesis on The Concept of Capitalism in the Recent German Literature, with his main focus on the work of Werner Sombart and Weber. It was clear from his discussion that he rejected Sombart's quasi-idealistic views and supported Weber's attempt to strike a balance between historicism, idealism and neo-Kantianism.\n The most crucial encounter for Parsons at Heidelberg was with the work of Max Weber about whom he had never heard before. Weber became tremendously important for Parsons because his upbringing with a liberal but strongly-religious father had made the question of the role of culture and religion in the basic processes of world history a persistent puzzle in his mind. Weber was the first scholar who truly provided Parsons with a compelling theoretical \"answer\" to the question.\n Parsons decided to translate Weber's work into English and approached Marianne Weber, Weber's widow. Parsons would eventually translate several of Weber's works.[27][28] His time in Heidelberg had him invited by Marianne Weber to \"sociological teas\", which were study group meetings that she held in the library room of her and Max's old apartment. One scholar that Parsons met at Heidelberg who shared his enthusiasm for Weber was Alexander von Schelting. Parsons later wrote a review article on von Schelting's book on Weber.[29] Generally, Parsons read extensively in religious literature, especially works focusing on the sociology of religion. One scholar who became especially important for Parsons was Ernst D. Troeltsch. Parsons also read widely on Calvinism. His reading included the work of Emile Doumerque,[30] Eug\u00e9ne Choisy, and Henri Hauser.\n In 1927, after a year of teaching at Amherst (1926\u20131927), Parsons entered Harvard, as an instructor in the Economics Department,[31] where he followed F.\u00a0W. Taussig's lectures on economist Alfred Marshall and became friends with the economist historian Edwin Gay, the founder of Harvard Business School. Parsons also became a close associate of Joseph Schumpeter and followed his course General Economics. Parsons was at odds with some of the trends in Harvard's department which then went in a highly-technical and a mathematical direction. He looked for other options at Harvard and gave courses in \"Social Ethics\" and in the \"Sociology of Religion\".\n The chance for a shift to sociology came in 1930, when Harvard's Sociology Department was created[32] under Russian scholar Pitirim Sorokin. Parsons became one of the new department's two instructors, along with Carle Zimmerman.[33] Parsons established close ties with biochemist and sociologist Lawrence Joseph Henderson, who took a personal interest in Parsons' career at Harvard. Parsons became part of L.\u00a0J. Henderson's famous Pareto study group, in which some of the most important[citation needed] intellectuals at Harvard participated, including Crane Brinton, George C. Homans, and Charles P. Curtis. Parsons wrote an article on Pareto's theory[34] and later explained that he had adopted the concept of \"social system\" from reading Pareto. Parsons also made strong connections with two other influential intellectuals with whom he corresponded for years: economist Frank H. Knight and businessman Chester Barnard. The relationship between Parsons and Sorokin turned sour. A pattern of personal tensions was aggravated by Sorokin's deep dislike for American civilization, which he regarded as a sensate culture that was in decline. Sorokin's writings became increasingly anti-scientistic in his later years, widening the gulf between his work and Parsons' and turning the increasingly positivistic American sociology community against him. Sorokin also tended to belittle all sociology tendencies that differed from his own writings, and by 1934 was quite unpopular at Harvard.\n Some of Parsons' students in the department of sociology were Robin Williams Jr., Robert K. Merton, Kingsley Davis, Wilbert Moore, Edward C. Devereux, Logan Wilson, Nicholas Demereth, John Riley Jr., and Mathilda White Riley. Later cohorts of students included Harry Johnson, Bernard Barber, Marion Levy and Jesse R. Pitts. Parsons established, at the students' request, an informal study group which met year after year in Adams' house. Toward the end of Parsons' career, German systems theorist Niklas Luhmann also attended his lectures.\n In 1932, Parsons bought a farmhouse near the small town of Acworth, but Parsons often, in his writing, referred to it as \"the farmhouse in Alstead\". The farmhouse was a very humble structure with almost no modern utilities. Still, it became central to Parsons' life, and many of his most important works were written there.\n In the academic year of 1939\u20131940 Parsons and Schumpeter conducted an informal faculty seminar at Harvard, which discussed the concept of rationality. Among the participants were D.\u00a0V. McGranahan, Abram Bergson, Wassily Leontief, Gottfried Haberler, and Paul Sweezy. Schumpeter contributed the essay \"Rationality in Economics\", and Parsons submitted the paper \"The Role of Rationality in Social Action\" for a general discussion.[35]\n \nIn the discussion between neoclassical economics and the institutionalists, which was one of the conflicts that prevailed within the field of economics in the 1920s and early 1930s, Parsons attempted to walk a very fine line. He was very critical about neoclassical theory, an attitude he maintained throughout his life and that is reflected in his critique of Milton Friedman and Gary Becker. He was opposed to the utilitarian bias within the neoclassical approach and could not embrace them fully. However, he agreed partly on their theoretical and methodological style of approach, which should be distinguished from its substance. He was thus unable to accept the institutionalist solution. In a 1975 interview, Parsons recalled a conversation with Schumpeter on the institutionalist methodological position:  An economist like Schumpeter, by contrast, would absolutely have none of that. I remember talking to him about the problem and .. I think Schumpeter was right. If economics had gone that way [like the institutionalists] it would have had to become a primarily empirical discipline, largely descriptive, and without theoretical focus. That's the way the 'institutionalists' went, and of course Mitchell was affiliated with that movement.[36] Parsons returned to Germany in the summer of 1930 and became an eyewitness to the feverish atmosphere in Weimar Germany during which the Nazi Party rose to power. Parsons received constant reports about the rise of Nazism through his friend, Edward Y. Hartshorne, who was traveling there. Parsons began, in the late 1930s, to warn the American public about the Nazi threat, but he had little success, as a poll showed that 91 percent of the country opposed the Second World War.[37]\nOne of the first articles that Parsons wrote was \"New Dark Age Seen If Nazis Should Win\". He was one of the key initiators of the Harvard Defense Committee, aimed at rallying the American public against the Nazis. Parsons' voice sounded again and again over Boston's local radio stations, and he also spoke against Nazism during a dramatic meeting at Harvard, which was disturbed by antiwar activists. Together with graduate student Charles O. Porter, Parsons rallied graduate students at Harvard for the war effort. During the war, Parsons conducted a special study group at Harvard, which analyzed what its members considered the causes of Nazism, and leading experts on that topic participated.\n In the spring of 1941, a discussion group on Japan began to meet at Harvard. The group's five core members were Parsons, John K. Fairbank, Edwin O. Reischauer, William M. McGovern, and Marion Levy Jr. A few others occasionally joined the group, including Ai-Li Sung and Edward Y. Hartshorne. The group arose out of a strong desire to understand the country, but, as Levy frankly admitted, \"Reischauer was the only one who knew anything about Japan.\"[38] Parsons, however, was eager to learn more about it and was \"concerned with general implications.\"\n In 1942, Parsons worked on arranging a major study of occupied countries with Bartholomew Landheer of the Netherlands Information Office in New York.[39] Parsons had mobilized Georges Gurvitch, Conrad Arnsberg, Dr. Safranek and Theodore Abel to participate,[40] but it never materialized for lack of funding. In early 1942, Parsons unsuccessfully approached Hartshorne, who had joined the Psychology Division of the Office of the Coordinator of Information (COI) in Washington to interest his agency in the research project. In February 1943, Parsons became the deputy director of the Harvard School of Overseas Administration, which educated administrators to \"run\" the occupied territories in Germany and the Pacific Ocean. The task of finding relevant literature on both Europe and Asia was mindboggling and occupied a fair amount of Parsons' time. One scholar Parsons came to know was Karl August Wittfogel and they discussed Weber. On China, Parsons received fundamental information from Chinese scholar Ai-Li Sung Chin and her husband, Robert Chin. Another Chinese scholar Parsons worked closely with in this period was Hsiao-Tung Fei (or Fei Xiaotong), who had studied at the London School of Economics and was an expert on the social structure of the Chinese village.\n Parsons met Alfred Sch\u00fctz during the rationality seminar, which he conducted together with Schumpeter, at Harvard in the spring of 1940. Sch\u00fctz had been close to Edmund Husserl and was deeply embedded in the latter's phenomenological philosophy.[41] Sch\u00fctz was born in Vienna but moved to the US in 1939, and for years, he worked on the project of developing a phenomenological sociology, primarily based on an attempt to find some point between Husserl's method and Weber's sociology.[42] Parsons had asked Sch\u00fctz to give a presentation at the rationality seminar, which he did on April 13, 1940, and Parsons and Sch\u00fctz had lunch together afterward. Sch\u00fctz was fascinated with Parsons' theory, which he regarded as the state-of-the-art social theory, and wrote an evaluation of Parsons' theory that he kindly asked Parsons to comment. That led to a short but intensive correspondence, which generally revealed that the gap between Sch\u00fctz's sociologized phenomenology and Parsons' concept of voluntaristic action was far too great.[43] From Parsons' point of view, Sch\u00fctz's position was too speculative and subjectivist, and tended to reduce social processes to the articulation of a Lebenswelt consciousness. For Parsons, the defining edge of human life was action as a catalyst for historical change, and it was essential for sociology, as a science, to pay strong attention to the subjective element of action, but it should never become completely absorbed in it since the purpose of a science was to explain causal relationships, by covering laws or by other types of explanatory devices. Sch\u00fctz's basic argument was that sociology cannot ground itself and that epistemology was not a luxury but a necessity for the social scientist. Parsons agreed but stressed the pragmatic need to demarcate science and philosophy and insisted moreover that the grounding of a conceptual scheme for empirical theory construction cannot aim at absolute solutions but needs to take a sensible stock-taking of the epistemological balance at each point in time. However, the two men shared many basic assumptions about the nature of social theory, which has kept the debate simmering ever since.[44][45] By request from Ilse Sch\u00fctz, after her husband's death, Parsons gave permission to publish the correspondence between him and Sch\u00fctz. Parsons also wrote \"A 1974 Retrospective Perspective\" to the correspondence, which characterized his position as a \"Kantian point of view\" and found that Sch\u00fctz's strong dependence on Husserl's \"phenomenological reduction\" would make it very difficult to reach the kind of \"conceptual scheme\" that Parsons found essential for theory-building in social sciences.[46]\n Between 1940 and 1944, Parsons and Eric Voegelin exchanged intellectual views through correspondence.[47][48][49] Parsons had probably met Voegelin in 1938 and 1939, when Voegelin held a temporary instructor appointment at Harvard. The bouncing point for their conversation was Parsons' manuscript on anti-Semitism and other materials that he had sent to Voegelin. Discussion touched on the nature of capitalism, the rise of the West, and the origin of Nazism. The key to the discussion was the implication of Weber's interpretation of Protestant ethics and the impact of Calvinism on modern history. Although the two scholars agreed on many fundamental characteristics about Calvinism, their understanding of its historical impact was quite different. Generally, Voegelin regarded Calvinism as essentially a dangerous totalitarian ideology; Parsons argued that its current features were temporary and that the functional implications of its long-term, emerging value-l system had revolutionary and not only \"negative\" impact on the general rise of the institutions of modernity.\n The two scholars also discussed Parsons' debate with Sch\u00fctz and especially why Parsons had ended his encounter with Schutz. Parsons found that Schutz, rather than attempting to build social science theory, tended to get consumed in philosophical detours. Parsons wrote to Voegelin: \"Possibly one of my troubles in my discussion with Schuetz lies in the fact that by cultural heritage I am a Calvinist. I do not want to be a philosopher \u2013 I shy away from the philosophical problems underlying my scientific work. By the same token I don't think he wants to be a scientist as I understand the term until he has settled all the underlying philosophical difficulties. If the physicists of the 17th century had been Schuetzes there might well have been no Newtonian system.\"[50]\n In 1942, Stuart C. Dodd published a major work, Dimensions of Society,[51] which attempted to build a general theory of society on the foundation of a mathematical and quantitative systematization of social sciences. Dodd advanced a particular approach, known as an \"S-theory\". Parsons discussed Dodd's theoretical outline in a review article the same year.[52] Parsons acknowledged Dodd's contribution to be an exceedingly formidable work but argued against its premises as a general paradigm for the social sciences. Parsons generally argued that Dodd's \"S-theory\", which included the so-called \"social distance\" scheme of Bogardus, was unable to construct a sufficiently sensitive and systematized theoretical matrix, compared with the \"traditional\" approach, which has developed around the lines of Weber, Pareto, \u00c9mile Durkheim, Sigmund Freud, William Isaac Thomas, and other important agents of an action-system approach with a clearer dialogue with the cultural and motivational dimensions of human interaction.\n In April 1944, Parsons participated in a conference, \"On Germany after the War\", of psychoanalytical oriented psychiatrists and a few social scientists to analyze the causes of Nazism and to discuss the principles for the coming occupation.[53]\n During the conference, Parsons opposed what he found to be Lawrence S. Kubie's reductionism. Kubie was a psychoanalyst, who strongly argued that the German national character was completely \"destructive\" and that it would be necessary for a special agency of the United Nations to control the German educational system directly. Parsons and many others at the conference were strongly opposed to Kubie's idea. Parsons argued that it would fail and suggested that Kubie was viewing the question of Germans' reorientation \"too exclusively in psychiatric terms\". Parsons was also against the extremely harsh Morgenthau Plan, published in September 1944. After the conference, Parsons wrote an article, \"The Problem of Controlled Institutional Change\", against the plan.[54]\n Parsons participated as a part-time adviser to the Foreign Economic Administration Agency between March and October 1945 to discuss postwar reparations and deindustrialization.[55][56]\n Parsons was elected a Fellow of the American Academy of Arts and Sciences in 1945.[57]\n Parsons' situation at Harvard University changed significantly in early 1944, when he received a good offer from Northwestern University. Harvard reacted to the offer by appointing Parsons as the chairman of the department, promoting him to the rank of full professor and accepting the process of reorganization, which led to the establishment of the new department of Social Relations. Parsons' letter to Dean Paul Buck, on April 3, 1944, reveals the high point of this moment.[58] Because of the new development at Harvard, Parsons chose to decline an offer from William Langer to join the Office of Strategic Services, the predecessor of the Central Intelligence Agency. Langer proposed for Parsons to follow the American army in its march into Germany and to function as a political adviser to the administration of the occupied territories. Late in 1944, under the auspices of the Cambridge Community Council, Parsons directed a project together with Elizabeth Schlesinger. They investigated ethnic and racial tensions in the Boston area between students from Radcliffe College and Wellesley College. This study was a reaction to an upsurge of anti-Semitism in the Boston area, which began in late 1943 and continued into 1944.[59] At the end of November 1946, the Social Research Council (SSRC) asked Parsons to write a comprehensive report of the topic of how the social sciences could contribute to the understanding of the modern world. The background was a controversy over whether the social sciences should be incorporated into the National Science Foundation.\n Parsons' report was in form of a large memorandum, \"Social Science: A Basic National Resource\", which became publicly available in July 1948 and remains a powerful historical statement about how he saw the role of modern social sciences.[60]\n Parsons became a member of the executive committee of the new Russian Research Center at Harvard in 1948, which had Parsons' close friend and colleague, Clyde Kluckhohn, as its director. Parsons went to Allied-occupied Germany in the summer of 1948, was a contact person for the RRC, and was interested in the Russian refugees who were stranded in Germany. He happened to interview in Germany a few members of the Vlasov Army, a Russian Liberation Army that had collaborated with the Germans during the war.[61] The movement was named after Andrey Vlasov, a Soviet general captured by the Germans in June 1942. The Vlasov movement's ideology was a hybrid of elements and has been called \"communism without Stalin\", but in the Prague Manifesto (1944), it had moved toward the framework of a constitutional liberal state.[62]\n In Germany in the summer of 1948 Parsons wrote several letters to Kluckhohn to report on his investigations.\n Parsons' fight against communism was a natural extension of his fight against fascism in the 1930s and the 1940s. For Parsons, communism and fascism were two aspects of the same problem; his article \"A Tentative Outline of American Values\", published posthumously in 1989,[63] called both collectivistic types \"empirical finalism\", which he believed was a secular \"mirror\" of religious types of \"salvationalism\". In contrast, Parsons highlighted that American values generally were based on the principle of \"instrumental activism\", which he believed was the outcome of Puritanism as a historical process. It represented what Parsons called \"worldly asceticism\" and represented the absolute opposite of empirical finalism. One can thus understand Parsons' statement late in life that the greatest threat to humanity is every type of \"fundamentalism\".[64] By the term empirical finalism, he implied the type of claim assessed by cultural and ideological actors about the correct or \"final\" ends of particular patterns of value orientation in the actual historical world (such as the notion of \"a truly just society\"), which was absolutist and \"indisputable\" in its manner of declaration and in its function as a belief system. A typical example would be the Jacobins' behavior during the French Revolution. Parsons' rejection of communist and fascist totalitarianism was theoretically and intellectually an integral part of his theory of world history, and he tended to regard the European Reformation as the most crucial event in \"modern\" world history. Like Weber,[65] he tended to highlight the crucial impact of Calvinist religiosity in the socio-political and socio-economic processes that followed.[66] He maintained it reached its most radical form in England in the 17th century and in effect gave birth to the special cultural mode that has characterized the American value system and history ever since. The Calvinist faith system, authoritarian in the beginning, eventually released in its accidental long-term institutional effects a fundamental democratic revolution in the world.[67] Parsons maintained that the revolution was steadily unfolding, as part of an interpenetration of Puritan values in the world at large.[68]\n Parsons defended American exceptionalism and argued that, because of a variety of historical circumstances, the impact of the Reformation had reached a certain intensity in British history. Puritan, essentially Calvinist, value patterns had become institutionalized in Britain's internal situation. The outcome was that Puritan radicalism was reflected in the religious radicalism of the Puritan sects, in the poetry of John Milton, in the English Civil War, and in the process leading to the Glorious Revolution of 1688. It was the radical fling of the Puritan Revolution that provided settlers in early 17th-century Colonial America, and the Puritans who settled in America represented radical views on individuality, egalitarianism, skepticism toward state power, and the zeal of the religious calling. The settlers established something unique in the world that was under the religious zeal of Calvinist values.\n Therefore, a new kind of nation was born, the character of which became clear by the time of the American Revolution and in the US constitution,[69] and its dynamics were later studied by Alexis de Tocqueville.[70] The French Revolution was a failed attempt to copy the American model. Although America has changed in its social composition since 1787, Parsons maintained that it preserves the basic revolutionary Calvinist value pattern. That has been further revealed in the pluralist and highly individualized America, with its thick, network-oriented civil society, which is of crucial importance to its success and these factors have provided it with its historical lead in the process of industrialization.\n Parsons maintained that this has continued to place it in the leading position in the world, but as a historical process and not in \"the nature of things\". Parsons viewed the \"highly special feature of the modern Western social world\" as \"dependent on the peculiar circumstances of its history, and not the necessary universal result of social development as a whole\".[71]\n In contrast to some \"radicals\", Parsons was a defender of modernity.[72] He believed that modern civilization, with its technology and its constantly evolving institutions, was ultimately strong, vibrant, and essentially progressive. He acknowledged that the future had no inherent guarantees, but as sociologists Robert Holton and Bryan Turner said that Parsons was not nostalgic[73] and that he did not believe in the past as a lost \"golden age\" but that he maintained that modernity generally had improved conditions, admittedly often in troublesome and painful ways but usually positively. He had faith in humanity's potential but not na\u00efvely. When asked at the Brown Seminary in 1973 if he was optimistic about the future, he answered, \"Oh, I think I'm basically optimistic about the human prospects in the long run.\" Parsons pointed out that he had been a student at Heidelberg at the height of the vogue of Oswald Spengler, author of The Decline of the West, \"and he didn't give the West more than 50 years of continuing vitality after the time he wrote.... Well, its more than 50 years later now, and I don't think the West has just simply declined. He was wrong in thinking it was the end.\"[74]\n At Harvard, Parsons was instrumental in forming the Department of Social Relations, an interdisciplinary venture among sociology, anthropology, and psychology. The new department was officially created in January 1946 with him as the chairman and with prominent figures at the faculty, such as Stouffer, Kluckhohn, Henry Murray and Gordon Allport. An appointment for Hartshorne was considered but he was killed in Germany by an unknown gunman as he was driving on the highway. His position went instead to George C. Homans. The new department was galvanized by Parsons' idea of creating a theoretical and institutional base for a unified social science. Parsons also became strongly interested in systems theory and cybernetics and began to adopt their basic ideas and concepts to the realm of social science, giving special attention to the work of Norbert Wiener.\n Some of the students who arrived at the Department of Social Relations in the years after the Second World War were David Aberle, Gardner Lindzey, Harold Garfinkel, David G. Hays, Benton Johnson, Marian Johnson, Kaspar Naegele, James Olds, Albert Cohen, Norman Birnbaum, Robin Murphy Williams, Jackson Toby, Robert N. Bellah, Joseph Kahl, Joseph Berger, Morris Zelditch, Ren\u00e9e Fox, Tom O'Dea, Ezra Vogel, Clifford Geertz, Joseph Elder, Theodore Mills, Mark Field, Edward Laumann, and Francis Sutton.\n Ren\u00e9e Fox, who arrived at Harvard in 1949, would become a very close friend of the Parsons family. Joseph Berger, who also arrived at Harvard in 1949 after finishing his BA from Brooklyn College, would become Parsons' research assistant from 1952 to 1953 and would get involved in his research projects with Robert F. Bales.\n According to Parsons' own account, it was during his conversations with Elton Mayo that he realized it was necessary for him to take a serious look at the work of Freud. In the fall of 1938, Parsons began to offer a series of non-credit evening courses on Freud. As time passed, Parsons developed a strong interest in psychoanalysis. He volunteered to participate in nontherapeutic training at the Boston Psychoanalytic Institute, where he began a didactic analysis with Grete Bibring in September 1946. Insight into psychoanalysis is significantly reflected in his later work, especially reflected in The Social System and his general writing on psychological issues and on the theory of socialization. That influence was also to some extent apparent in his empirical analysis of fascism during the war. Wolfgang K\u00f6hler's study of the mentality of apes and Kurt Koffka's ideas of Gestalt psychology also received Parsons' attention.\n During the late 1940s and the early 1950s, he worked very hard on producing some major theoretical statements. In 1951, Parsons published two major theoretical works, The Social System[75] and Toward a General Theory of Action.[76] The latter work, which was coauthored with Edward Tolman, Edward Shils and several others, was the outcome of the so-called Carnegie Seminar at Harvard University, which had taken place in the period of September 1949 and January 1950.[77] The former work was Parsons' first major attempt to present his basic outline of a general theory of society since The Structure of Social Action (1937). He discusses the basic methodological and metatheoretical principles for such a theory. He attempts to present a general social system theory that is built systematically from most basic premises and so he featured the idea of an interaction situation based on need-dispositions and facilitated through the basic concepts of cognitive, cathectic, and evaluative orientation. The work also became known for introducing his famous pattern variables, which in reality represented choices distributed along a Gemeinschaft vs. Gesellschaft axis.\n The details of Parsons' thought about the outline of the social system went through a rapid series of changes in the following years, but the basics remained. During the early 1950s, the idea of the AGIL model took place in Parsons's mind gradually. According to Parsons, its key idea was sparked during his work with Bales on motivational processes in small groups.[78]\n Parsons carried the idea into the major work that he co-authored with a student, Neil Smelser, which was published in 1956 as Economy and Society.[79] \nWithin this work, the first rudimentary model of the AGIL scheme was presented. It reorganized the basic concepts of the pattern variables in a new way and presented the solution within a system-theoretical approach by using the idea of a cybernetic hierarchy as an organizing principle. The real innovation in the model was the concept of the \"latent function\" or the pattern maintenance function, which became the crucial key to the whole cybernetic hierarchy.\n During its theoretical development, Parsons showed a persistent interest in symbolism. An important statement is Parsons' \"The Theory of Symbolism in Relation to Action\".[80] The article was stimulated by a series of informal discussion group meetings, which Parsons and several other colleagues in the spring of 1951 had conducted with philosopher and semiotician Charles W. Morris.[81] His interest in symbolism went hand in hand with his interest in Freud's theory and \"The Superego and the Theory of Social Systems\", written in May 1951 for a meeting of the American Psychiatric Association. The paper can be regarded as the main statement of his own interpretation of Freud,[82] but also as a statement of how Parsons tried to use Freud's pattern of symbolization to structure the theory of social system and eventually to codify the cybernetic hierarchy of the AGIL system within the parameter of a system of symbolic differentiation. His discussion of Freud also contains several layers of criticism that reveal that Parsons' use of Freud was selective rather than orthodox. In particular, he claimed that Freud had \"introduced an unreal separation between the superego and the ego\".\n Parsons was an early subscriber to systems theory. He had early been fascinated by the writings of Walter B. Cannon and his concept of homeostasis[83] as well as the writings of French physiologist Claude Bernard.[84] His interest in systems theory had been further stimulated by his contract with L.J. Henderson. Parsons called the concept of \"system\" for an indispensable master concept in the work of building theoretical paradigms for social sciences.[85] From 1952 to 1957, Parsons participated in an ongoing Conference on System Theory under the chairmanship of Roy R. Grinker, Sr., in Chicago.\n Parsons came into contact with several prominent intellectuals of the time and was particularly impressed by the ideas of social insect biologist Alfred Emerson. Parsons was especially compelled by Emerson's idea that, in the sociocultural world, the functional equivalent of the gene was that of the \"symbol\". Parsons also participated in two of the meetings of the famous Macy Conferences on systems theory and on issues that are now classified as cognitive science, which took place in New York from 1946 to 1953 and included scientists like John von Neumann. Parsons read widely on systems theory at the time, especially works of Norbert Wiener[86] and William Ross Ashby,[87] who were also among the core participants in the conferences. Around the same time, Parsons also benefited from conversations with political scientist Karl Deutsch on systems theory. In one conference, the Fourth Conference of the problems of consciousness in March 1953 at Princeton and sponsored by the Macy Foundation, Parsons would give a presentation on \"Conscious and Symbolic Processes\" and embark on an intensive group discussion which included exchange with child psychologist Jean Piaget.[88]\n Among the other participants were Mary A.B. Brazier, Frieda Fromm-Reichmann, Nathaniel Kleitman, Margaret Mead and Gregory Zilboorg. Parsons would defend the thesis that consciousness is essentially a social action phenomenon, not primarily a \"biological\" one. During the conference, Parsons criticized Piaget for not sufficiently separating cultural factors from a physiologistic concept of \"energy\".\n During the McCarthy era, on April 1, 1952, J. Edgar Hoover, the director of the Federal Bureau of Investigation, received a personal letter from an informant who reported on communist activities at Harvard. During a later interview, the informant claimed that \"Parsons... was probably the leader of an inner group\" of communist sympathizers at Harvard. The informant reported that the old department under Sorokin had been conservative and had \"loyal Americans of good character\" but that the new Department of Social Relations had turned into a decisive left-wing place as a result of \"Parsons's manipulations and machinations\". On October 27, 1952, Hoover authorized the Boston FBI to initiate a security-type investigation on Parsons. In February 1954, a colleague, Stouffer, wrote to Parsons in England to inform him that Stouffer had been denied access to classified documents and that part of the stated reason was that Stouffer knew communists, including Parsons, \"who was a member of the Communist Party\".[89]\n Parsons immediately wrote an affidavit in defense of Stouffer, and he also defended himself against the charges that were in the affidavit: \"This allegation is so preposterous that I cannot understand how any reasonable person could come to the conclusion that I was a member of the Communist Party or ever had been.\"[90] In a personal letter to Stouffer, Parsons wrote, \"I will fight for you against this evil with everything there is in me: I am in it with you to the death.\" The charges against Parsons resulted in Parsons being unable to participate in a UNESCO conference, and it was not until January 1955 that he was acquitted of the charges.\n Since the late 1930s, Parsons had continued to show great interest in psychology and in psychoanalysis. In the academic year of 1955\u20131956, he taught a seminar at Boston Psychoanalytic Society and Institute entitled \"Sociology and Psychoanalysis\". In 1956, he published a major work, Family, Socialization and Interaction Process,[91] which explored the way in which psychology and psychoanalysis bounce into the theories of motivation and socialization, as well into the question of kinship, which for Parsons established the fundamental axis for that subsystem he later would call \"the social community\".\n It contained articles written by Parsons and articles written in collaboration with Robert F. Bales, James Olds, Morris Zelditch Jr., and Philip E. Slater. The work included a theory of personality as well as studies of role differentiation. The strongest intellectual stimulus that Parsons most likely got then was from brain researcher James Olds, one of the founders of neuroscience and whose 1955 book on learning and motivation was strongly influenced from his conversations with Parsons.[92] Some of the ideas in the book had been submitted by Parsons in an intellectual brainstorm in an informal \"work group\" which he had organized with Joseph Berger, William Caudill, Frank E. Jones, Kaspar D. Naegele, Theodore M. Mills, Bengt G. Rundblad, and others. Albert J. Reiss from Vanderbilt University had submitted his critical commentary.\n In the mid-1950s, Parsons also had extensive discussions with Olds about the motivational structure of psychosomatic problems, and at this time Parsons' concept of psychosomatic problems was strongly influenced by readings and direct conversations with Franz Alexander (a psychoanalyst, originally associated with the Berlin Psychoanalytic Institute, who was a pioneer of psychosomatic medicine), Grinker and John Spiegel.[93]\n In 1955, Fran\u00e7ois Bourricaud was preparing a reader of some of Parsons' work for a French audience, and Parsons wrote a preface for the book Au lecteur fran\u00e7ais (To the French Reader); it also went over Bourricaud's introduction very carefully. In his correspondence with Bourricaud, Parsons insisted that he did not necessarily treat values as the only, let alone \"the primary empirical reference point\" of the action system since so many other factors were also involved in the actual historical pattern of an action situation.[94]\n Parsons spent 1957 to 1958 at the Center of Advanced Study in the Behavioral Sciences in Palo Alto, California, where he met for the first time Kenneth Burke; Burke's flamboyant, explosive temperament made a great impression on Parsons, and the two men became close friends.[95] Parsons explained in a letter the impression Burke had left on him: \"The big thing to me is that Burke more than anyone else has helped me to fill a major gap in my own theoretical interests, in the field of the analysis of expressive symbolism.\"\n Another scholar whom Parsons met at the Center of Advanced Studies in the Behavioral Sciences at Palo Alto was Alfred L. Kroeber, the \"dean of American anthropologists\". Kroeber, who had received his PhD at Columbia and who had worked with the Arapaho Indians, was about 81 when Parsons met him. Parsons had the greatest admiration for Kroeber and called him \"my favorite elder statesman\".\n In Palo Alto, Kroeber suggested to Parsons that they write a joint statement to clarify the distinction between cultural and social systems, then the subject of endless debates. In October 1958, Parsons and Kroeber published their joint statement in a short article, \"The Concept of Culture and the Social System\", which became highly influential.[96] Parsons and Kroeber declared that it is important both to keep a clear distinction between the two concepts and to avoid a methodology by which either would be reduced to the other.\n In 1955 to 1956, a group of faculty members at Cornell University met regularly and discussed Parsons' writings. The next academic year, a series of seven widely attended public seminars followed and culminated in a session at which he answered his critics. The discussions in the seminars were summed up in a book edited by Max Black, The Social Theories of Talcott Parsons: A Critical Examination. It included an essay by Parsons, \"The Point of View of the Author\".[97] The scholars included in the volume were Edward C. Devereux Jr., Robin M. Williams Jr., Chandler Morse, Alfred L. Baldwin, Urie Bronfenbrenner, Henry A. Landsberger, William Foote Whyte, Black, and Andrew Hacker. The contributions converted many angles including personality theory, organizational theory, and various methodological discussions. Parsons' essay is particularly notable because it and another essay, \"Pattern Variables Revisited\",[98] both represented the most full-scale accounts of the basic elements of his theoretical strategy and the general principles behind his approach to theory-building when they were published in 1960.\n One essay also included, in metatheoretical terms, a criticism of the theoretical foundations for so-called conflict theory.\n From the late 1950s to the student rebellion in the 1960s and its aftermath, Parsons' theory was criticized by some scholars and intellectuals of the left, who claimed that Parsons's theory was inherently conservative, if not reactionary. Alvin Gouldner even claimed that Parsons had been an opponent of the New Deal. Parsons' theory was further regarded as unable to reflect social change, human suffering, poverty, deprivation, and conflict. Theda Skocpol thought that the apartheid system in South Africa was the ultimate proof that Parsons's theory was \"wrong\".[99]\n At the same time, Parsons' idea of the individual was seen as \"oversocialized\", \"repressive\", or subjugated in normative \"conformity\". In addition, J\u00fcrgen Habermas[100] and countless others were of the belief that Parsons' system theory and his action theory were inherently opposed and mutually hostile and that his system theory was especially \"mechanical\", \"positivistic\", \"anti-individualistic\", \"anti-voluntaristic\", and \"de-humanizing\" by the sheer nature of its intrinsic theoretical context.\n By the same token, his evolutionary theory was regarded as \"uni-linear\", \"mechanical\", \"biologistic\", an ode to world system status quo, or simply an ill-concealed instruction manual for \"the capitalist nation-state\". The first manifestations of that branch of criticism would be intellectuals like Lewis Coser,[101] Ralf Dahrendorf,[102] David Lockwood,[103] John Rex,[104] C. Wright Mills,[105] Tom Bottomore[106] and Gouldner.[107]\n Parsons supported John F. Kennedy on November 8, 1960; from 1923, with one exception, Parsons voted for Democrats all his life.[108] He discussed the Kennedy election widely in his correspondence at the time. Parsons was especially interested in the symbolic implications involved in the fact of Kennedy's Catholic background for the implications for the United States as an integral community (it was the first time that a Catholic had become President of the United States).\n In a letter to Robert N. Bellah, he wrote, \"I am sure you have been greatly intrigued by the involvement of the religious issue in our election.\"[109] Parsons, who described himself as a \"Stevenson Democrat\", was especially enthusiastic that his favored politician, Adlai Stevenson II, had been appointed United States Ambassador to the United Nations. Parsons had supported Stevenson in 1952 and 1956 and was greatly disappointed that Stevenson lost heavily both times.\n In the early 1960s, it became obvious that his ideas had a great impact on much of the theories of modernization at the time. His influence was very extensive but at the same time, the concrete adoption of his theory was often quite selective, half-hearted, superficial, and eventually confused. Many modernization theorists never used the full power of Parsons' theory but concentrated on some formalist formula, which often was taken out of the context that had the deeper meaning with which Parsons originally introduced them.\n In works by Gabriel A. Almond and James S. Coleman, Karl W. Deutsch, S.\u00a0N. Eisenstadt, Seymour Martin Lipset, Samuel P. Huntington, David E. Apter, Lucian W. Pye, Sidney Verba, and Chalmers Johnson, and others, Parsons' influence is clear. Indeed, it was the intensive influence of Parsons' ideas in political sociology that originally got scholar William Buxton interested in his work.[110] In addition, David Easton would claim that in the history of political science, the two scholars who had made any serious attempt to construct a general theory for political science on the issue of political support were Easton and Parsons.[111]\n One of the scholars with whom he corresponded extensively with during his lifetime and whose opinion he highly valued was Robert N. Bellah. Parsons's discussion with Bellah would cover a wide range of topics, including the theology of Paul Tillich.[112] The correspondence would continue when Bellah, in the early fall of 1960, went to Japan to study Japanese religion and ideology. In August 1960, Parsons sent Bellah a draft of his paper on \"The Religious Background of the American Value System\" to ask for his commentary.[113]\n In a letter to Bellah of September 30, 1960, Parsons discussed his reading of Perry Miller's Errand into the Wilderness.[114] Parsons wrote that Miller's discussion of the role of Calvinism \"in the early New England theology... is a first rate and fit beautifully with the broad position I have taken.\"[115] Miller was a literary Harvard historian whose books such as The New England Mind[116] established new standards for the writing of American cultural and religious history. Miller remained one of Parsons' most favoured historians throughout his life. Indeed, religion had always a special place in Parsons' heart, but his son, in an interview, maintained that he that his father was probably not really \"religious.\"\n Throughout his life, Parsons interacted with a broad range of intellectuals and others who took a deep interest in religious belief systems, doctrines, and institutions. One notable person who interacted with Parsons was Marie Augusta Neal, a nun of the Sisters of Notre Dame de Namur who sent Parsons a huge number of her manuscripts and invited him to conferences and intellectual events in her Catholic Church. Neal received her PhD from Harvard under Parsons's supervision in 1963, and she would eventually become professor and then chair of sociology at Emmanuel College.[117]\n Parsons and Winston White cowrote an article, \"The Link Between Character and Society\", which was published in 1961.[118] It was a critical discussion of David Riesman's The Lonely Crowd,[119] which had been published a decade earlier and had turned into an unexpected bestseller, reaching 1 million sold copies in 1977. Riesman was a prominent member of the American academic left, influenced by Erich Fromm and the Frankfurt School. In reality, Riesman's book was an academic attempt to give credit to the concept of \"mass society\" and especially to the idea of an America suffocated in social conformity. Riesman had essentially argued that at the emerging of highly advanced capitalism, the America basic value system and its socializing roles had changed from an \"inner-directed\" toward an \"other-directed\" pattern of value-orientation.\n Parsons and White challenged Riesman's idea and argued that there had been no change away from an inner-directed personality structure. The said that Riesman's \"other-directness\" looked like a caricature of Charles Cooley's looking-glass self,[120] and they argued that the framework of \"institutional individualism\" as the basic code-structure of America's normative system had essentially not changed. What had happened, however, was that the industrialized process and its increased pattern of societal differentiation had changed the family's generalized symbolic function in society and had allowed for a greater permissiveness in the way the child related to its parents. Parsons and White argued that was not the prelude to greater \"otherdirectness\" but a more complicated way by which inner-directed pattern situated itself in the social environment.\n 1963 was a notable year in Parsons's theoretical development because it was the year when he published two important articles: one on political power[121] and one on the concept of social influence.[122] The two articles represented Parsons's first published attempt to work out the idea of Generalized Symbolic Media as an integral part of the exchange processes within the AGIL system. It was a theoretical development, which Parsons had worked on ever since the publication of Economy and Society (1956).\n The prime model for the generalized symbolic media was money and Parsons was reflecting on the question whether the functional characteristics of money represented an exclusive uniqueness of the economic system or whether it was possible to identify other generalized symbolic media in other subsystems as well. Although each medium had unique characteristics, Parsons claimed that power (for the political system) and influence (for the societal community) had institutional functions, which essentially was structurally similar to the general systemic function of money. Using Roman Jakobson's idea of \"code\" and \"message\", Parsons divided the components of the media into a question of value-principle versus coordination standards for the \"code-structure\" and the question of factor versus product control within those social process which carried the \"message\" components. While \"utility\" could be regarded as the value-principle for the economy (medium: money), \"effectiveness\" was the value-principle for the political system (by political power) and social solidarity for the societal community (by social influence). Parsons would eventually choose the concept of value-commitment as the generalized symbolic medium for the fiduciary system with integrity as the value principle.[123]\n In August 1963, Parsons got a new research assistant, Victor Lidz, who would become an important collaborator and colleague. In 1964, Parsons flew to Heidelberg to celebrate the 100th birthday of Weber and discuss Weber's work with Habermas, Herbert Marcuse, and others.[124] Parsons delivered his paper \"Evaluation and Objectivity in Social Science: An Interpretation of Max Weber's Contribution\".[125] The meeting became mostly a clash between pro-Weberian scholars and the Frankfurt School. Before leaving for Germany, Parsons discussed the upcoming meeting with Reinhard Bendix and commented, \"I am afraid I will be something of a Daniel in the Lion's den.\"[126] Bendix wrote back and told Parsons that Marcuse sounded very much like Christoph Steding, a Nazi philosopher.[127]\n Parsons conducted a persistent correspondence with noted scholar Benjamin Nelson,[128] and they shared a common interest in the rise and the destiny of civilizations until Nelson's death in 1977. The two scholars also shared a common enthusiasm for the work of Weber and would generally agree on the main interpretative approach to the study of Weber. Nelson had participated in the Weber Centennial in Heidelberg.\n Parsons was opposed to the Vietnam War but was disturbed by what he considered the anti-intellectual tendency in the student rebellion: that serious debate was often substituted by handy slogans from communists Karl Marx, Mao Zedong and Fidel Castro.[citation needed]\n Nelson got into a violent argument with Herbert Marcuse and accused him of tarnishing Weber.[129] In reading the written version of Nelson's contribution to the Weber Centennial, Parsons wrote, \"I cannot let the occasion pass without a word of congratulations which is strong enough so that if it were concert I should shout bravo.\"[130] In several letters, Nelson would keep Parsons informed of the often-turbulent leftist environment of Marcuse.[131] In the letter of September 1967, Nelson would tell Parsons how much he enjoyed reading Parsons' essay on Kinship and The Associational Aspect of Social Structure.[132] Also, one of the scholars on whose work Parsons and Nelson would share internal commentaries was Habermas.\n Parsons had for years corresponded with his former graduate student David M. Schneider, who had taught at the University of California Berkeley until the latter, in 1960, accepted a position as professor in anthropology at the University of Chicago. Schneider had received his PhD at Harvard in social anthropology in 1949 and had become a leading expert on the American kinship system. Schneider, in 1968, published American Kinship: A Cultural Account[133] which became a classic in the field, and he had sent Parsons a copy of the copyedited manuscript before its publication. Parsons was highly appreciative of Schneider's work, which became in many ways a crucial turning point in his own attempt to understand the fundamental elements of the American kinship system, a key to understanding the factor of ethnicity and especially building the theoretical foundation of his concept of the societal community, which, by the beginning of the early 1970s, had become a strong priority in the number of theoretical projects of his own intellectual life.\n Parsons borrowed the term \"diffuse enduring solidarity\" from Schneider, as a major concept for his own considerations on the theoretical construction of the concept of the societal community. In the spring of 1968, Parsons and Schneider had discussed Clifford Geertz's article on religion as a cultural system[134] on which Parsons wrote a review.[135] Parsons, who was a close friend of Geertz, was puzzled over Geertz's article. In a letter to Schneider, Parsons spoke about \"the rather sharp strictures on what he [Geertz] calls the extremely narrow intellectual tradition with special reference to Weber, but also to Durkheim. My basic point is in this respect, he greatly overstated his case seeming to argue that this intellectual tradition was by now irrelevant.\"[136]\n Schneider wrote back to Parsons, \"So much, so often, as I read Cliff's stuff I cannot get a clear consistent picture of just what the religious system consist in instead only how it is said to work.\"[137]\n In a letter of July 1968 to Gene Tanke of the University of California Press, Parsons offered a critical note on the state of psychoanalytical theory and wrote: \"The use of psychoanalytical theory in interpretation of social and historical subject matter is somewhat hazardous enterprise, and a good deal of nonsense has been written in the name of such attempts.\"[138] Around 1969, Parsons was approached by the prestigious Encyclopedia of the History of Idea about writing an entry in the encyclopedia on the topic of the \"Sociology of Knowledge\". Parsons accepted and wrote one of his most powerful essays, \"The Sociology of Knowledge and the History of Ideas\",[139] in 1969 or 1970. Parsons discussed how the sociology of knowledge, as a modern intellectual discipline, had emerged from the dynamics of European intellectual history and had reached a kind of cutting point in the philosophy of Kant and further explored by Hegel but reached its first \"classical\" formulation in the writing of Mannheim,[140] whose brilliance Parsons acknowledged but disagreed with his German historicism for its antipositivistic epistemology; that was largely rejected in the more positivistic world of American social science. For various reasons, the editors of the encyclopedia turned down Parsons' essay, which did not fit the general format of their volume. The essay was not published until 2006.[141]\n Parsons had several conversations with Daniel Bell on a \"post-industrial society\", some of which were conducted over lunch at William James Hall. After reading an early version of Bell's magnum opus, The Coming of the Post-Industrial Society, Parsons wrote a letter to Bell, dated November 30, 1971, to offer his criticism. Among his many critical points, Parsons stressed especially that Bell's discussion of technology tended to \"separate off culture\" and treat the two categories \"as what I would call culture minus the cognitive component\".\n Parsons' interest in the role of ethnicity and religion in the genesis of social solidarity within the local community heavily influenced another of his early 1960s graduate students, Edward Laumann. As a student, Laumann was interested in the role of social network structure in shaping community-level solidarity. Combining Parsons' interest in the role of ethnicity in shaping local community solidarity with W. Lloyd Warner's structural approach to social class, Laumann argued that ethnicity, religion, and perceived social class all play a large role in structuring community social networks.[142][143][144] Laumann's work found that community networks are highly partitioned along lines of ethnicity, religion, and occupational social status. It also highlighted the tension individuals experience between their preference to associate with people who are like them (homophily) and their simultaneous desire to affiliate with higher-status others. Later, at the beginning of his career at the University of Chicago, Laumann would argue that how the impulses are resolved by individuals forms the basis of corporate or competitive class consciousness within a given community.[145] In addition to demonstrating how community solidarity can be conceptualized as a social network and the role of ethnicity, religion, and class in shaping such networks, Laumann's dissertation became one of the first examples of the use of population-based surveys in the collection of social network data, and thus a precursor to decades of egocentric social network analysis.[146] Parsons thus played an important role in shaping the early interest of social network analysis in homophily and the use of egocentric network data to assess group- and community-level social network structures.\n In his later years, Parsons became increasingly interested in working out the higher conceptual parameters of the human condition, which was in part what led him toward rethinking questions of cultural and social evolution and the \"nature\" of telic systems, the latter which he especially discussed with Bellah, Lidz, Fox, Willy de Craemer, and others. Parsons became increasingly interested in clarifying the relationship between biological and social theory. Parsons was the initiator of the first Daedalus conference on \"Some Relations between Biological and Social Theory\", sponsored by the American Academy of Arts and Sciences. Parsons wrote a memorandum dated September 16, 1971, in which he spelled out the intellectual framework for the conference. As Parsons explained in the memo, the basic goal of the conference was to establish a conceptual fundament for a theory of living systems. The first conference was held on January 7, 1972. Among the participants beside Parsons and Lidz were Ernst Mayr, Seymour Kety, Gerald Holton, A. Hunter Dupree, and William K. Wimsatt. A second Daedalus Conference on Living Systems was held on March 1\u20132, 1974 and included Edward O. Wilson, who was about to publish his famous work on sociobiology. Other new participants were John T. Bonner, Karl H. Pribram, Eric Lennenberg, and Stephen J. Gould.\n Parsons began in the fall of 1972 to conduct a seminar on \"Law and Sociology\" with legal philosopher Lon L. Fuller, well known for his book The Morality of Law (1964). The seminar and conversations with Fuller stimulated Parsons to write one of his most influential articles, \"Law as an Intellectual Stepchild\".[147] Parsons discuses Roberto Mangabeira Unger's Law in Modern Society (1976). Another indication of Parsons' interest in law was reflected in his students, such as John Akula, who wrote his dissertation in sociology, Law and the Development of Citizenship (1973). In September 1972, Parsons participated in a conference in Salzburg on \"The Social Consequences of Modernization in Socialist Countries\". Among the other participants were Alex Inkeles, Ezra Vogel, and Ralf Dahrendorf.\n In 1972, Parsons wrote two review articles to discuss the work of Bendix, which provide a clear statement on Parsons' approach to the study of Weber. Bendix had become well known for his interpretations of Weber. In the first review article, Parsons analyzed the immigrant Bendix's Embattled Reason,[148] and he praised its attempt to defend the basic values of cognitive rationality, which he unconditionally shared, and he agreed with Bendix that the question of cognitive rationality was primarily a cultural issue, not a category that could be reduced from biological, economic, and social factors. However, Parsons criticized how Bendix had proceeded, who he felt especially had misrepresented the work of Freud and Durkheim. Parsons found that the misrepresentation was how Bendix tended to conceive the question of systematic theorizing, under the concept of \"reductionism\".[149] Parsons further found that Bendix's approach suffered from a \"conspicuous hostility\" to the idea of evolution. Although Parsons assessed that Weber rejected the linear evolutionary approaches of Marx and Herbert Spencer, Weber might not have rejected the question of evolution as a generalized question.\n In a second article, a review of Bendix and Guenther Roth's Scholarship and Partisanship: Essays on Max Weber,[150] Parsons continued his line of criticism. Parsons was especially concerned with a statement by Bendix that claimed Weber believed Marx's notion that ideas were \"the epiphenomena of the organization of production\". Parsons strongly rejected that interpretation: \"I should contend that certainly the intellectual 'mature' Weber never was an 'hypothetical' Marxist.\"[151] Somewhere behind the attitudes of Bendix, Parsons detected a discomfort for the former to move out of an \"idiographic\" mode of theorizing.\n In 1973, Parsons published The American University, which he had authored with Gerald M. Platt.[152] The idea had originally emerged when Martin Meyerson and Stephen Graubard of the American Academy of Arts and Sciences, in 1969, asked Parsons to undertake a monographic study of the American university system. The work on the book went on for years until it was finished in June 1972.\n From a theoretical point of view, the book had several functions. It substantiated Parsons' concept of the educational revolution, a crucial component in his theory of the rise of the modern world. What was equally intellectually compelling, however, was Parsons' discussion of \"the cognitive complex\", aimed at explaining how cognitive rationality and learning operated as an interpenetrative zone on the level of the general action-system in society. In retrospect, the categories of the cognitive complex are a theoretical foundation to understand what has been called the modern knowledge-based society.\n He officially retired from Harvard in 1973 but continued his writing, teaching, and other activities in the same rapid pace as before. Parsons also continued his extensive correspondence with a wide group of colleagues and intellectuals. He taught at the University of Pennsylvania, Brown University, Rutgers University, the University of Chicago, and the University of California at Berkeley. At Parsons' retirement banquet, on May 18, 1973, Robert K. Merton was asked to preside, while John Riley, Bernard Barber, Jesse Pitts, Neil J. Smelser, and John Akula were asked to share their experiences of the man with the audience.\n One scholar who became important in Parsons' later years was professor Martin U. Martel, of Brown University. They had made contact in the early 1970s at a discussion of an article that Martel had written about Parsons' work.[153] Martel arranged a series of seminars at Brown University in 1973 to 1974, and Parsons spoke about his life and work and answered questions from students and faculty.[154] Among the participants at the seminars were Martel, Robert M. Marsh, Dietrich Rueschemeyer, C. Parker Wolf, Albert F. Wessen, A. Hunter Dupree, Philip L. Quinn, Adrian Hayes and Mark A. Shields. In February to May 1974, Parsons also gave the Culver Lectures at Brown and spoke on \"The Evolution of Society\". The lectures and were videotaped.\n Late in life, Parsons began to work out a new level of the AGIL model, which he called \"A Paradigm of the Human Condition\".[155] The new level of the AGIL model crystallized in the summer of 1974. He worked out the ideas of the new paradigm with a variety of people but especially Lidz, Fox and Harold Bershady. The new metaparadigm featured the environment of the general action system, which included the physical system, the biological system, and what Parsons called the telic system. The telic system represents the sphere of ultimate values in a sheer metaphysical sense. Parsons also worked toward a more comprehensive understanding of the code-structure of social systems[156] and on the logic of the cybernetic pattern of control facilitating the AGIL model. He wrote a bulk of notes: two being \"Thoughts on the Linking of Systems\" and \"Money and Time\".[157] He had also extensive discussions with Larry Brownstein and Adrian Hayes on the possibility of a mathematical formalization of Parsons' theory.[158]\n Parsons had worked intensively with questions of medical sociology, the medical profession, psychiatry, psychosomatic problems, and the questions of health and illness. Most of all Parsons had become known for his concept of \"the Sick role\". The last field of social research was an issue that Parsons constantly developed through elaboration and self-criticism. Parsons participated at the World Congress of Sociology in Toronto in August 1974 at which he presented a paper, \"The Sick Role Revisited: A Response to Critics and an Updating in Terms of the Theory of Action\", which was published under a slightly different title, \"The Sick Role and the Role of the Physician Reconsidered\", in 1975.[159] In this essay, Parsons highlighted that his concept of \"sick role\" never was meant to be confined to \"deviant behavior\", but \"its negative valuation should not be forgotten\". It was also important to keep a certain focus on the \"motivatedness\" of illness, since there is always a factor of unconscious motivation in the therapeutic aspects of the sick role.\n In 1975, Bellah published The Broken Covenant.[160] Bellah referred to the sermon delivered by John Winthrop (1587\u20131649) to his flock on the ship Arbella on the evening of the landing in Massachusetts Bay in 1630. Winthrop declared that the Puritan colonists' emigration to the New World was part of a covenant, a special pact with God, to create a holy community and noted: \"For we must consider that we shall be a city on the hill. The eyes of all people are upon us.\" Parsons disagreed strongly with Bellah's analysis and insisted that the covenant was not broken. Parsons later used much of his influential article, \"Law as an Intellectual Stepchild\",[161] to discuss Bellah's position.\n Parsons thought that Bellah trivialized the tensions of individual interests and society's interests by reducing them to \"capitalism\"; Bellah, in his characterization of the negative aspects of American society, was compelled by a charismatic-based optimalism moral absolutism.\n In 1975, Parsons responded to an article by Jonathan H. Turner, \"Parsons as a Symbolic Interactionist: A Comparison of Action and Interaction Theory\".[162] Parsons acknowledged that action theory and symbolic interactionism should not be regarded as two separate, antagonistic positions but have overlapping structures of conceptualization.[163] Parsons regarded symbolic interactionism and the theory of George Herbert Mead as valuable contributions to action theory that specify certain aspects of the theory of the personality of the individual. Parsons, however, criticized the symbolic interactionism of Herbert Blumer since Blumer's theory had no end to the openness of action. Parsons regarded Blumer as the mirror image of Claude L\u00e9vi-Strauss,[164] who tended to stress the quasi-determined nature of macro-structural systems. Action theory, Parsons maintained, represented a middle ground between both extremes.\n In 1976, Parsons was asked to contribute to a volume to celebrate the 80th birthday of Jean Piaget. Parsons contributed with an essay, \"A Few Considerations on the Place of Rationality in Modern Culture and Society\". Parsons characterized Piaget as the most eminent contributor to cognitive theory in the 20th century. However, he also argued that the future study of cognition had to go beyond its narrow encounter with psychology to aim at a higher understanding of how cognition as a human intellectual force was entangled in the processes of social and cultural institutionalization.[165]\n In 1978, when James Grier Miller published his famous work Living Systems,[166] Parsons was approached by Contemporary Sociology to write a review article on Miller's work. Parsons had already complained in a letter to A. Hunter Dupree[167] that American intellectual life suffered from a deep-seated tradition of empiricism and saw Miller's book the latest confirmation of that tradition. In his review, \"Concrete Systems and \"Abstracted Systems\",[168] he generally praised the herculean task behind Miller's work but criticized Miller for getting caught in the effort of hierarchize concrete systems but underplay the importance of structural categories in theory building. Parsons also complained about Miller's lack of any clear distinction between cultural and non-cultural systems.\n Japan had long been a keen interest in Parsons' work. As early as 1958, a Japanese translation of Economy and Society appeared. Also, The Structure of Social Action was translated into Japanese.[169] The Social System was translated into Japanese by Tsutomu Sato in 1974. Indeed, Ryozo Takeda had, as early as 1952 in his Shakaigaku no Kozo (\"The Framework of Sociology\") introduced Japanese scholars to some of Parsons' ideas. Parsons had visited Japan for the first time in 1972 and he gave a lecture on November 25 to the Japanese Sociological Association, \"Some Reflections on Post-Industrial Society\" that was published in The Japanese Sociological Review.[170] At the same time, Parsons participated in an international symposium on \"New Problems of Advanced Societies\", held in Tokyo, and he wrote short articles written that appeared in the proceedings of the symposium.[171][172] Tominaga, born in 1931, a leading figure in Japanese sociology and a professor at the University of Tokyo, was asked by Lidz to contribute to a two-volume collection of essays to honor Parsons. Tominaga wrote an essay on the industrial growth model of Japan and used Parsons' AGIL model.[173]\n In 1977, Washio Kurata, the new dean of the Faculty of Sociology of Kwansei Gakuin University, wrote to Parsons and invited him to visit Japan during the 1978\u20131979 academic year. In early spring, Parsons accepted the invitation, and on October 20, 1978, Parsons arrived at the Osaka Airport, accompanied by his wife, and was greeted by a large entourage.\n Parsons began weekly lectures at Kwansei's sociology department from October 23 to December 15. Parsons gave his first public lecture to a huge mass of undergraduates, \"The Development of Contemporary Sociology\".\n On November 17\u201318, when the Sengari Seminar House was opened, Parsons was invited as the key speaker at the event and gave two lectures, \"On the Crisis of Modern Society\"[174] and \"Modern Society and Religion\".[175] Present were Tominaga, Mutsundo Atarashi, Kazuo Muto, and Hideichiro Nakano.\n On November 25, lectures at Kobe University were organized by Hiroshi Mannari. Parsons lectured on organization theory to the faculty and the graduate students from the Departments of Economics, Management and Sociology. Also, faculty members from Kyoto and Osaka universities were present. A text was published the next year.[176] On November 30 to December 1, Parsons participated in a Tsukuba University Conference in Tokyo; Parsons spoke on \"Enter the New Society: The Problem of the Relationship of Work and Leisure in Relation to Economic and Cultural Values\".[177]\n On December 5, Parsons gave a lecture at Kyoto University on \"A Sociologist Looks at Contemporary U.S. Society\".[178]\n At a special lecture at Osaka on December 12, Parsons spoke, at the suggestion of Tominaga, on \"Social System Theory and Organization Theory\" to the Japanese Sociological Association.\n On December 14, Kwansei Gakuin University granted Parsons an honorary doctor degree. Some of his lectures would be collected into a volume by Kurata and published in 1983. The Parsons flew back to the US in mid-December 1978.\n Parsons died May 8, 1979, in Munich on a trip to Germany, where he was celebrating the 50th anniversary of his degree at Heidelberg. The day before, he had given a lecture on social class to an audience of German intellectuals, including Habermas, Niklas Luhmann and Wolfgang Schluchter.\n Parsons produced a general theoretical system for the analysis of society, which he called \"theory of action\", based on the methodological and epistemological principle of \"analytical realism\" and on the ontological assumption of \"voluntaristic action\".[179] Parsons' concept of analytical realism can be regarded as a kind of compromise between nominalist and realist views on the nature of reality and human knowledge.[180] Parsons believed that objective reality can be related to only by a particular encounter of such reality and that general intellectual understanding is feasible through conceptual schemes and theories. Interaction with objective reality on an intellectual level should always be understood as an approach. Parsons often explained the meaning of analytical realism by quoting a statement by Henderson: \"A fact is a statement about experience in terms of a conceptual scheme.\"[181]\n Generally, Parsons maintained that his inspiration regarding analytical realism had been Lawrence Joseph Henderson and Alfred North Whitehead[182] although he might have gotten the idea much earlier. It is important for Parsons' \"analytical realism\" to insist on the reference to an objective reality since he repeatedly highlighted that his concept of \"analytical realism\" was very different from the \"fictionalism\" of Hans Vaihiger (Hans Vaihinger):[183]\n We must start with the assertion that all knowledge which purports to be valid in anything like the scientific sense presumes both the reality of object known and of a knower. I think we can go beyond that and say that there must be a community of knowers who are able to communicate with each other. Without such a presupposition it would seem difficult to avoid the pitfall of solipsism. The so-called natural sciences do not, however, impute the \"status of knowing subjects\" to the objects with which they deal.[184] The Structure of Social Action (SSA), Parsons' most famous work, took form piece by piece. Its central figure was Weber, and the other key figures in the discussion were added, little by little, as the central idea took form. One important work that helped Parsons' central argument in was, in 1932, unexpectedly found: \u00c9lie Hal\u00e9vy's La formation du radicalisme philosophique (1901\u20131904); he read the three-volume work in French. Parsons explained, \"Well, Hal\u00e9vy was just a different world\u00a0... and helped me to really get in to many clarifications of the assumptions distinctive to the main line of British utilitarian thought; assumptions about the 'natural identity of interest', and so on. I still think it is one of the true masterpieces in intellectual history.\"[36] Parsons first achieved significant recognition with the publication of The Structure of Social Action (1937), his first grand synthesis, combining the ideas of Durkheim, Weber, Pareto, and others. In 1998, the International Sociological Association listed it as the ninth most important sociological book of the 20th Century.[185]\n Parsons' action theory can be characterized as an attempt to maintain the scientific rigour of positivism while acknowledging the necessity of the \"subjective dimension\" of human action incorporated in hermeneutic types of sociological theories. It is cardinal in Parsons' general theoretical and methodological view that human action must be understood in conjunction with the motivational component of the human act. Social science must consider the question of ends, purpose, and ideals in its analysis of human action. Parsons' strong reaction to behavioristic theory as well as to sheer materialistic approaches derives from the attempt of the theoretical positions to eliminate ends, purpose, and ideals as factors of analysis. Parsons, in his term papers at Amherst, was already criticizing attempts to reduce human life to psychological, biological, or materialist forces. What was essential in human life, Parsons maintained, was how the factor of culture was codified. Culture, however, was to Parsons an independent variable in that it could not be \"deducted\" from any other factor of the social system. That methodological intention is given the most elaborate presentation in The Structure of Social Action, which was Parsons' first basic discussion of the methodological foundation of the social sciences.\n Some of the themes in The Structure of Social Action had been presented in a compelling essay two years earlier in \"The Place of Ultimate Values in Sociological Theory\".[186]\n An intense correspondence and dialogue between Talcott Parsons and Alfred Schutz serves to highlight the meaning of central concepts in The Structure of Social Action.\n Parsons developed his ideas during a period when systems theory and cybernetics were very much on the front burner of social and behavioral science. In using systems thinking, he postulated that the relevant systems treated in social and behavioral science were \"open:\" they were embedded in an environment with other systems. For social and behavioral science, the largest system is \"the action system,\" the interrelated behaviors of human beings, embedded in a physical-organic environment.[187]\n As Parsons developed his theory, it became increasingly bound to the fields of cybernetics and system theory but also to Emerson's concept of homeostasis[188] and Ernst Mayr's concept of \"teleonomic processes\".[189] On the metatheoretical level, Parson attempted to balance psychologist phenomenology and idealism on the one hand and pure types of what Parsons called the utilitarian-positivistic complex, on the other hand.\n The theory includes a general theory of social evolution and a concrete interpretation of the major drives of world history. In Parsons' theory of history and evolution, the constitutive-cognitive symbolization of the cybernetic hierarchy of action-systemic levels has, in principle, the same function as genetic information in DNA's control of biological evolution, but that factor of metasystemic control does not \"determine\" any outcome but defines the orientational boundaries of the real pathfinder, which is action itself. Parsons compares the constitutive level of society with Noam Chomsky's concept of \"deep structure\".\n As Parsons wrote, \"The deep structures do not as such articulate any sentences which could convey coherent meaning. The surface structures constitute the level at which this occurs. The connecting link between them is a set of rules of transformation, to use Chomsky's own phrase.\"[190] The transformative processes and entities are generally, at least on one level of empirical analysis, performed or actualized by myths and religions,[191] but philosophies, art systems, or even semiotic consumer behavior can, in principle, perform that function.[192]\n Parsons' theory reflects a vision of a unified concept of social science and indeed of living systems[193] in general. His approach differs in essence from Niklas Luhmann's theory of social systems because Parsons rejects the idea that systems can be autopoietic, short of the actual action system of individual actors. Systems have immanent capacities but only as an outcome of the institutionalized processes of action-systems, which, in the final analysis, is the historical effort of individual actors. While Luhmann focused on the systemic immanence, Parsons insisted that the question of autocatalytic and homeostatic processes and the question about the actor as the ultimate \"first mover\" on the other hand was not mutually exclusive. Homeostatic processes might be necessary if and when they occur but action is necessitating.\n It is only that perspective of the ultimate reference in action that Parsons' dictum (that higher-order cybernetic systems in history will tend to control social forms that are organized on the lower levels of the cybernetic hierarchy) should be understood. For Parsons, the highest levels of the cybernetic hierarchy as far as the general action level is concerned is what Parsons calls the constitutive part of the cultural system (the L of the L). However, within the interactional processes of the system, attention should be paid especially to the cultural-expressivistic axis (the L-G line in the AGIL). By the term constitutive, Parsons generally referred to very highly codified cultural values especially religious elements (but other interpretation of the term \"constitutive\" is possible).[194]\n Cultural systems have an independent status from that of the normative and orientational pattern of the social system; neither system can be reduced to the other. For example, the question of the \"cultural capital\" of a social system as a sheer historical entity (in its function as a \"fiduciary system\"), is not identical to the higher cultural values of that system; that is, the cultural system is embodied with a metastructural logic that cannot be reduced to any given social system or cannot be viewed as a materialist (or behavioralist) deduction from the \"necessities\" of the social system (or from the \"necessities\" of its economy).[195] Within that context, culture would have an independent power of transition, not only as factors of actual sociocultural units (like Western civilization) but also how original cultural bases would tend to \"universalize\" through interpenetration and spread over large numbers of social systems as with Classical Greece and Ancient Israel, where the original social bases had died but the cultural system survived as an independently \"working\" cultural pattern, as in the case of Greek philosophy or in the case of Christianity, as a modified derivation from its origins in Israel.[196]\n It is important to highlight that Parsons distinguished two \"meanings\" or modes of the term general theory. He sometimes wrote about general theory as aspects of theoretical concerns of social sciences whose focus is on the most \"constitutive\" elements of cognitive concern for the basic theoretical systematization of a given field. Parsons would include the basic conceptual scheme for the given field, including its highest order of theoretical relations and naturally also the necessary specification of this system's axiomatic, epistemological, and methodological foundations from the point of view of logical implications.[197][198] All the elements would signify the quest for a general theory on the highest level of theoretical concern.\n However, general theory could also refer to a more fully/operational system whose implications of the conceptual scheme were \"spelled out\" on lower levels of cognitive structuralization, levels standing closer to a perceived \"empirical object\". In his speech to the American Sociological Society in 1947, he spoke of five levels:[199]\n During his life, he would work on developing all five fields of theoretical concerns but pay special attention to the development on the highest \"constitutive\" level, as the rest of the building would stand or fall on the solidity of the highest level.[200]\n Despite myths, Parsons never thought that modern societies exist in some kind of perfect harmony with their norms or that most modern societies were necessarily characterized by some high level of consensus or a \"happy\" institutional integration. Parsons highlighted that is almost logically impossible that there can be any \"perfect fit\" or perfect consensus in the basic normative structure of complex modern societies because the basic value pattern of modern societies is generally differentiated in such a way that some of the basic normative categories exist in inherent or at least potential conflict with each other. For example, freedom and equality are generally viewed as fundamental and non-negotiable values of modern societies. Each represents a kind of ultimate imperative about what the higher values of humanity. However, as Parsons emphasizes, no simple answer on the priority of freedom or equality or any simple solution on how they possibly can be mediated, if at all. Therefore, all modern societies are faced with the inherent conflict prevailing between the two values, and there is no \"eternal solution\" as such. There cannot be any perfect match between motivational pattern, normative solutions, and the prevailing value pattern in any modern society. Parsons also maintained that the \"dispute\" between \"left\" and \"right\" has something to do with the fact that they both defend ultimately \"justified\" human values (or ideals), which alone is indispensable as values but are always in an endless conflictual position to each other.\n Parsons always maintained that the integration of the normative pattern in society is generally problematic and that the level of integration that is reached in principle is always far from harmonious and perfect. If some \"harmonious pattern\" emerges, it is related to specific historical circumstances but is not a general law of the social systems.\n The heuristic scheme that Parsons used to analyze systems and subsystems is called the AGIL paradigm or the AGIL scheme.[201] To survive or maintain equilibrium with respect to its environment, any system must to some degree:\n The concepts can be abbreviated as AGIL and are called the system's functional imperatives. Parsons' AGIL model is an analytical scheme for the sake of theoretical \"production\", but it is not any simple \"copy\" or any direct historical \"summary\" of empirical reality. Also, the scheme itself does not explain \"anything\", just as the periodic table explains nothing by itself in the natural sciences. The AGIL scheme is a tool for explanations and is no better than the quality of the theories and explanation by which it is processed.\n In the case of the analysis of a social action system, the AGIL paradigm, according to Parsons, yields four interrelated and interpenetrating subsystems: the behavioral systems of its members (A), the personality systems of those members (G), the social system (as such) (I), and the cultural system of that society (L). To analyze a society as a social system (the I subsystem of action), people are posited to enact roles associated with positions. The positions and roles become differentiated to some extent and, in a modern society, are associated with things such as occupational, political, judicial, and educational roles.\n Considering the interrelation of these specialized roles as well as functionally differentiated collectivities (like firms and political parties), a society can be analyzed as a complex system of interrelated functional subsystems:\n The pure AGIL model for all living systems:\n The Social System Level:\n The General Action Level:\n The cultural level:\n The Generalized Symbolic media:\n Social System level:\n Parsons elaborated upon the idea that each of these systems also developed some specialized symbolic mechanisms of interaction analogous to money in the economy, like influence in the social community. Various processes of \"interchange\" among the subsystems of the social system were postulated.\n Parsons' use of social systems analysis based on the AGIL scheme was established in his work Economy and Society (with N. Smelser, 1956) and prevailed in all his subsequent work. However, the AGIL system existed only in a \"rudimentary\" form in the beginning and was gradually elaborated and expanded in the decades which followed. A brief introduction to Parsons' AGIL scheme appears in Chapter 2 of The American University.[202]\nThere is, however, no single place in his writing in which the total AGIL system is visually displayed or explained: the complete system has to be reconstructed from multiple places in his writing. The system displayed in \"The American University\" has only the most basic elements and should not be mistaken for the whole system.\n Parsons contributed to social evolutionism and neoevolutionism. He divided evolution into four sub-processes:\n Furthermore, Parsons explored the sub-processes within three stages of evolution:\n Parsons viewed Western civilization as the pinnacle of modern societies and the United States as the one that is most dynamically developed.\n Parsons' late work focused on a new theoretical synthesis around four functions that he claimed are common to all systems of action, from the behavioral to the cultural, and a set of symbolic media that enables communication across them. His attempt to structure the world of action according to a scheme that focused on order was unacceptable for American sociologists, who were retreating from the grand pretensions of the 1960s to a more empirical, grounded approach.\n Parsons asserted that there are not two dimensions to societies (instrumental and expressive) but that there are qualitative differences between kinds of social interaction.\n He observed that people can have personalized and formally detached relationships, based on the roles that they play. The pattern variables are what he called the characteristics that are associated with each kind of interaction.\n An interaction can be characterized by one of the identifiers of each contrastive pair:\n From the 1940s to the 1970s, Parsons was one of the most famous and most influential but also most controversial sociologists in the world, particularly in the US.[18] His later works were met with criticism and were generally dismissed in the 1970s by the view that his theories were too abstract, inaccessible, and socially conservative.[18][203]\n Recently, interest has increased in Parsons' ideas and especially often-overlooked later works.[17] Attempts to revive his thinking have been made by Parsonsian sociologists and social scientists like Jeffrey Alexander, Bryan Turner, Richard M\u00fcnch, and Roland Robertson, and Uta Gerhardt has written about Parsons from a biographical and historical perspective. In addition to the United States, the key centers of interest in Parsons today are Germany, Japan, Italy, and the United Kingdom.[citation needed]\n Parsons had a seminal influence and early mentorship of many American and international scholars, such as Ralf Dahrendorf, Alain Touraine, Niklas Luhmann, and Habermas.[citation needed] His best-known pupil was Merton.[18] Parsons was a member of the American Philosophical Society.[204]\n In 1930 Parson's published a translation of Weber's classic work The Protestant Ethic and the Spirit of Capitalism\n",
      "timestamp": "2025-10-09 19:12:45.286705"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Talcott_Parsons",
      "title": "Talcott Parsons - Wikipedia",
      "description": "",
      "text": "\n Talcott Parsons (December 13, 1902 \u2013 May 8, 1979) was an American sociologist of the classical tradition, best known for his social action theory and structural functionalism. Parsons is considered one of the most influential figures in sociology in the 20th century.[17] After earning a PhD in economics, he served on the faculty at Harvard University from 1927 to 1973. In 1930, he was among the first professors in its new sociology department.[18] Later, he was instrumental in the establishment of the Department of Social Relations at Harvard.\n Based on empirical data, Parsons' social action theory was the first broad, systematic, and generalizable theory of social systems developed in the United States and Europe.[19] Some of Parsons' largest contributions to sociology in the English-speaking world were his translations of Max Weber's work and his analyses of works by Weber, \u00c9mile Durkheim, and Vilfredo Pareto. Their work heavily influenced Parsons' view and was the foundation for his social action theory. Parsons viewed voluntaristic action through the lens of the cultural values and social structures that constrain choices and ultimately determine all social actions, as opposed to actions that are determined based on internal psychological processes.[19] Although Parsons is generally considered a structural functionalist, towards the end of his career, in 1975, he published an article that stated that \"functional\" and \"structural functionalist\" were inappropriate ways to describe the character of his theory.[20]\n From the 1970s on, a new generation of sociologists criticized Parsons' theories as socially conservative and his writings as unnecessarily complex. Sociology courses have placed less emphasis on his theories than at the peak of his popularity (from the 1940s to the 1970s). However, there has been a recent resurgence of interest in his ideas.[18]\n Parsons was a strong advocate for the professionalization of sociology and its expansion in American academia. He was elected president of the American Sociological Association in 1949 and served as its secretary from 1960 to 1965.\n Parsons was born on December 13, 1902, in Colorado Springs, Colorado, to Edward Smith Parsons and Mary Augusta Ingersoll. His father had attended Yale Divinity School, was ordained as a Congregationalist minister, and served first as a minister for a pioneer community in Greeley, Colorado. At the time of Parsons' birth, his father was a professor in English and vice-president at Colorado College. During his Congregational ministry in Greeley, Edward had become sympathetic to the Social Gospel movement but tended to view it from a higher theological position and was hostile to the ideology of socialism.[21]\n As an undergraduate, Parsons studied biology and philosophy at Amherst College and received his BA in 1924. Amherst College had become the Parsons' family college by tradition; his father and his uncle Frank had attended it, as had his elder brother, Charles Edward. Initially, Parsons was attracted to a career in medicine, as he was inspired by his elder brother[22]:\u200a826\u200a so he studied a great deal of biology and spent a summer working at the Oceanographic Institution at Woods Hole, Massachusetts.\n Parsons' biology professors at Amherst were Otto C. Glaser and Henry Plough. Gently mocked as \"Little Talcott, the gilded cherub,\" Parsons became one of the student leaders at Amherst. Parsons also took courses with Walton Hale Hamilton and the philosopher Clarence Edwin Ayres, both known as \"institutional economists\". Hamilton, in particular, drew Parsons toward social science.[22]:\u200a826\u200a They exposed him to literature by authors such as Thorstein Veblen, John Dewey, and William Graham Sumner. Parsons also took a course with George Brown in the philosophy of Immanuel Kant and a course in modern German philosophy with Otto Manthey-Zorn, who was a great interpreter of Kant. Parsons showed from early on, a great interest in the topic of philosophy.\n Two term papers that Parsons wrote as a student for Clarence E. Ayres at Amherst have survived. They are referred to as the Amherst Papers and have been of strong interest to Parsons scholars. The first was written on December 19, 1922, \"The Theory of Human Behavior in its Individual and Social Aspects.\"[23] The second was written on March 27, 1923, \"A Behavioristic Conception of the Nature of Morals\".[24] The papers reveal Parsons' early interest in social evolution.[25] The Amherst Papers also reveal that Parsons did not agree with his professors since he wrote in his Amherst papers that technological development and moral progress are two structurally-independent empirical processes.\n After Amherst, he studied at the London School of Economics for a year, where he was exposed to the work of Bronis\u0142aw Malinowski, R. H. Tawney, L. T. Hobhouse, and Harold Laski.[22]:\u200a826\u200a During his days at LSE, he made friends with E. E. Evans-Pritchard, Meyer Fortes, and Raymond Firth, who all participated in the Malinowski seminar. Also, he made a close personal friendship with Arthur and Eveline M. Burns.\n At LSE he met Helen Bancroft Walker, a young American, and they married on April 30, 1927. The couple had three children: Anne, Charles, and Susan.\n In June, Parsons went on to the University of Heidelberg, where he received his PhD in sociology and economics in 1927. At Heidelberg, he worked with Alfred Weber, Max Weber's brother; Edgar Salin, his dissertation adviser; Emil Lederer; and Karl Mannheim. He was examined on Kant's Critique of Pure Reason by the philosopher Karl Jaspers.[26] At Heidelberg, Parsons was also examined by Willy Andreas on the French Revolution. Parsons wrote his Dr. Phil. thesis on The Concept of Capitalism in the Recent German Literature, with his main focus on the work of Werner Sombart and Weber. It was clear from his discussion that he rejected Sombart's quasi-idealistic views and supported Weber's attempt to strike a balance between historicism, idealism and neo-Kantianism.\n The most crucial encounter for Parsons at Heidelberg was with the work of Max Weber about whom he had never heard before. Weber became tremendously important for Parsons because his upbringing with a liberal but strongly-religious father had made the question of the role of culture and religion in the basic processes of world history a persistent puzzle in his mind. Weber was the first scholar who truly provided Parsons with a compelling theoretical \"answer\" to the question.\n Parsons decided to translate Weber's work into English and approached Marianne Weber, Weber's widow. Parsons would eventually translate several of Weber's works.[27][28] His time in Heidelberg had him invited by Marianne Weber to \"sociological teas\", which were study group meetings that she held in the library room of her and Max's old apartment. One scholar that Parsons met at Heidelberg who shared his enthusiasm for Weber was Alexander von Schelting. Parsons later wrote a review article on von Schelting's book on Weber.[29] Generally, Parsons read extensively in religious literature, especially works focusing on the sociology of religion. One scholar who became especially important for Parsons was Ernst D. Troeltsch. Parsons also read widely on Calvinism. His reading included the work of Emile Doumerque,[30] Eug\u00e9ne Choisy, and Henri Hauser.\n In 1927, after a year of teaching at Amherst (1926\u20131927), Parsons entered Harvard, as an instructor in the Economics Department,[31] where he followed F.\u00a0W. Taussig's lectures on economist Alfred Marshall and became friends with the economist historian Edwin Gay, the founder of Harvard Business School. Parsons also became a close associate of Joseph Schumpeter and followed his course General Economics. Parsons was at odds with some of the trends in Harvard's department which then went in a highly-technical and a mathematical direction. He looked for other options at Harvard and gave courses in \"Social Ethics\" and in the \"Sociology of Religion\".\n The chance for a shift to sociology came in 1930, when Harvard's Sociology Department was created[32] under Russian scholar Pitirim Sorokin. Parsons became one of the new department's two instructors, along with Carle Zimmerman.[33] Parsons established close ties with biochemist and sociologist Lawrence Joseph Henderson, who took a personal interest in Parsons' career at Harvard. Parsons became part of L.\u00a0J. Henderson's famous Pareto study group, in which some of the most important[citation needed] intellectuals at Harvard participated, including Crane Brinton, George C. Homans, and Charles P. Curtis. Parsons wrote an article on Pareto's theory[34] and later explained that he had adopted the concept of \"social system\" from reading Pareto. Parsons also made strong connections with two other influential intellectuals with whom he corresponded for years: economist Frank H. Knight and businessman Chester Barnard. The relationship between Parsons and Sorokin turned sour. A pattern of personal tensions was aggravated by Sorokin's deep dislike for American civilization, which he regarded as a sensate culture that was in decline. Sorokin's writings became increasingly anti-scientistic in his later years, widening the gulf between his work and Parsons' and turning the increasingly positivistic American sociology community against him. Sorokin also tended to belittle all sociology tendencies that differed from his own writings, and by 1934 was quite unpopular at Harvard.\n Some of Parsons' students in the department of sociology were Robin Williams Jr., Robert K. Merton, Kingsley Davis, Wilbert Moore, Edward C. Devereux, Logan Wilson, Nicholas Demereth, John Riley Jr., and Mathilda White Riley. Later cohorts of students included Harry Johnson, Bernard Barber, Marion Levy and Jesse R. Pitts. Parsons established, at the students' request, an informal study group which met year after year in Adams' house. Toward the end of Parsons' career, German systems theorist Niklas Luhmann also attended his lectures.\n In 1932, Parsons bought a farmhouse near the small town of Acworth, but Parsons often, in his writing, referred to it as \"the farmhouse in Alstead\". The farmhouse was a very humble structure with almost no modern utilities. Still, it became central to Parsons' life, and many of his most important works were written there.\n In the academic year of 1939\u20131940 Parsons and Schumpeter conducted an informal faculty seminar at Harvard, which discussed the concept of rationality. Among the participants were D.\u00a0V. McGranahan, Abram Bergson, Wassily Leontief, Gottfried Haberler, and Paul Sweezy. Schumpeter contributed the essay \"Rationality in Economics\", and Parsons submitted the paper \"The Role of Rationality in Social Action\" for a general discussion.[35]\n \nIn the discussion between neoclassical economics and the institutionalists, which was one of the conflicts that prevailed within the field of economics in the 1920s and early 1930s, Parsons attempted to walk a very fine line. He was very critical about neoclassical theory, an attitude he maintained throughout his life and that is reflected in his critique of Milton Friedman and Gary Becker. He was opposed to the utilitarian bias within the neoclassical approach and could not embrace them fully. However, he agreed partly on their theoretical and methodological style of approach, which should be distinguished from its substance. He was thus unable to accept the institutionalist solution. In a 1975 interview, Parsons recalled a conversation with Schumpeter on the institutionalist methodological position:  An economist like Schumpeter, by contrast, would absolutely have none of that. I remember talking to him about the problem and .. I think Schumpeter was right. If economics had gone that way [like the institutionalists] it would have had to become a primarily empirical discipline, largely descriptive, and without theoretical focus. That's the way the 'institutionalists' went, and of course Mitchell was affiliated with that movement.[36] Parsons returned to Germany in the summer of 1930 and became an eyewitness to the feverish atmosphere in Weimar Germany during which the Nazi Party rose to power. Parsons received constant reports about the rise of Nazism through his friend, Edward Y. Hartshorne, who was traveling there. Parsons began, in the late 1930s, to warn the American public about the Nazi threat, but he had little success, as a poll showed that 91 percent of the country opposed the Second World War.[37]\nOne of the first articles that Parsons wrote was \"New Dark Age Seen If Nazis Should Win\". He was one of the key initiators of the Harvard Defense Committee, aimed at rallying the American public against the Nazis. Parsons' voice sounded again and again over Boston's local radio stations, and he also spoke against Nazism during a dramatic meeting at Harvard, which was disturbed by antiwar activists. Together with graduate student Charles O. Porter, Parsons rallied graduate students at Harvard for the war effort. During the war, Parsons conducted a special study group at Harvard, which analyzed what its members considered the causes of Nazism, and leading experts on that topic participated.\n In the spring of 1941, a discussion group on Japan began to meet at Harvard. The group's five core members were Parsons, John K. Fairbank, Edwin O. Reischauer, William M. McGovern, and Marion Levy Jr. A few others occasionally joined the group, including Ai-Li Sung and Edward Y. Hartshorne. The group arose out of a strong desire to understand the country, but, as Levy frankly admitted, \"Reischauer was the only one who knew anything about Japan.\"[38] Parsons, however, was eager to learn more about it and was \"concerned with general implications.\"\n In 1942, Parsons worked on arranging a major study of occupied countries with Bartholomew Landheer of the Netherlands Information Office in New York.[39] Parsons had mobilized Georges Gurvitch, Conrad Arnsberg, Dr. Safranek and Theodore Abel to participate,[40] but it never materialized for lack of funding. In early 1942, Parsons unsuccessfully approached Hartshorne, who had joined the Psychology Division of the Office of the Coordinator of Information (COI) in Washington to interest his agency in the research project. In February 1943, Parsons became the deputy director of the Harvard School of Overseas Administration, which educated administrators to \"run\" the occupied territories in Germany and the Pacific Ocean. The task of finding relevant literature on both Europe and Asia was mindboggling and occupied a fair amount of Parsons' time. One scholar Parsons came to know was Karl August Wittfogel and they discussed Weber. On China, Parsons received fundamental information from Chinese scholar Ai-Li Sung Chin and her husband, Robert Chin. Another Chinese scholar Parsons worked closely with in this period was Hsiao-Tung Fei (or Fei Xiaotong), who had studied at the London School of Economics and was an expert on the social structure of the Chinese village.\n Parsons met Alfred Sch\u00fctz during the rationality seminar, which he conducted together with Schumpeter, at Harvard in the spring of 1940. Sch\u00fctz had been close to Edmund Husserl and was deeply embedded in the latter's phenomenological philosophy.[41] Sch\u00fctz was born in Vienna but moved to the US in 1939, and for years, he worked on the project of developing a phenomenological sociology, primarily based on an attempt to find some point between Husserl's method and Weber's sociology.[42] Parsons had asked Sch\u00fctz to give a presentation at the rationality seminar, which he did on April 13, 1940, and Parsons and Sch\u00fctz had lunch together afterward. Sch\u00fctz was fascinated with Parsons' theory, which he regarded as the state-of-the-art social theory, and wrote an evaluation of Parsons' theory that he kindly asked Parsons to comment. That led to a short but intensive correspondence, which generally revealed that the gap between Sch\u00fctz's sociologized phenomenology and Parsons' concept of voluntaristic action was far too great.[43] From Parsons' point of view, Sch\u00fctz's position was too speculative and subjectivist, and tended to reduce social processes to the articulation of a Lebenswelt consciousness. For Parsons, the defining edge of human life was action as a catalyst for historical change, and it was essential for sociology, as a science, to pay strong attention to the subjective element of action, but it should never become completely absorbed in it since the purpose of a science was to explain causal relationships, by covering laws or by other types of explanatory devices. Sch\u00fctz's basic argument was that sociology cannot ground itself and that epistemology was not a luxury but a necessity for the social scientist. Parsons agreed but stressed the pragmatic need to demarcate science and philosophy and insisted moreover that the grounding of a conceptual scheme for empirical theory construction cannot aim at absolute solutions but needs to take a sensible stock-taking of the epistemological balance at each point in time. However, the two men shared many basic assumptions about the nature of social theory, which has kept the debate simmering ever since.[44][45] By request from Ilse Sch\u00fctz, after her husband's death, Parsons gave permission to publish the correspondence between him and Sch\u00fctz. Parsons also wrote \"A 1974 Retrospective Perspective\" to the correspondence, which characterized his position as a \"Kantian point of view\" and found that Sch\u00fctz's strong dependence on Husserl's \"phenomenological reduction\" would make it very difficult to reach the kind of \"conceptual scheme\" that Parsons found essential for theory-building in social sciences.[46]\n Between 1940 and 1944, Parsons and Eric Voegelin exchanged intellectual views through correspondence.[47][48][49] Parsons had probably met Voegelin in 1938 and 1939, when Voegelin held a temporary instructor appointment at Harvard. The bouncing point for their conversation was Parsons' manuscript on anti-Semitism and other materials that he had sent to Voegelin. Discussion touched on the nature of capitalism, the rise of the West, and the origin of Nazism. The key to the discussion was the implication of Weber's interpretation of Protestant ethics and the impact of Calvinism on modern history. Although the two scholars agreed on many fundamental characteristics about Calvinism, their understanding of its historical impact was quite different. Generally, Voegelin regarded Calvinism as essentially a dangerous totalitarian ideology; Parsons argued that its current features were temporary and that the functional implications of its long-term, emerging value-l system had revolutionary and not only \"negative\" impact on the general rise of the institutions of modernity.\n The two scholars also discussed Parsons' debate with Sch\u00fctz and especially why Parsons had ended his encounter with Schutz. Parsons found that Schutz, rather than attempting to build social science theory, tended to get consumed in philosophical detours. Parsons wrote to Voegelin: \"Possibly one of my troubles in my discussion with Schuetz lies in the fact that by cultural heritage I am a Calvinist. I do not want to be a philosopher \u2013 I shy away from the philosophical problems underlying my scientific work. By the same token I don't think he wants to be a scientist as I understand the term until he has settled all the underlying philosophical difficulties. If the physicists of the 17th century had been Schuetzes there might well have been no Newtonian system.\"[50]\n In 1942, Stuart C. Dodd published a major work, Dimensions of Society,[51] which attempted to build a general theory of society on the foundation of a mathematical and quantitative systematization of social sciences. Dodd advanced a particular approach, known as an \"S-theory\". Parsons discussed Dodd's theoretical outline in a review article the same year.[52] Parsons acknowledged Dodd's contribution to be an exceedingly formidable work but argued against its premises as a general paradigm for the social sciences. Parsons generally argued that Dodd's \"S-theory\", which included the so-called \"social distance\" scheme of Bogardus, was unable to construct a sufficiently sensitive and systematized theoretical matrix, compared with the \"traditional\" approach, which has developed around the lines of Weber, Pareto, \u00c9mile Durkheim, Sigmund Freud, William Isaac Thomas, and other important agents of an action-system approach with a clearer dialogue with the cultural and motivational dimensions of human interaction.\n In April 1944, Parsons participated in a conference, \"On Germany after the War\", of psychoanalytical oriented psychiatrists and a few social scientists to analyze the causes of Nazism and to discuss the principles for the coming occupation.[53]\n During the conference, Parsons opposed what he found to be Lawrence S. Kubie's reductionism. Kubie was a psychoanalyst, who strongly argued that the German national character was completely \"destructive\" and that it would be necessary for a special agency of the United Nations to control the German educational system directly. Parsons and many others at the conference were strongly opposed to Kubie's idea. Parsons argued that it would fail and suggested that Kubie was viewing the question of Germans' reorientation \"too exclusively in psychiatric terms\". Parsons was also against the extremely harsh Morgenthau Plan, published in September 1944. After the conference, Parsons wrote an article, \"The Problem of Controlled Institutional Change\", against the plan.[54]\n Parsons participated as a part-time adviser to the Foreign Economic Administration Agency between March and October 1945 to discuss postwar reparations and deindustrialization.[55][56]\n Parsons was elected a Fellow of the American Academy of Arts and Sciences in 1945.[57]\n Parsons' situation at Harvard University changed significantly in early 1944, when he received a good offer from Northwestern University. Harvard reacted to the offer by appointing Parsons as the chairman of the department, promoting him to the rank of full professor and accepting the process of reorganization, which led to the establishment of the new department of Social Relations. Parsons' letter to Dean Paul Buck, on April 3, 1944, reveals the high point of this moment.[58] Because of the new development at Harvard, Parsons chose to decline an offer from William Langer to join the Office of Strategic Services, the predecessor of the Central Intelligence Agency. Langer proposed for Parsons to follow the American army in its march into Germany and to function as a political adviser to the administration of the occupied territories. Late in 1944, under the auspices of the Cambridge Community Council, Parsons directed a project together with Elizabeth Schlesinger. They investigated ethnic and racial tensions in the Boston area between students from Radcliffe College and Wellesley College. This study was a reaction to an upsurge of anti-Semitism in the Boston area, which began in late 1943 and continued into 1944.[59] At the end of November 1946, the Social Research Council (SSRC) asked Parsons to write a comprehensive report of the topic of how the social sciences could contribute to the understanding of the modern world. The background was a controversy over whether the social sciences should be incorporated into the National Science Foundation.\n Parsons' report was in form of a large memorandum, \"Social Science: A Basic National Resource\", which became publicly available in July 1948 and remains a powerful historical statement about how he saw the role of modern social sciences.[60]\n Parsons became a member of the executive committee of the new Russian Research Center at Harvard in 1948, which had Parsons' close friend and colleague, Clyde Kluckhohn, as its director. Parsons went to Allied-occupied Germany in the summer of 1948, was a contact person for the RRC, and was interested in the Russian refugees who were stranded in Germany. He happened to interview in Germany a few members of the Vlasov Army, a Russian Liberation Army that had collaborated with the Germans during the war.[61] The movement was named after Andrey Vlasov, a Soviet general captured by the Germans in June 1942. The Vlasov movement's ideology was a hybrid of elements and has been called \"communism without Stalin\", but in the Prague Manifesto (1944), it had moved toward the framework of a constitutional liberal state.[62]\n In Germany in the summer of 1948 Parsons wrote several letters to Kluckhohn to report on his investigations.\n Parsons' fight against communism was a natural extension of his fight against fascism in the 1930s and the 1940s. For Parsons, communism and fascism were two aspects of the same problem; his article \"A Tentative Outline of American Values\", published posthumously in 1989,[63] called both collectivistic types \"empirical finalism\", which he believed was a secular \"mirror\" of religious types of \"salvationalism\". In contrast, Parsons highlighted that American values generally were based on the principle of \"instrumental activism\", which he believed was the outcome of Puritanism as a historical process. It represented what Parsons called \"worldly asceticism\" and represented the absolute opposite of empirical finalism. One can thus understand Parsons' statement late in life that the greatest threat to humanity is every type of \"fundamentalism\".[64] By the term empirical finalism, he implied the type of claim assessed by cultural and ideological actors about the correct or \"final\" ends of particular patterns of value orientation in the actual historical world (such as the notion of \"a truly just society\"), which was absolutist and \"indisputable\" in its manner of declaration and in its function as a belief system. A typical example would be the Jacobins' behavior during the French Revolution. Parsons' rejection of communist and fascist totalitarianism was theoretically and intellectually an integral part of his theory of world history, and he tended to regard the European Reformation as the most crucial event in \"modern\" world history. Like Weber,[65] he tended to highlight the crucial impact of Calvinist religiosity in the socio-political and socio-economic processes that followed.[66] He maintained it reached its most radical form in England in the 17th century and in effect gave birth to the special cultural mode that has characterized the American value system and history ever since. The Calvinist faith system, authoritarian in the beginning, eventually released in its accidental long-term institutional effects a fundamental democratic revolution in the world.[67] Parsons maintained that the revolution was steadily unfolding, as part of an interpenetration of Puritan values in the world at large.[68]\n Parsons defended American exceptionalism and argued that, because of a variety of historical circumstances, the impact of the Reformation had reached a certain intensity in British history. Puritan, essentially Calvinist, value patterns had become institutionalized in Britain's internal situation. The outcome was that Puritan radicalism was reflected in the religious radicalism of the Puritan sects, in the poetry of John Milton, in the English Civil War, and in the process leading to the Glorious Revolution of 1688. It was the radical fling of the Puritan Revolution that provided settlers in early 17th-century Colonial America, and the Puritans who settled in America represented radical views on individuality, egalitarianism, skepticism toward state power, and the zeal of the religious calling. The settlers established something unique in the world that was under the religious zeal of Calvinist values.\n Therefore, a new kind of nation was born, the character of which became clear by the time of the American Revolution and in the US constitution,[69] and its dynamics were later studied by Alexis de Tocqueville.[70] The French Revolution was a failed attempt to copy the American model. Although America has changed in its social composition since 1787, Parsons maintained that it preserves the basic revolutionary Calvinist value pattern. That has been further revealed in the pluralist and highly individualized America, with its thick, network-oriented civil society, which is of crucial importance to its success and these factors have provided it with its historical lead in the process of industrialization.\n Parsons maintained that this has continued to place it in the leading position in the world, but as a historical process and not in \"the nature of things\". Parsons viewed the \"highly special feature of the modern Western social world\" as \"dependent on the peculiar circumstances of its history, and not the necessary universal result of social development as a whole\".[71]\n In contrast to some \"radicals\", Parsons was a defender of modernity.[72] He believed that modern civilization, with its technology and its constantly evolving institutions, was ultimately strong, vibrant, and essentially progressive. He acknowledged that the future had no inherent guarantees, but as sociologists Robert Holton and Bryan Turner said that Parsons was not nostalgic[73] and that he did not believe in the past as a lost \"golden age\" but that he maintained that modernity generally had improved conditions, admittedly often in troublesome and painful ways but usually positively. He had faith in humanity's potential but not na\u00efvely. When asked at the Brown Seminary in 1973 if he was optimistic about the future, he answered, \"Oh, I think I'm basically optimistic about the human prospects in the long run.\" Parsons pointed out that he had been a student at Heidelberg at the height of the vogue of Oswald Spengler, author of The Decline of the West, \"and he didn't give the West more than 50 years of continuing vitality after the time he wrote.... Well, its more than 50 years later now, and I don't think the West has just simply declined. He was wrong in thinking it was the end.\"[74]\n At Harvard, Parsons was instrumental in forming the Department of Social Relations, an interdisciplinary venture among sociology, anthropology, and psychology. The new department was officially created in January 1946 with him as the chairman and with prominent figures at the faculty, such as Stouffer, Kluckhohn, Henry Murray and Gordon Allport. An appointment for Hartshorne was considered but he was killed in Germany by an unknown gunman as he was driving on the highway. His position went instead to George C. Homans. The new department was galvanized by Parsons' idea of creating a theoretical and institutional base for a unified social science. Parsons also became strongly interested in systems theory and cybernetics and began to adopt their basic ideas and concepts to the realm of social science, giving special attention to the work of Norbert Wiener.\n Some of the students who arrived at the Department of Social Relations in the years after the Second World War were David Aberle, Gardner Lindzey, Harold Garfinkel, David G. Hays, Benton Johnson, Marian Johnson, Kaspar Naegele, James Olds, Albert Cohen, Norman Birnbaum, Robin Murphy Williams, Jackson Toby, Robert N. Bellah, Joseph Kahl, Joseph Berger, Morris Zelditch, Ren\u00e9e Fox, Tom O'Dea, Ezra Vogel, Clifford Geertz, Joseph Elder, Theodore Mills, Mark Field, Edward Laumann, and Francis Sutton.\n Ren\u00e9e Fox, who arrived at Harvard in 1949, would become a very close friend of the Parsons family. Joseph Berger, who also arrived at Harvard in 1949 after finishing his BA from Brooklyn College, would become Parsons' research assistant from 1952 to 1953 and would get involved in his research projects with Robert F. Bales.\n According to Parsons' own account, it was during his conversations with Elton Mayo that he realized it was necessary for him to take a serious look at the work of Freud. In the fall of 1938, Parsons began to offer a series of non-credit evening courses on Freud. As time passed, Parsons developed a strong interest in psychoanalysis. He volunteered to participate in nontherapeutic training at the Boston Psychoanalytic Institute, where he began a didactic analysis with Grete Bibring in September 1946. Insight into psychoanalysis is significantly reflected in his later work, especially reflected in The Social System and his general writing on psychological issues and on the theory of socialization. That influence was also to some extent apparent in his empirical analysis of fascism during the war. Wolfgang K\u00f6hler's study of the mentality of apes and Kurt Koffka's ideas of Gestalt psychology also received Parsons' attention.\n During the late 1940s and the early 1950s, he worked very hard on producing some major theoretical statements. In 1951, Parsons published two major theoretical works, The Social System[75] and Toward a General Theory of Action.[76] The latter work, which was coauthored with Edward Tolman, Edward Shils and several others, was the outcome of the so-called Carnegie Seminar at Harvard University, which had taken place in the period of September 1949 and January 1950.[77] The former work was Parsons' first major attempt to present his basic outline of a general theory of society since The Structure of Social Action (1937). He discusses the basic methodological and metatheoretical principles for such a theory. He attempts to present a general social system theory that is built systematically from most basic premises and so he featured the idea of an interaction situation based on need-dispositions and facilitated through the basic concepts of cognitive, cathectic, and evaluative orientation. The work also became known for introducing his famous pattern variables, which in reality represented choices distributed along a Gemeinschaft vs. Gesellschaft axis.\n The details of Parsons' thought about the outline of the social system went through a rapid series of changes in the following years, but the basics remained. During the early 1950s, the idea of the AGIL model took place in Parsons's mind gradually. According to Parsons, its key idea was sparked during his work with Bales on motivational processes in small groups.[78]\n Parsons carried the idea into the major work that he co-authored with a student, Neil Smelser, which was published in 1956 as Economy and Society.[79] \nWithin this work, the first rudimentary model of the AGIL scheme was presented. It reorganized the basic concepts of the pattern variables in a new way and presented the solution within a system-theoretical approach by using the idea of a cybernetic hierarchy as an organizing principle. The real innovation in the model was the concept of the \"latent function\" or the pattern maintenance function, which became the crucial key to the whole cybernetic hierarchy.\n During its theoretical development, Parsons showed a persistent interest in symbolism. An important statement is Parsons' \"The Theory of Symbolism in Relation to Action\".[80] The article was stimulated by a series of informal discussion group meetings, which Parsons and several other colleagues in the spring of 1951 had conducted with philosopher and semiotician Charles W. Morris.[81] His interest in symbolism went hand in hand with his interest in Freud's theory and \"The Superego and the Theory of Social Systems\", written in May 1951 for a meeting of the American Psychiatric Association. The paper can be regarded as the main statement of his own interpretation of Freud,[82] but also as a statement of how Parsons tried to use Freud's pattern of symbolization to structure the theory of social system and eventually to codify the cybernetic hierarchy of the AGIL system within the parameter of a system of symbolic differentiation. His discussion of Freud also contains several layers of criticism that reveal that Parsons' use of Freud was selective rather than orthodox. In particular, he claimed that Freud had \"introduced an unreal separation between the superego and the ego\".\n Parsons was an early subscriber to systems theory. He had early been fascinated by the writings of Walter B. Cannon and his concept of homeostasis[83] as well as the writings of French physiologist Claude Bernard.[84] His interest in systems theory had been further stimulated by his contract with L.J. Henderson. Parsons called the concept of \"system\" for an indispensable master concept in the work of building theoretical paradigms for social sciences.[85] From 1952 to 1957, Parsons participated in an ongoing Conference on System Theory under the chairmanship of Roy R. Grinker, Sr., in Chicago.\n Parsons came into contact with several prominent intellectuals of the time and was particularly impressed by the ideas of social insect biologist Alfred Emerson. Parsons was especially compelled by Emerson's idea that, in the sociocultural world, the functional equivalent of the gene was that of the \"symbol\". Parsons also participated in two of the meetings of the famous Macy Conferences on systems theory and on issues that are now classified as cognitive science, which took place in New York from 1946 to 1953 and included scientists like John von Neumann. Parsons read widely on systems theory at the time, especially works of Norbert Wiener[86] and William Ross Ashby,[87] who were also among the core participants in the conferences. Around the same time, Parsons also benefited from conversations with political scientist Karl Deutsch on systems theory. In one conference, the Fourth Conference of the problems of consciousness in March 1953 at Princeton and sponsored by the Macy Foundation, Parsons would give a presentation on \"Conscious and Symbolic Processes\" and embark on an intensive group discussion which included exchange with child psychologist Jean Piaget.[88]\n Among the other participants were Mary A.B. Brazier, Frieda Fromm-Reichmann, Nathaniel Kleitman, Margaret Mead and Gregory Zilboorg. Parsons would defend the thesis that consciousness is essentially a social action phenomenon, not primarily a \"biological\" one. During the conference, Parsons criticized Piaget for not sufficiently separating cultural factors from a physiologistic concept of \"energy\".\n During the McCarthy era, on April 1, 1952, J. Edgar Hoover, the director of the Federal Bureau of Investigation, received a personal letter from an informant who reported on communist activities at Harvard. During a later interview, the informant claimed that \"Parsons... was probably the leader of an inner group\" of communist sympathizers at Harvard. The informant reported that the old department under Sorokin had been conservative and had \"loyal Americans of good character\" but that the new Department of Social Relations had turned into a decisive left-wing place as a result of \"Parsons's manipulations and machinations\". On October 27, 1952, Hoover authorized the Boston FBI to initiate a security-type investigation on Parsons. In February 1954, a colleague, Stouffer, wrote to Parsons in England to inform him that Stouffer had been denied access to classified documents and that part of the stated reason was that Stouffer knew communists, including Parsons, \"who was a member of the Communist Party\".[89]\n Parsons immediately wrote an affidavit in defense of Stouffer, and he also defended himself against the charges that were in the affidavit: \"This allegation is so preposterous that I cannot understand how any reasonable person could come to the conclusion that I was a member of the Communist Party or ever had been.\"[90] In a personal letter to Stouffer, Parsons wrote, \"I will fight for you against this evil with everything there is in me: I am in it with you to the death.\" The charges against Parsons resulted in Parsons being unable to participate in a UNESCO conference, and it was not until January 1955 that he was acquitted of the charges.\n Since the late 1930s, Parsons had continued to show great interest in psychology and in psychoanalysis. In the academic year of 1955\u20131956, he taught a seminar at Boston Psychoanalytic Society and Institute entitled \"Sociology and Psychoanalysis\". In 1956, he published a major work, Family, Socialization and Interaction Process,[91] which explored the way in which psychology and psychoanalysis bounce into the theories of motivation and socialization, as well into the question of kinship, which for Parsons established the fundamental axis for that subsystem he later would call \"the social community\".\n It contained articles written by Parsons and articles written in collaboration with Robert F. Bales, James Olds, Morris Zelditch Jr., and Philip E. Slater. The work included a theory of personality as well as studies of role differentiation. The strongest intellectual stimulus that Parsons most likely got then was from brain researcher James Olds, one of the founders of neuroscience and whose 1955 book on learning and motivation was strongly influenced from his conversations with Parsons.[92] Some of the ideas in the book had been submitted by Parsons in an intellectual brainstorm in an informal \"work group\" which he had organized with Joseph Berger, William Caudill, Frank E. Jones, Kaspar D. Naegele, Theodore M. Mills, Bengt G. Rundblad, and others. Albert J. Reiss from Vanderbilt University had submitted his critical commentary.\n In the mid-1950s, Parsons also had extensive discussions with Olds about the motivational structure of psychosomatic problems, and at this time Parsons' concept of psychosomatic problems was strongly influenced by readings and direct conversations with Franz Alexander (a psychoanalyst, originally associated with the Berlin Psychoanalytic Institute, who was a pioneer of psychosomatic medicine), Grinker and John Spiegel.[93]\n In 1955, Fran\u00e7ois Bourricaud was preparing a reader of some of Parsons' work for a French audience, and Parsons wrote a preface for the book Au lecteur fran\u00e7ais (To the French Reader); it also went over Bourricaud's introduction very carefully. In his correspondence with Bourricaud, Parsons insisted that he did not necessarily treat values as the only, let alone \"the primary empirical reference point\" of the action system since so many other factors were also involved in the actual historical pattern of an action situation.[94]\n Parsons spent 1957 to 1958 at the Center of Advanced Study in the Behavioral Sciences in Palo Alto, California, where he met for the first time Kenneth Burke; Burke's flamboyant, explosive temperament made a great impression on Parsons, and the two men became close friends.[95] Parsons explained in a letter the impression Burke had left on him: \"The big thing to me is that Burke more than anyone else has helped me to fill a major gap in my own theoretical interests, in the field of the analysis of expressive symbolism.\"\n Another scholar whom Parsons met at the Center of Advanced Studies in the Behavioral Sciences at Palo Alto was Alfred L. Kroeber, the \"dean of American anthropologists\". Kroeber, who had received his PhD at Columbia and who had worked with the Arapaho Indians, was about 81 when Parsons met him. Parsons had the greatest admiration for Kroeber and called him \"my favorite elder statesman\".\n In Palo Alto, Kroeber suggested to Parsons that they write a joint statement to clarify the distinction between cultural and social systems, then the subject of endless debates. In October 1958, Parsons and Kroeber published their joint statement in a short article, \"The Concept of Culture and the Social System\", which became highly influential.[96] Parsons and Kroeber declared that it is important both to keep a clear distinction between the two concepts and to avoid a methodology by which either would be reduced to the other.\n In 1955 to 1956, a group of faculty members at Cornell University met regularly and discussed Parsons' writings. The next academic year, a series of seven widely attended public seminars followed and culminated in a session at which he answered his critics. The discussions in the seminars were summed up in a book edited by Max Black, The Social Theories of Talcott Parsons: A Critical Examination. It included an essay by Parsons, \"The Point of View of the Author\".[97] The scholars included in the volume were Edward C. Devereux Jr., Robin M. Williams Jr., Chandler Morse, Alfred L. Baldwin, Urie Bronfenbrenner, Henry A. Landsberger, William Foote Whyte, Black, and Andrew Hacker. The contributions converted many angles including personality theory, organizational theory, and various methodological discussions. Parsons' essay is particularly notable because it and another essay, \"Pattern Variables Revisited\",[98] both represented the most full-scale accounts of the basic elements of his theoretical strategy and the general principles behind his approach to theory-building when they were published in 1960.\n One essay also included, in metatheoretical terms, a criticism of the theoretical foundations for so-called conflict theory.\n From the late 1950s to the student rebellion in the 1960s and its aftermath, Parsons' theory was criticized by some scholars and intellectuals of the left, who claimed that Parsons's theory was inherently conservative, if not reactionary. Alvin Gouldner even claimed that Parsons had been an opponent of the New Deal. Parsons' theory was further regarded as unable to reflect social change, human suffering, poverty, deprivation, and conflict. Theda Skocpol thought that the apartheid system in South Africa was the ultimate proof that Parsons's theory was \"wrong\".[99]\n At the same time, Parsons' idea of the individual was seen as \"oversocialized\", \"repressive\", or subjugated in normative \"conformity\". In addition, J\u00fcrgen Habermas[100] and countless others were of the belief that Parsons' system theory and his action theory were inherently opposed and mutually hostile and that his system theory was especially \"mechanical\", \"positivistic\", \"anti-individualistic\", \"anti-voluntaristic\", and \"de-humanizing\" by the sheer nature of its intrinsic theoretical context.\n By the same token, his evolutionary theory was regarded as \"uni-linear\", \"mechanical\", \"biologistic\", an ode to world system status quo, or simply an ill-concealed instruction manual for \"the capitalist nation-state\". The first manifestations of that branch of criticism would be intellectuals like Lewis Coser,[101] Ralf Dahrendorf,[102] David Lockwood,[103] John Rex,[104] C. Wright Mills,[105] Tom Bottomore[106] and Gouldner.[107]\n Parsons supported John F. Kennedy on November 8, 1960; from 1923, with one exception, Parsons voted for Democrats all his life.[108] He discussed the Kennedy election widely in his correspondence at the time. Parsons was especially interested in the symbolic implications involved in the fact of Kennedy's Catholic background for the implications for the United States as an integral community (it was the first time that a Catholic had become President of the United States).\n In a letter to Robert N. Bellah, he wrote, \"I am sure you have been greatly intrigued by the involvement of the religious issue in our election.\"[109] Parsons, who described himself as a \"Stevenson Democrat\", was especially enthusiastic that his favored politician, Adlai Stevenson II, had been appointed United States Ambassador to the United Nations. Parsons had supported Stevenson in 1952 and 1956 and was greatly disappointed that Stevenson lost heavily both times.\n In the early 1960s, it became obvious that his ideas had a great impact on much of the theories of modernization at the time. His influence was very extensive but at the same time, the concrete adoption of his theory was often quite selective, half-hearted, superficial, and eventually confused. Many modernization theorists never used the full power of Parsons' theory but concentrated on some formalist formula, which often was taken out of the context that had the deeper meaning with which Parsons originally introduced them.\n In works by Gabriel A. Almond and James S. Coleman, Karl W. Deutsch, S.\u00a0N. Eisenstadt, Seymour Martin Lipset, Samuel P. Huntington, David E. Apter, Lucian W. Pye, Sidney Verba, and Chalmers Johnson, and others, Parsons' influence is clear. Indeed, it was the intensive influence of Parsons' ideas in political sociology that originally got scholar William Buxton interested in his work.[110] In addition, David Easton would claim that in the history of political science, the two scholars who had made any serious attempt to construct a general theory for political science on the issue of political support were Easton and Parsons.[111]\n One of the scholars with whom he corresponded extensively with during his lifetime and whose opinion he highly valued was Robert N. Bellah. Parsons's discussion with Bellah would cover a wide range of topics, including the theology of Paul Tillich.[112] The correspondence would continue when Bellah, in the early fall of 1960, went to Japan to study Japanese religion and ideology. In August 1960, Parsons sent Bellah a draft of his paper on \"The Religious Background of the American Value System\" to ask for his commentary.[113]\n In a letter to Bellah of September 30, 1960, Parsons discussed his reading of Perry Miller's Errand into the Wilderness.[114] Parsons wrote that Miller's discussion of the role of Calvinism \"in the early New England theology... is a first rate and fit beautifully with the broad position I have taken.\"[115] Miller was a literary Harvard historian whose books such as The New England Mind[116] established new standards for the writing of American cultural and religious history. Miller remained one of Parsons' most favoured historians throughout his life. Indeed, religion had always a special place in Parsons' heart, but his son, in an interview, maintained that he that his father was probably not really \"religious.\"\n Throughout his life, Parsons interacted with a broad range of intellectuals and others who took a deep interest in religious belief systems, doctrines, and institutions. One notable person who interacted with Parsons was Marie Augusta Neal, a nun of the Sisters of Notre Dame de Namur who sent Parsons a huge number of her manuscripts and invited him to conferences and intellectual events in her Catholic Church. Neal received her PhD from Harvard under Parsons's supervision in 1963, and she would eventually become professor and then chair of sociology at Emmanuel College.[117]\n Parsons and Winston White cowrote an article, \"The Link Between Character and Society\", which was published in 1961.[118] It was a critical discussion of David Riesman's The Lonely Crowd,[119] which had been published a decade earlier and had turned into an unexpected bestseller, reaching 1 million sold copies in 1977. Riesman was a prominent member of the American academic left, influenced by Erich Fromm and the Frankfurt School. In reality, Riesman's book was an academic attempt to give credit to the concept of \"mass society\" and especially to the idea of an America suffocated in social conformity. Riesman had essentially argued that at the emerging of highly advanced capitalism, the America basic value system and its socializing roles had changed from an \"inner-directed\" toward an \"other-directed\" pattern of value-orientation.\n Parsons and White challenged Riesman's idea and argued that there had been no change away from an inner-directed personality structure. The said that Riesman's \"other-directness\" looked like a caricature of Charles Cooley's looking-glass self,[120] and they argued that the framework of \"institutional individualism\" as the basic code-structure of America's normative system had essentially not changed. What had happened, however, was that the industrialized process and its increased pattern of societal differentiation had changed the family's generalized symbolic function in society and had allowed for a greater permissiveness in the way the child related to its parents. Parsons and White argued that was not the prelude to greater \"otherdirectness\" but a more complicated way by which inner-directed pattern situated itself in the social environment.\n 1963 was a notable year in Parsons's theoretical development because it was the year when he published two important articles: one on political power[121] and one on the concept of social influence.[122] The two articles represented Parsons's first published attempt to work out the idea of Generalized Symbolic Media as an integral part of the exchange processes within the AGIL system. It was a theoretical development, which Parsons had worked on ever since the publication of Economy and Society (1956).\n The prime model for the generalized symbolic media was money and Parsons was reflecting on the question whether the functional characteristics of money represented an exclusive uniqueness of the economic system or whether it was possible to identify other generalized symbolic media in other subsystems as well. Although each medium had unique characteristics, Parsons claimed that power (for the political system) and influence (for the societal community) had institutional functions, which essentially was structurally similar to the general systemic function of money. Using Roman Jakobson's idea of \"code\" and \"message\", Parsons divided the components of the media into a question of value-principle versus coordination standards for the \"code-structure\" and the question of factor versus product control within those social process which carried the \"message\" components. While \"utility\" could be regarded as the value-principle for the economy (medium: money), \"effectiveness\" was the value-principle for the political system (by political power) and social solidarity for the societal community (by social influence). Parsons would eventually choose the concept of value-commitment as the generalized symbolic medium for the fiduciary system with integrity as the value principle.[123]\n In August 1963, Parsons got a new research assistant, Victor Lidz, who would become an important collaborator and colleague. In 1964, Parsons flew to Heidelberg to celebrate the 100th birthday of Weber and discuss Weber's work with Habermas, Herbert Marcuse, and others.[124] Parsons delivered his paper \"Evaluation and Objectivity in Social Science: An Interpretation of Max Weber's Contribution\".[125] The meeting became mostly a clash between pro-Weberian scholars and the Frankfurt School. Before leaving for Germany, Parsons discussed the upcoming meeting with Reinhard Bendix and commented, \"I am afraid I will be something of a Daniel in the Lion's den.\"[126] Bendix wrote back and told Parsons that Marcuse sounded very much like Christoph Steding, a Nazi philosopher.[127]\n Parsons conducted a persistent correspondence with noted scholar Benjamin Nelson,[128] and they shared a common interest in the rise and the destiny of civilizations until Nelson's death in 1977. The two scholars also shared a common enthusiasm for the work of Weber and would generally agree on the main interpretative approach to the study of Weber. Nelson had participated in the Weber Centennial in Heidelberg.\n Parsons was opposed to the Vietnam War but was disturbed by what he considered the anti-intellectual tendency in the student rebellion: that serious debate was often substituted by handy slogans from communists Karl Marx, Mao Zedong and Fidel Castro.[citation needed]\n Nelson got into a violent argument with Herbert Marcuse and accused him of tarnishing Weber.[129] In reading the written version of Nelson's contribution to the Weber Centennial, Parsons wrote, \"I cannot let the occasion pass without a word of congratulations which is strong enough so that if it were concert I should shout bravo.\"[130] In several letters, Nelson would keep Parsons informed of the often-turbulent leftist environment of Marcuse.[131] In the letter of September 1967, Nelson would tell Parsons how much he enjoyed reading Parsons' essay on Kinship and The Associational Aspect of Social Structure.[132] Also, one of the scholars on whose work Parsons and Nelson would share internal commentaries was Habermas.\n Parsons had for years corresponded with his former graduate student David M. Schneider, who had taught at the University of California Berkeley until the latter, in 1960, accepted a position as professor in anthropology at the University of Chicago. Schneider had received his PhD at Harvard in social anthropology in 1949 and had become a leading expert on the American kinship system. Schneider, in 1968, published American Kinship: A Cultural Account[133] which became a classic in the field, and he had sent Parsons a copy of the copyedited manuscript before its publication. Parsons was highly appreciative of Schneider's work, which became in many ways a crucial turning point in his own attempt to understand the fundamental elements of the American kinship system, a key to understanding the factor of ethnicity and especially building the theoretical foundation of his concept of the societal community, which, by the beginning of the early 1970s, had become a strong priority in the number of theoretical projects of his own intellectual life.\n Parsons borrowed the term \"diffuse enduring solidarity\" from Schneider, as a major concept for his own considerations on the theoretical construction of the concept of the societal community. In the spring of 1968, Parsons and Schneider had discussed Clifford Geertz's article on religion as a cultural system[134] on which Parsons wrote a review.[135] Parsons, who was a close friend of Geertz, was puzzled over Geertz's article. In a letter to Schneider, Parsons spoke about \"the rather sharp strictures on what he [Geertz] calls the extremely narrow intellectual tradition with special reference to Weber, but also to Durkheim. My basic point is in this respect, he greatly overstated his case seeming to argue that this intellectual tradition was by now irrelevant.\"[136]\n Schneider wrote back to Parsons, \"So much, so often, as I read Cliff's stuff I cannot get a clear consistent picture of just what the religious system consist in instead only how it is said to work.\"[137]\n In a letter of July 1968 to Gene Tanke of the University of California Press, Parsons offered a critical note on the state of psychoanalytical theory and wrote: \"The use of psychoanalytical theory in interpretation of social and historical subject matter is somewhat hazardous enterprise, and a good deal of nonsense has been written in the name of such attempts.\"[138] Around 1969, Parsons was approached by the prestigious Encyclopedia of the History of Idea about writing an entry in the encyclopedia on the topic of the \"Sociology of Knowledge\". Parsons accepted and wrote one of his most powerful essays, \"The Sociology of Knowledge and the History of Ideas\",[139] in 1969 or 1970. Parsons discussed how the sociology of knowledge, as a modern intellectual discipline, had emerged from the dynamics of European intellectual history and had reached a kind of cutting point in the philosophy of Kant and further explored by Hegel but reached its first \"classical\" formulation in the writing of Mannheim,[140] whose brilliance Parsons acknowledged but disagreed with his German historicism for its antipositivistic epistemology; that was largely rejected in the more positivistic world of American social science. For various reasons, the editors of the encyclopedia turned down Parsons' essay, which did not fit the general format of their volume. The essay was not published until 2006.[141]\n Parsons had several conversations with Daniel Bell on a \"post-industrial society\", some of which were conducted over lunch at William James Hall. After reading an early version of Bell's magnum opus, The Coming of the Post-Industrial Society, Parsons wrote a letter to Bell, dated November 30, 1971, to offer his criticism. Among his many critical points, Parsons stressed especially that Bell's discussion of technology tended to \"separate off culture\" and treat the two categories \"as what I would call culture minus the cognitive component\".\n Parsons' interest in the role of ethnicity and religion in the genesis of social solidarity within the local community heavily influenced another of his early 1960s graduate students, Edward Laumann. As a student, Laumann was interested in the role of social network structure in shaping community-level solidarity. Combining Parsons' interest in the role of ethnicity in shaping local community solidarity with W. Lloyd Warner's structural approach to social class, Laumann argued that ethnicity, religion, and perceived social class all play a large role in structuring community social networks.[142][143][144] Laumann's work found that community networks are highly partitioned along lines of ethnicity, religion, and occupational social status. It also highlighted the tension individuals experience between their preference to associate with people who are like them (homophily) and their simultaneous desire to affiliate with higher-status others. Later, at the beginning of his career at the University of Chicago, Laumann would argue that how the impulses are resolved by individuals forms the basis of corporate or competitive class consciousness within a given community.[145] In addition to demonstrating how community solidarity can be conceptualized as a social network and the role of ethnicity, religion, and class in shaping such networks, Laumann's dissertation became one of the first examples of the use of population-based surveys in the collection of social network data, and thus a precursor to decades of egocentric social network analysis.[146] Parsons thus played an important role in shaping the early interest of social network analysis in homophily and the use of egocentric network data to assess group- and community-level social network structures.\n In his later years, Parsons became increasingly interested in working out the higher conceptual parameters of the human condition, which was in part what led him toward rethinking questions of cultural and social evolution and the \"nature\" of telic systems, the latter which he especially discussed with Bellah, Lidz, Fox, Willy de Craemer, and others. Parsons became increasingly interested in clarifying the relationship between biological and social theory. Parsons was the initiator of the first Daedalus conference on \"Some Relations between Biological and Social Theory\", sponsored by the American Academy of Arts and Sciences. Parsons wrote a memorandum dated September 16, 1971, in which he spelled out the intellectual framework for the conference. As Parsons explained in the memo, the basic goal of the conference was to establish a conceptual fundament for a theory of living systems. The first conference was held on January 7, 1972. Among the participants beside Parsons and Lidz were Ernst Mayr, Seymour Kety, Gerald Holton, A. Hunter Dupree, and William K. Wimsatt. A second Daedalus Conference on Living Systems was held on March 1\u20132, 1974 and included Edward O. Wilson, who was about to publish his famous work on sociobiology. Other new participants were John T. Bonner, Karl H. Pribram, Eric Lennenberg, and Stephen J. Gould.\n Parsons began in the fall of 1972 to conduct a seminar on \"Law and Sociology\" with legal philosopher Lon L. Fuller, well known for his book The Morality of Law (1964). The seminar and conversations with Fuller stimulated Parsons to write one of his most influential articles, \"Law as an Intellectual Stepchild\".[147] Parsons discuses Roberto Mangabeira Unger's Law in Modern Society (1976). Another indication of Parsons' interest in law was reflected in his students, such as John Akula, who wrote his dissertation in sociology, Law and the Development of Citizenship (1973). In September 1972, Parsons participated in a conference in Salzburg on \"The Social Consequences of Modernization in Socialist Countries\". Among the other participants were Alex Inkeles, Ezra Vogel, and Ralf Dahrendorf.\n In 1972, Parsons wrote two review articles to discuss the work of Bendix, which provide a clear statement on Parsons' approach to the study of Weber. Bendix had become well known for his interpretations of Weber. In the first review article, Parsons analyzed the immigrant Bendix's Embattled Reason,[148] and he praised its attempt to defend the basic values of cognitive rationality, which he unconditionally shared, and he agreed with Bendix that the question of cognitive rationality was primarily a cultural issue, not a category that could be reduced from biological, economic, and social factors. However, Parsons criticized how Bendix had proceeded, who he felt especially had misrepresented the work of Freud and Durkheim. Parsons found that the misrepresentation was how Bendix tended to conceive the question of systematic theorizing, under the concept of \"reductionism\".[149] Parsons further found that Bendix's approach suffered from a \"conspicuous hostility\" to the idea of evolution. Although Parsons assessed that Weber rejected the linear evolutionary approaches of Marx and Herbert Spencer, Weber might not have rejected the question of evolution as a generalized question.\n In a second article, a review of Bendix and Guenther Roth's Scholarship and Partisanship: Essays on Max Weber,[150] Parsons continued his line of criticism. Parsons was especially concerned with a statement by Bendix that claimed Weber believed Marx's notion that ideas were \"the epiphenomena of the organization of production\". Parsons strongly rejected that interpretation: \"I should contend that certainly the intellectual 'mature' Weber never was an 'hypothetical' Marxist.\"[151] Somewhere behind the attitudes of Bendix, Parsons detected a discomfort for the former to move out of an \"idiographic\" mode of theorizing.\n In 1973, Parsons published The American University, which he had authored with Gerald M. Platt.[152] The idea had originally emerged when Martin Meyerson and Stephen Graubard of the American Academy of Arts and Sciences, in 1969, asked Parsons to undertake a monographic study of the American university system. The work on the book went on for years until it was finished in June 1972.\n From a theoretical point of view, the book had several functions. It substantiated Parsons' concept of the educational revolution, a crucial component in his theory of the rise of the modern world. What was equally intellectually compelling, however, was Parsons' discussion of \"the cognitive complex\", aimed at explaining how cognitive rationality and learning operated as an interpenetrative zone on the level of the general action-system in society. In retrospect, the categories of the cognitive complex are a theoretical foundation to understand what has been called the modern knowledge-based society.\n He officially retired from Harvard in 1973 but continued his writing, teaching, and other activities in the same rapid pace as before. Parsons also continued his extensive correspondence with a wide group of colleagues and intellectuals. He taught at the University of Pennsylvania, Brown University, Rutgers University, the University of Chicago, and the University of California at Berkeley. At Parsons' retirement banquet, on May 18, 1973, Robert K. Merton was asked to preside, while John Riley, Bernard Barber, Jesse Pitts, Neil J. Smelser, and John Akula were asked to share their experiences of the man with the audience.\n One scholar who became important in Parsons' later years was professor Martin U. Martel, of Brown University. They had made contact in the early 1970s at a discussion of an article that Martel had written about Parsons' work.[153] Martel arranged a series of seminars at Brown University in 1973 to 1974, and Parsons spoke about his life and work and answered questions from students and faculty.[154] Among the participants at the seminars were Martel, Robert M. Marsh, Dietrich Rueschemeyer, C. Parker Wolf, Albert F. Wessen, A. Hunter Dupree, Philip L. Quinn, Adrian Hayes and Mark A. Shields. In February to May 1974, Parsons also gave the Culver Lectures at Brown and spoke on \"The Evolution of Society\". The lectures and were videotaped.\n Late in life, Parsons began to work out a new level of the AGIL model, which he called \"A Paradigm of the Human Condition\".[155] The new level of the AGIL model crystallized in the summer of 1974. He worked out the ideas of the new paradigm with a variety of people but especially Lidz, Fox and Harold Bershady. The new metaparadigm featured the environment of the general action system, which included the physical system, the biological system, and what Parsons called the telic system. The telic system represents the sphere of ultimate values in a sheer metaphysical sense. Parsons also worked toward a more comprehensive understanding of the code-structure of social systems[156] and on the logic of the cybernetic pattern of control facilitating the AGIL model. He wrote a bulk of notes: two being \"Thoughts on the Linking of Systems\" and \"Money and Time\".[157] He had also extensive discussions with Larry Brownstein and Adrian Hayes on the possibility of a mathematical formalization of Parsons' theory.[158]\n Parsons had worked intensively with questions of medical sociology, the medical profession, psychiatry, psychosomatic problems, and the questions of health and illness. Most of all Parsons had become known for his concept of \"the Sick role\". The last field of social research was an issue that Parsons constantly developed through elaboration and self-criticism. Parsons participated at the World Congress of Sociology in Toronto in August 1974 at which he presented a paper, \"The Sick Role Revisited: A Response to Critics and an Updating in Terms of the Theory of Action\", which was published under a slightly different title, \"The Sick Role and the Role of the Physician Reconsidered\", in 1975.[159] In this essay, Parsons highlighted that his concept of \"sick role\" never was meant to be confined to \"deviant behavior\", but \"its negative valuation should not be forgotten\". It was also important to keep a certain focus on the \"motivatedness\" of illness, since there is always a factor of unconscious motivation in the therapeutic aspects of the sick role.\n In 1975, Bellah published The Broken Covenant.[160] Bellah referred to the sermon delivered by John Winthrop (1587\u20131649) to his flock on the ship Arbella on the evening of the landing in Massachusetts Bay in 1630. Winthrop declared that the Puritan colonists' emigration to the New World was part of a covenant, a special pact with God, to create a holy community and noted: \"For we must consider that we shall be a city on the hill. The eyes of all people are upon us.\" Parsons disagreed strongly with Bellah's analysis and insisted that the covenant was not broken. Parsons later used much of his influential article, \"Law as an Intellectual Stepchild\",[161] to discuss Bellah's position.\n Parsons thought that Bellah trivialized the tensions of individual interests and society's interests by reducing them to \"capitalism\"; Bellah, in his characterization of the negative aspects of American society, was compelled by a charismatic-based optimalism moral absolutism.\n In 1975, Parsons responded to an article by Jonathan H. Turner, \"Parsons as a Symbolic Interactionist: A Comparison of Action and Interaction Theory\".[162] Parsons acknowledged that action theory and symbolic interactionism should not be regarded as two separate, antagonistic positions but have overlapping structures of conceptualization.[163] Parsons regarded symbolic interactionism and the theory of George Herbert Mead as valuable contributions to action theory that specify certain aspects of the theory of the personality of the individual. Parsons, however, criticized the symbolic interactionism of Herbert Blumer since Blumer's theory had no end to the openness of action. Parsons regarded Blumer as the mirror image of Claude L\u00e9vi-Strauss,[164] who tended to stress the quasi-determined nature of macro-structural systems. Action theory, Parsons maintained, represented a middle ground between both extremes.\n In 1976, Parsons was asked to contribute to a volume to celebrate the 80th birthday of Jean Piaget. Parsons contributed with an essay, \"A Few Considerations on the Place of Rationality in Modern Culture and Society\". Parsons characterized Piaget as the most eminent contributor to cognitive theory in the 20th century. However, he also argued that the future study of cognition had to go beyond its narrow encounter with psychology to aim at a higher understanding of how cognition as a human intellectual force was entangled in the processes of social and cultural institutionalization.[165]\n In 1978, when James Grier Miller published his famous work Living Systems,[166] Parsons was approached by Contemporary Sociology to write a review article on Miller's work. Parsons had already complained in a letter to A. Hunter Dupree[167] that American intellectual life suffered from a deep-seated tradition of empiricism and saw Miller's book the latest confirmation of that tradition. In his review, \"Concrete Systems and \"Abstracted Systems\",[168] he generally praised the herculean task behind Miller's work but criticized Miller for getting caught in the effort of hierarchize concrete systems but underplay the importance of structural categories in theory building. Parsons also complained about Miller's lack of any clear distinction between cultural and non-cultural systems.\n Japan had long been a keen interest in Parsons' work. As early as 1958, a Japanese translation of Economy and Society appeared. Also, The Structure of Social Action was translated into Japanese.[169] The Social System was translated into Japanese by Tsutomu Sato in 1974. Indeed, Ryozo Takeda had, as early as 1952 in his Shakaigaku no Kozo (\"The Framework of Sociology\") introduced Japanese scholars to some of Parsons' ideas. Parsons had visited Japan for the first time in 1972 and he gave a lecture on November 25 to the Japanese Sociological Association, \"Some Reflections on Post-Industrial Society\" that was published in The Japanese Sociological Review.[170] At the same time, Parsons participated in an international symposium on \"New Problems of Advanced Societies\", held in Tokyo, and he wrote short articles written that appeared in the proceedings of the symposium.[171][172] Tominaga, born in 1931, a leading figure in Japanese sociology and a professor at the University of Tokyo, was asked by Lidz to contribute to a two-volume collection of essays to honor Parsons. Tominaga wrote an essay on the industrial growth model of Japan and used Parsons' AGIL model.[173]\n In 1977, Washio Kurata, the new dean of the Faculty of Sociology of Kwansei Gakuin University, wrote to Parsons and invited him to visit Japan during the 1978\u20131979 academic year. In early spring, Parsons accepted the invitation, and on October 20, 1978, Parsons arrived at the Osaka Airport, accompanied by his wife, and was greeted by a large entourage.\n Parsons began weekly lectures at Kwansei's sociology department from October 23 to December 15. Parsons gave his first public lecture to a huge mass of undergraduates, \"The Development of Contemporary Sociology\".\n On November 17\u201318, when the Sengari Seminar House was opened, Parsons was invited as the key speaker at the event and gave two lectures, \"On the Crisis of Modern Society\"[174] and \"Modern Society and Religion\".[175] Present were Tominaga, Mutsundo Atarashi, Kazuo Muto, and Hideichiro Nakano.\n On November 25, lectures at Kobe University were organized by Hiroshi Mannari. Parsons lectured on organization theory to the faculty and the graduate students from the Departments of Economics, Management and Sociology. Also, faculty members from Kyoto and Osaka universities were present. A text was published the next year.[176] On November 30 to December 1, Parsons participated in a Tsukuba University Conference in Tokyo; Parsons spoke on \"Enter the New Society: The Problem of the Relationship of Work and Leisure in Relation to Economic and Cultural Values\".[177]\n On December 5, Parsons gave a lecture at Kyoto University on \"A Sociologist Looks at Contemporary U.S. Society\".[178]\n At a special lecture at Osaka on December 12, Parsons spoke, at the suggestion of Tominaga, on \"Social System Theory and Organization Theory\" to the Japanese Sociological Association.\n On December 14, Kwansei Gakuin University granted Parsons an honorary doctor degree. Some of his lectures would be collected into a volume by Kurata and published in 1983. The Parsons flew back to the US in mid-December 1978.\n Parsons died May 8, 1979, in Munich on a trip to Germany, where he was celebrating the 50th anniversary of his degree at Heidelberg. The day before, he had given a lecture on social class to an audience of German intellectuals, including Habermas, Niklas Luhmann and Wolfgang Schluchter.\n Parsons produced a general theoretical system for the analysis of society, which he called \"theory of action\", based on the methodological and epistemological principle of \"analytical realism\" and on the ontological assumption of \"voluntaristic action\".[179] Parsons' concept of analytical realism can be regarded as a kind of compromise between nominalist and realist views on the nature of reality and human knowledge.[180] Parsons believed that objective reality can be related to only by a particular encounter of such reality and that general intellectual understanding is feasible through conceptual schemes and theories. Interaction with objective reality on an intellectual level should always be understood as an approach. Parsons often explained the meaning of analytical realism by quoting a statement by Henderson: \"A fact is a statement about experience in terms of a conceptual scheme.\"[181]\n Generally, Parsons maintained that his inspiration regarding analytical realism had been Lawrence Joseph Henderson and Alfred North Whitehead[182] although he might have gotten the idea much earlier. It is important for Parsons' \"analytical realism\" to insist on the reference to an objective reality since he repeatedly highlighted that his concept of \"analytical realism\" was very different from the \"fictionalism\" of Hans Vaihiger (Hans Vaihinger):[183]\n We must start with the assertion that all knowledge which purports to be valid in anything like the scientific sense presumes both the reality of object known and of a knower. I think we can go beyond that and say that there must be a community of knowers who are able to communicate with each other. Without such a presupposition it would seem difficult to avoid the pitfall of solipsism. The so-called natural sciences do not, however, impute the \"status of knowing subjects\" to the objects with which they deal.[184] The Structure of Social Action (SSA), Parsons' most famous work, took form piece by piece. Its central figure was Weber, and the other key figures in the discussion were added, little by little, as the central idea took form. One important work that helped Parsons' central argument in was, in 1932, unexpectedly found: \u00c9lie Hal\u00e9vy's La formation du radicalisme philosophique (1901\u20131904); he read the three-volume work in French. Parsons explained, \"Well, Hal\u00e9vy was just a different world\u00a0... and helped me to really get in to many clarifications of the assumptions distinctive to the main line of British utilitarian thought; assumptions about the 'natural identity of interest', and so on. I still think it is one of the true masterpieces in intellectual history.\"[36] Parsons first achieved significant recognition with the publication of The Structure of Social Action (1937), his first grand synthesis, combining the ideas of Durkheim, Weber, Pareto, and others. In 1998, the International Sociological Association listed it as the ninth most important sociological book of the 20th Century.[185]\n Parsons' action theory can be characterized as an attempt to maintain the scientific rigour of positivism while acknowledging the necessity of the \"subjective dimension\" of human action incorporated in hermeneutic types of sociological theories. It is cardinal in Parsons' general theoretical and methodological view that human action must be understood in conjunction with the motivational component of the human act. Social science must consider the question of ends, purpose, and ideals in its analysis of human action. Parsons' strong reaction to behavioristic theory as well as to sheer materialistic approaches derives from the attempt of the theoretical positions to eliminate ends, purpose, and ideals as factors of analysis. Parsons, in his term papers at Amherst, was already criticizing attempts to reduce human life to psychological, biological, or materialist forces. What was essential in human life, Parsons maintained, was how the factor of culture was codified. Culture, however, was to Parsons an independent variable in that it could not be \"deducted\" from any other factor of the social system. That methodological intention is given the most elaborate presentation in The Structure of Social Action, which was Parsons' first basic discussion of the methodological foundation of the social sciences.\n Some of the themes in The Structure of Social Action had been presented in a compelling essay two years earlier in \"The Place of Ultimate Values in Sociological Theory\".[186]\n An intense correspondence and dialogue between Talcott Parsons and Alfred Schutz serves to highlight the meaning of central concepts in The Structure of Social Action.\n Parsons developed his ideas during a period when systems theory and cybernetics were very much on the front burner of social and behavioral science. In using systems thinking, he postulated that the relevant systems treated in social and behavioral science were \"open:\" they were embedded in an environment with other systems. For social and behavioral science, the largest system is \"the action system,\" the interrelated behaviors of human beings, embedded in a physical-organic environment.[187]\n As Parsons developed his theory, it became increasingly bound to the fields of cybernetics and system theory but also to Emerson's concept of homeostasis[188] and Ernst Mayr's concept of \"teleonomic processes\".[189] On the metatheoretical level, Parson attempted to balance psychologist phenomenology and idealism on the one hand and pure types of what Parsons called the utilitarian-positivistic complex, on the other hand.\n The theory includes a general theory of social evolution and a concrete interpretation of the major drives of world history. In Parsons' theory of history and evolution, the constitutive-cognitive symbolization of the cybernetic hierarchy of action-systemic levels has, in principle, the same function as genetic information in DNA's control of biological evolution, but that factor of metasystemic control does not \"determine\" any outcome but defines the orientational boundaries of the real pathfinder, which is action itself. Parsons compares the constitutive level of society with Noam Chomsky's concept of \"deep structure\".\n As Parsons wrote, \"The deep structures do not as such articulate any sentences which could convey coherent meaning. The surface structures constitute the level at which this occurs. The connecting link between them is a set of rules of transformation, to use Chomsky's own phrase.\"[190] The transformative processes and entities are generally, at least on one level of empirical analysis, performed or actualized by myths and religions,[191] but philosophies, art systems, or even semiotic consumer behavior can, in principle, perform that function.[192]\n Parsons' theory reflects a vision of a unified concept of social science and indeed of living systems[193] in general. His approach differs in essence from Niklas Luhmann's theory of social systems because Parsons rejects the idea that systems can be autopoietic, short of the actual action system of individual actors. Systems have immanent capacities but only as an outcome of the institutionalized processes of action-systems, which, in the final analysis, is the historical effort of individual actors. While Luhmann focused on the systemic immanence, Parsons insisted that the question of autocatalytic and homeostatic processes and the question about the actor as the ultimate \"first mover\" on the other hand was not mutually exclusive. Homeostatic processes might be necessary if and when they occur but action is necessitating.\n It is only that perspective of the ultimate reference in action that Parsons' dictum (that higher-order cybernetic systems in history will tend to control social forms that are organized on the lower levels of the cybernetic hierarchy) should be understood. For Parsons, the highest levels of the cybernetic hierarchy as far as the general action level is concerned is what Parsons calls the constitutive part of the cultural system (the L of the L). However, within the interactional processes of the system, attention should be paid especially to the cultural-expressivistic axis (the L-G line in the AGIL). By the term constitutive, Parsons generally referred to very highly codified cultural values especially religious elements (but other interpretation of the term \"constitutive\" is possible).[194]\n Cultural systems have an independent status from that of the normative and orientational pattern of the social system; neither system can be reduced to the other. For example, the question of the \"cultural capital\" of a social system as a sheer historical entity (in its function as a \"fiduciary system\"), is not identical to the higher cultural values of that system; that is, the cultural system is embodied with a metastructural logic that cannot be reduced to any given social system or cannot be viewed as a materialist (or behavioralist) deduction from the \"necessities\" of the social system (or from the \"necessities\" of its economy).[195] Within that context, culture would have an independent power of transition, not only as factors of actual sociocultural units (like Western civilization) but also how original cultural bases would tend to \"universalize\" through interpenetration and spread over large numbers of social systems as with Classical Greece and Ancient Israel, where the original social bases had died but the cultural system survived as an independently \"working\" cultural pattern, as in the case of Greek philosophy or in the case of Christianity, as a modified derivation from its origins in Israel.[196]\n It is important to highlight that Parsons distinguished two \"meanings\" or modes of the term general theory. He sometimes wrote about general theory as aspects of theoretical concerns of social sciences whose focus is on the most \"constitutive\" elements of cognitive concern for the basic theoretical systematization of a given field. Parsons would include the basic conceptual scheme for the given field, including its highest order of theoretical relations and naturally also the necessary specification of this system's axiomatic, epistemological, and methodological foundations from the point of view of logical implications.[197][198] All the elements would signify the quest for a general theory on the highest level of theoretical concern.\n However, general theory could also refer to a more fully/operational system whose implications of the conceptual scheme were \"spelled out\" on lower levels of cognitive structuralization, levels standing closer to a perceived \"empirical object\". In his speech to the American Sociological Society in 1947, he spoke of five levels:[199]\n During his life, he would work on developing all five fields of theoretical concerns but pay special attention to the development on the highest \"constitutive\" level, as the rest of the building would stand or fall on the solidity of the highest level.[200]\n Despite myths, Parsons never thought that modern societies exist in some kind of perfect harmony with their norms or that most modern societies were necessarily characterized by some high level of consensus or a \"happy\" institutional integration. Parsons highlighted that is almost logically impossible that there can be any \"perfect fit\" or perfect consensus in the basic normative structure of complex modern societies because the basic value pattern of modern societies is generally differentiated in such a way that some of the basic normative categories exist in inherent or at least potential conflict with each other. For example, freedom and equality are generally viewed as fundamental and non-negotiable values of modern societies. Each represents a kind of ultimate imperative about what the higher values of humanity. However, as Parsons emphasizes, no simple answer on the priority of freedom or equality or any simple solution on how they possibly can be mediated, if at all. Therefore, all modern societies are faced with the inherent conflict prevailing between the two values, and there is no \"eternal solution\" as such. There cannot be any perfect match between motivational pattern, normative solutions, and the prevailing value pattern in any modern society. Parsons also maintained that the \"dispute\" between \"left\" and \"right\" has something to do with the fact that they both defend ultimately \"justified\" human values (or ideals), which alone is indispensable as values but are always in an endless conflictual position to each other.\n Parsons always maintained that the integration of the normative pattern in society is generally problematic and that the level of integration that is reached in principle is always far from harmonious and perfect. If some \"harmonious pattern\" emerges, it is related to specific historical circumstances but is not a general law of the social systems.\n The heuristic scheme that Parsons used to analyze systems and subsystems is called the AGIL paradigm or the AGIL scheme.[201] To survive or maintain equilibrium with respect to its environment, any system must to some degree:\n The concepts can be abbreviated as AGIL and are called the system's functional imperatives. Parsons' AGIL model is an analytical scheme for the sake of theoretical \"production\", but it is not any simple \"copy\" or any direct historical \"summary\" of empirical reality. Also, the scheme itself does not explain \"anything\", just as the periodic table explains nothing by itself in the natural sciences. The AGIL scheme is a tool for explanations and is no better than the quality of the theories and explanation by which it is processed.\n In the case of the analysis of a social action system, the AGIL paradigm, according to Parsons, yields four interrelated and interpenetrating subsystems: the behavioral systems of its members (A), the personality systems of those members (G), the social system (as such) (I), and the cultural system of that society (L). To analyze a society as a social system (the I subsystem of action), people are posited to enact roles associated with positions. The positions and roles become differentiated to some extent and, in a modern society, are associated with things such as occupational, political, judicial, and educational roles.\n Considering the interrelation of these specialized roles as well as functionally differentiated collectivities (like firms and political parties), a society can be analyzed as a complex system of interrelated functional subsystems:\n The pure AGIL model for all living systems:\n The Social System Level:\n The General Action Level:\n The cultural level:\n The Generalized Symbolic media:\n Social System level:\n Parsons elaborated upon the idea that each of these systems also developed some specialized symbolic mechanisms of interaction analogous to money in the economy, like influence in the social community. Various processes of \"interchange\" among the subsystems of the social system were postulated.\n Parsons' use of social systems analysis based on the AGIL scheme was established in his work Economy and Society (with N. Smelser, 1956) and prevailed in all his subsequent work. However, the AGIL system existed only in a \"rudimentary\" form in the beginning and was gradually elaborated and expanded in the decades which followed. A brief introduction to Parsons' AGIL scheme appears in Chapter 2 of The American University.[202]\nThere is, however, no single place in his writing in which the total AGIL system is visually displayed or explained: the complete system has to be reconstructed from multiple places in his writing. The system displayed in \"The American University\" has only the most basic elements and should not be mistaken for the whole system.\n Parsons contributed to social evolutionism and neoevolutionism. He divided evolution into four sub-processes:\n Furthermore, Parsons explored the sub-processes within three stages of evolution:\n Parsons viewed Western civilization as the pinnacle of modern societies and the United States as the one that is most dynamically developed.\n Parsons' late work focused on a new theoretical synthesis around four functions that he claimed are common to all systems of action, from the behavioral to the cultural, and a set of symbolic media that enables communication across them. His attempt to structure the world of action according to a scheme that focused on order was unacceptable for American sociologists, who were retreating from the grand pretensions of the 1960s to a more empirical, grounded approach.\n Parsons asserted that there are not two dimensions to societies (instrumental and expressive) but that there are qualitative differences between kinds of social interaction.\n He observed that people can have personalized and formally detached relationships, based on the roles that they play. The pattern variables are what he called the characteristics that are associated with each kind of interaction.\n An interaction can be characterized by one of the identifiers of each contrastive pair:\n From the 1940s to the 1970s, Parsons was one of the most famous and most influential but also most controversial sociologists in the world, particularly in the US.[18] His later works were met with criticism and were generally dismissed in the 1970s by the view that his theories were too abstract, inaccessible, and socially conservative.[18][203]\n Recently, interest has increased in Parsons' ideas and especially often-overlooked later works.[17] Attempts to revive his thinking have been made by Parsonsian sociologists and social scientists like Jeffrey Alexander, Bryan Turner, Richard M\u00fcnch, and Roland Robertson, and Uta Gerhardt has written about Parsons from a biographical and historical perspective. In addition to the United States, the key centers of interest in Parsons today are Germany, Japan, Italy, and the United Kingdom.[citation needed]\n Parsons had a seminal influence and early mentorship of many American and international scholars, such as Ralf Dahrendorf, Alain Touraine, Niklas Luhmann, and Habermas.[citation needed] His best-known pupil was Merton.[18] Parsons was a member of the American Philosophical Society.[204]\n In 1930 Parson's published a translation of Weber's classic work The Protestant Ethic and the Spirit of Capitalism\n",
      "timestamp": "2025-10-09 19:12:45.296420"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Stuart_Umpleby",
      "text": "Stuart Anspach Umpleby (born March 5, 1944) is an American cybernetician and professor in the Department of Management and Director of the Research Program in Social and Organizational Learning in the School of Business at the George Washington University.\n Umpleby attended the University of Illinois Urbana-Champaign where he received degrees in engineering in 1967, in political science in 1967 and in 1969, and a PhD in communications in 1975.\n In the 1960s, while a student at the University of Illinois, Umpleby worked in the Institute of Communications Research, The Biological Computer Laboratory, and the Computer-based Education Research Laboratory, the PLATO system.[1] From 1967 to 1975 he and other students developed computer conferencing systems and other applications for time shared computers.[2]\n After moving to George Washington University, he was the moderator from 1977 to 1980, of a computer conference on general systems theory supported by the National Science Foundation.[3] Between 1982 and 1988 he arranged scientific meetings involving American and Soviet scientists in the area of cybernetics and general systems theory.[4]\n From 1975 to present he has been a professor in the Department of Management at The George Washington University, where he teaches courses ranging from cybernetics, systems theory, and system dynamics to the philosophy of science, cross-cultural management, and organizational behavior. From 1994 to 1997 he was the faculty facilitator of a Quality and Innovation Initiative in the GW School of Business.[5]\n He is a past president of the American Society for Cybernetics (ASC). In 2007 Stuart Umpleby was awarded The Wiener Gold Medal of the American Society for Cybernetics.[6]\nIn 2010 he was elected an Academician in the International Academy for Systems and Cybernetic Sciences,[7]  an honor society created by the International Federation for Systems Research.\nHe is twice divorced and has two sons.\n Umpleby's research interests are in the fields of cybernetics and systems theory, the philosophy of science, and management methods.  Other interests have been demography, the year 2000 computer crisis, academic globalization, and the transitions in the post-communist countries.\n In the early 1970s Umpleby studied cybernetics with Heinz von Foerster and Ross Ashby in the Biological Computer Laboratory at the University of Illinois Urbana-Champaign.  With Heinz von Foerster and Leo Steg he organized the first Gordon Research Conference on cybernetics in 1984. He worked to develop and promote second-order cybernetics or biological cybernetics.  He also helped to create social cybernetics.[8]  He provided an example of the amplification of management capability.[9] He clarified the nature of information in descriptions of the physical relationships among matter, energy, and information.[10]\nAnd he has pointed out that George Soros's reflexivity theory is quite compatible with cybernetics.[11]\n Following his work on biological cybernetics and social cybernetics Umpleby suggested a way of unifying the philosophies of realism, constructivism, and pragmatism by combining world, description, and observer.[12]\nBuilding on the work of E.A. Singer, Jr., C. West Churchman, and Russell L. Ackoff, Umpleby has suggested that, since managers are part of the system they seek to influence, methods rather than theories are more effective ways to present knowledge of management.[13]\n Umpleby recently has worked to further develop the Quality Improvement Priority Matrix, a method for determining priorities for improvement and for monitoring perceived improvement.[14]\n In 1960 Heinz von Foerster published an article in Science showing that if demographic trends of the past two millennia continue, world population would go to infinity in approximately 2026.  Although contested in the 1960s, the equation proved remarkably accurate, indeed even conservative, until the early 1990s.[15]\nDiscussions of the doomsday equation revealed that demographers and natural scientists have fundamentally different ways of dealing with estimates and that these differences are not generally known by the public, science journalists, or other scientists.[16]\n From 1997 to 2000 Umpleby worked on the Year 2000 Computer Problem, viewing it as an opportunity to test social science theories using a before and after research design.[17]\n Between 1977 and 1980 he was the moderator of a computer conference on general systems theory supported by the National Science Foundation.  This project was one of nine \"experimental trials of electronic information exchange for small research communities\".  About sixty scientists in the United States, Canada, and Europe interacted for a period of two and a half years using the Electronic Information Exchange System (EIES) located at New Jersey Institute of Technology.[18]\nContinuing the work with computer-based communications media, Umpleby has experimented with applications of the internet.  Currently he is developing the idea of academic globalization, since it is now possible for academics to collaborate via the internet with colleagues in foreign countries for purposes of education, research or community service.[19]\n With the collapse of communism in 1989 many social scientists both in Russia and the West said that, although Karl Marx had described the transition from capitalism to socialism to communism, there were no theories to guide the transition from communism to capitalism.  Umpleby refuted this claim by organizing meetings in 1990 in both Washington, DC, and Vienna, Austria, to discuss the theories of economic, political, and social development that can guide the transformation of socialist societies.[20]\n Since 1994 the Research Program in Social and Organizational Learning at The George Washington University, which Umpleby heads, has hosted over 150 visiting scholars supported by the U.S. Department of State.  Most of these scholars have come from the former Soviet Union and Southeast Europe.  While on campus the scholars work with professors in their fields.  They also learn process improvement and group facilitation methods, so they can be more effective in introducing changes when they return home. In this way Umpleby has experimented with ways to encourage the use of participatory methods in other countries.[21]  He has found that the Participatory Strategic Planning methods developed by the Institute of Cultural Affairs not only improve the effectiveness of organizations but also lead to more humane management practices and build mutual trust among the participants.[22]\n Umpleby has been an Academician in the International Academy for Systems and Cybernetic Sciences,[23] an honor society created by the International Federation for Systems Research, since 2010.[24]\n Stuart Umpleby has written numerous articles, edited several special issues of the journal Cybernetics and Systems, and edited two books: \n",
      "timestamp": "2025-10-09 19:12:45.679416"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Stuart_Umpleby",
      "title": "Stuart Umpleby - Wikipedia",
      "description": "",
      "text": "Stuart Anspach Umpleby (born March 5, 1944) is an American cybernetician and professor in the Department of Management and Director of the Research Program in Social and Organizational Learning in the School of Business at the George Washington University.\n Umpleby attended the University of Illinois Urbana-Champaign where he received degrees in engineering in 1967, in political science in 1967 and in 1969, and a PhD in communications in 1975.\n In the 1960s, while a student at the University of Illinois, Umpleby worked in the Institute of Communications Research, The Biological Computer Laboratory, and the Computer-based Education Research Laboratory, the PLATO system.[1] From 1967 to 1975 he and other students developed computer conferencing systems and other applications for time shared computers.[2]\n After moving to George Washington University, he was the moderator from 1977 to 1980, of a computer conference on general systems theory supported by the National Science Foundation.[3] Between 1982 and 1988 he arranged scientific meetings involving American and Soviet scientists in the area of cybernetics and general systems theory.[4]\n From 1975 to present he has been a professor in the Department of Management at The George Washington University, where he teaches courses ranging from cybernetics, systems theory, and system dynamics to the philosophy of science, cross-cultural management, and organizational behavior. From 1994 to 1997 he was the faculty facilitator of a Quality and Innovation Initiative in the GW School of Business.[5]\n He is a past president of the American Society for Cybernetics (ASC). In 2007 Stuart Umpleby was awarded The Wiener Gold Medal of the American Society for Cybernetics.[6]\nIn 2010 he was elected an Academician in the International Academy for Systems and Cybernetic Sciences,[7]  an honor society created by the International Federation for Systems Research.\nHe is twice divorced and has two sons.\n Umpleby's research interests are in the fields of cybernetics and systems theory, the philosophy of science, and management methods.  Other interests have been demography, the year 2000 computer crisis, academic globalization, and the transitions in the post-communist countries.\n In the early 1970s Umpleby studied cybernetics with Heinz von Foerster and Ross Ashby in the Biological Computer Laboratory at the University of Illinois Urbana-Champaign.  With Heinz von Foerster and Leo Steg he organized the first Gordon Research Conference on cybernetics in 1984. He worked to develop and promote second-order cybernetics or biological cybernetics.  He also helped to create social cybernetics.[8]  He provided an example of the amplification of management capability.[9] He clarified the nature of information in descriptions of the physical relationships among matter, energy, and information.[10]\nAnd he has pointed out that George Soros's reflexivity theory is quite compatible with cybernetics.[11]\n Following his work on biological cybernetics and social cybernetics Umpleby suggested a way of unifying the philosophies of realism, constructivism, and pragmatism by combining world, description, and observer.[12]\nBuilding on the work of E.A. Singer, Jr., C. West Churchman, and Russell L. Ackoff, Umpleby has suggested that, since managers are part of the system they seek to influence, methods rather than theories are more effective ways to present knowledge of management.[13]\n Umpleby recently has worked to further develop the Quality Improvement Priority Matrix, a method for determining priorities for improvement and for monitoring perceived improvement.[14]\n In 1960 Heinz von Foerster published an article in Science showing that if demographic trends of the past two millennia continue, world population would go to infinity in approximately 2026.  Although contested in the 1960s, the equation proved remarkably accurate, indeed even conservative, until the early 1990s.[15]\nDiscussions of the doomsday equation revealed that demographers and natural scientists have fundamentally different ways of dealing with estimates and that these differences are not generally known by the public, science journalists, or other scientists.[16]\n From 1997 to 2000 Umpleby worked on the Year 2000 Computer Problem, viewing it as an opportunity to test social science theories using a before and after research design.[17]\n Between 1977 and 1980 he was the moderator of a computer conference on general systems theory supported by the National Science Foundation.  This project was one of nine \"experimental trials of electronic information exchange for small research communities\".  About sixty scientists in the United States, Canada, and Europe interacted for a period of two and a half years using the Electronic Information Exchange System (EIES) located at New Jersey Institute of Technology.[18]\nContinuing the work with computer-based communications media, Umpleby has experimented with applications of the internet.  Currently he is developing the idea of academic globalization, since it is now possible for academics to collaborate via the internet with colleagues in foreign countries for purposes of education, research or community service.[19]\n With the collapse of communism in 1989 many social scientists both in Russia and the West said that, although Karl Marx had described the transition from capitalism to socialism to communism, there were no theories to guide the transition from communism to capitalism.  Umpleby refuted this claim by organizing meetings in 1990 in both Washington, DC, and Vienna, Austria, to discuss the theories of economic, political, and social development that can guide the transformation of socialist societies.[20]\n Since 1994 the Research Program in Social and Organizational Learning at The George Washington University, which Umpleby heads, has hosted over 150 visiting scholars supported by the U.S. Department of State.  Most of these scholars have come from the former Soviet Union and Southeast Europe.  While on campus the scholars work with professors in their fields.  They also learn process improvement and group facilitation methods, so they can be more effective in introducing changes when they return home. In this way Umpleby has experimented with ways to encourage the use of participatory methods in other countries.[21]  He has found that the Participatory Strategic Planning methods developed by the Institute of Cultural Affairs not only improve the effectiveness of organizations but also lead to more humane management practices and build mutual trust among the participants.[22]\n Umpleby has been an Academician in the International Academy for Systems and Cybernetic Sciences,[23] an honor society created by the International Federation for Systems Research, since 2010.[24]\n Stuart Umpleby has written numerous articles, edited several special issues of the journal Cybernetics and Systems, and edited two books: \n",
      "timestamp": "2025-10-09 19:12:45.682083"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Stuart_Kauffman",
      "text": "\n Stuart Alan Kauffman (born September 28, 1939) is an American medical doctor, theoretical biologist, and complex systems researcher who studies the origin of life on Earth. He was a professor at the University of Chicago, University of Pennsylvania, and University of Calgary. He is currently emeritus professor of biochemistry at the University of Pennsylvania and affiliate faculty at the Institute for Systems Biology. He has a number of awards including a MacArthur Fellowship and a Wiener Medal.\n He is best known for arguing that the complexity of biological systems and organisms might result as much from self-organization and far-from-equilibrium dynamics as from Darwinian natural selection, as discussed in his book Origins of Order (1993). In 1967[1] and 1969[2] he used random Boolean networks to investigate generic self-organizing properties of gene regulatory networks, proposing that cell types are dynamical attractors in gene regulatory networks and that cell differentiation can be understood as transitions between attractors. Recent evidence suggests that cell types in humans and other organisms are attractors.[3][4] In 1971 he suggested that a zygote may not be able to access all the cell type attractors in its gene regulatory network during development and that some of the developmentally inaccessible cell types might be cancer cell types.[5] This suggested the possibility of \"cancer differentiation therapy\". He also proposed the self-organized emergence of collectively autocatalytic sets of polymers, specifically peptides, for the origin of molecular reproduction,[6][7] which have found experimental support.[8][9]\n Kauffman graduated from Dartmouth in 1960, was awarded the BA (Hons) by Oxford University (where he was a Marshall Scholar) in 1963, and completed a medical degree (M.D.) at the University of California, San Francisco in 1968. After completing his internship, he moved into developmental genetics of the fruit fly, holding appointments first at the University of Chicago from 1969 to 1973, the National Cancer Institute from 1973 to 1975, and then at the University of Pennsylvania from 1975 to 1994, where he rose to professor of biochemistry and biophysics.\n Kauffman became known through his association with the Santa Fe Institute (a non-profit research institute dedicated to the study of complex systems), where he was faculty in residence from 1986 to 1997, and through his work on models in various areas of biology. These included autocatalytic sets in origin of life research, gene regulatory networks in developmental biology, and fitness landscapes in evolutionary biology. With Marc Ballivet, Kauffman holds the founding broad biotechnology patents in combinatorial chemistry and applied molecular evolution, first issued in France in 1987,[10] in England in 1989, and later in North America.[11][12]\n In 1996, with Ernst and Young, Kauffman started BiosGroup, a Santa Fe, New Mexico-based for-profit company that applied complex systems methodology to business problems. BiosGroup was acquired by NuTech Solutions in early 2003. NuTech was bought by Netezza in 2008, and later by IBM.[13][14][15]\n From 2005 to 2009 Kauffman held a joint appointment at the University of Calgary in biological sciences, physics, and astronomy. He was also an adjunct professor in the Department of Philosophy at the University of Calgary. He was an iCORE (Informatics Research Circle of Excellence) chair and the director of the Institute for Biocomplexity and Informatics. Kauffman was also invited to help launch the Science and Religion initiative at Harvard Divinity School; serving as visiting professor in 2009.\n In January 2009 Kauffman became a Finland Distinguished Professor (FiDiPro) at Tampere University of Technology, Department of Signal Processing. The appointment ended in December, 2012. The subject of the FiDiPro research project is the development of delayed stochastic models of genetic regulatory networks based on gene expression data at the single molecule level.\n In January 2010 Kauffman joined the University of Vermont faculty where he continued his work for two years with UVM's Complex Systems Center.[16] From early 2011 to April 2013, Kauffman was a regular contributor to the NPR Blog 13.7, Cosmos and Culture,[17] with topics ranging from the life sciences, systems biology, and medicine, to spirituality, economics, and the law.[17]\n In May 2013 he joined the Institute for Systems Biology, in Seattle, Washington. Following the death of his wife, Kauffman cofounded Transforming Medicine: The Elizabeth Kauffman Institute.[18]\n In 2014, Kauffman with Samuli Niiranen and Gabor Vattay was issued a founding patent[19] on the poised realm (see below), an apparently new \"state of matter\" hovering reversibly between quantum and classical realms.[20]\n In 2015, he was invited to help initiate a general a discussion on rethinking economic growth for the United Nations.[21] Around the same time, he did research with University of Oxford professor Teppo Felin.[22]\n Kauffman's NK model defines a combinatorial phase space, consisting of every string (chosen from a given alphabet) of length \n\n\n\nN\n\n\n{\\displaystyle N}\n\n. For each string in this search space, a scalar value (called the fitness) is defined. If a distance metric is defined between strings, the resulting structure is a landscape.\n Fitness values are defined according to the specific incarnation of the model, but the key feature of the NK model is that the fitness of a given string \n\n\n\nS\n\n\n{\\displaystyle S}\n\n is the sum of contributions from each locus \n\n\n\n\nS\n\ni\n\n\n\n\n{\\displaystyle S_{i}}\n\n in the string:\n and the contribution from each locus in general depends on the value of \n\n\n\nK\n\n\n{\\displaystyle K}\n\n other loci:\n where \n\n\n\n\nS\n\nj\n\n\ni\n\n\n\n\n{\\displaystyle S_{j}^{i}}\n\n are the other loci upon which the fitness of \n\n\n\n\nS\n\ni\n\n\n\n\n{\\displaystyle S_{i}}\n\n depends.\n Hence, the fitness function \n\n\n\nf\n(\n\nS\n\ni\n\n\n,\n\nS\n\n1\n\n\ni\n\n\n,\n\u2026\n,\n\nS\n\nK\n\n\ni\n\n\n)\n\n\n{\\displaystyle f(S_{i},S_{1}^{i},\\dots ,S_{K}^{i})}\n\n is a mapping between strings of length K\u00a0+\u00a01 and scalars, which Weinberger's later work calls \"fitness contributions\". Such fitness contributions are often chosen randomly from some specified probability distribution.\n In 1991, Weinberger published a detailed analysis[23] of the case in which \n\n\n\n1\n<<\nk\n\u2264\nN\n\n\n{\\displaystyle 1<<k\\leq N}\n\n and the fitness contributions are chosen randomly. His analytical estimate of the number of local optima was later shown to be flawed.[citation needed] However, numerical experiments included in Weinberger's analysis support his analytical result that the expected fitness of a string is normally distributed with a mean of approximately\n\n\n\n\n\u03bc\n+\n\u03c3\n\n\n\n\n2\nln\n\u2061\n(\nk\n+\n1\n)\n\n\nk\n+\n1\n\n\n\n\n\n\n{\\displaystyle \\mu +\\sigma {\\sqrt {{2\\ln(k+1)} \\over {k+1}}}}\n\n\nand a variance of approximately\n\n\n\n\n\n\n\n(\nk\n+\n1\n)\n\n\u03c3\n\n2\n\n\n\n\nN\n[\nk\n+\n1\n+\n2\n(\nk\n+\n2\n)\nln\n\u2061\n(\nk\n+\n1\n)\n]\n\n\n\n\n\n{\\displaystyle {{(k+1)\\sigma ^{2}} \\over {N[k+1+2(k+2)\\ln(k+1)]}}}\n\n.\n Kauffman held a MacArthur Fellowship between 1987 and 1992. He also holds an Honorary Degree in Science from the University of Louvain (1997); He was awarded the Norbert Wiener Memorial Gold Medal for Cybernetics in 1973, the Gold Medal of the Accademia dei Lincei in Rome in 1990, the Trotter Prize for Information and Complexity in 2001, and the Herbert Simon award for Complex Systems in 2013. He became a Fellow of the Royal Society of Canada in 2009.\n Kauffman is best known for arguing that the complexity of biological systems and organisms might result as much from self-organization and far-from-equilibrium dynamics as from Darwinian natural selection in three areas of evolutionary biology, namely population dynamics, molecular evolution, and morphogenesis. With respect to molecular biology, Kauffman's structuralist approach has been criticized for ignoring the role of energy in driving biochemical reactions in cells, which can fairly be called self-catalyzing but which do not simply self-organize.[24] Some biologists and physicists working in Kauffman's area have questioned his claims about self-organization and evolution. A case in point is some comments in the 2001 book Self-Organization in Biological Systems.[25] Roger Sansom's 2011 book Ingenious Genes: How Gene Regulation Networks Evolve to Control Development is an extended criticism of Kauffman's model of self-organization in relation to gene regulatory networks.[26]\n Borrowing from spin glass models in physics, Kauffman invented \"N-K\" fitness landscapes, which have found applications in biology[27] and economics.[28][29] In related work, Kauffman and colleagues have examined subcritical, critical, and supracritical behavior in economic systems.[30]\n Kauffman's work translates his biological findings to the mind-body problem and issues in neuroscience, proposing attributes of a new \"poised realm\" that hovers indefinitely between quantum coherence and classicality. He published on this topic in his paper \"Answering Descartes: beyond Turing\".[31] With Giuseppe Longo and Ma\u00ebl Mont\u00e9vil, he wrote (January 2012) \"No Entailing Laws, But Enablement in the Evolution of the Biosphere\",[32] which argued that evolution is not \"law entailed\" like physics.\n Kauffman's work is posted on Physics ArXiv, including \"Beyond the Stalemate: Mind/Body, Quantum Mechanics, Free Will, Possible Panpsychism, Possible Solution to the Quantum Enigma\" (October 2014)[33] and \"Quantum Criticality at the Origin of Life\" (February 2015).[20]\n Kauffman has contributed to the emerging field of cumulative technological evolution by introducing a mathematics of the adjacent possible.[34][35]\n He has published over 350 articles and 6 books: The Origins of Order (1993), At Home in the Universe (1995), Investigations (2000), Reinventing the Sacred (2008), Humanity in a Creative Universe (2016), and A World Beyond Physics (2019).\n In 2016, Kauffman wrote a children's story, \"Patrick, Rupert, Sly & Gus Protocells\", a narrative about unprestatable niche creation in the biosphere, which was later produced as a short animated video.[36]\n In 2017, exploring the concept that reality consists of both ontologically real \"possibles\" (res potentia) and ontologically real \"actuals\" (res extensa), Kauffman co-authored, with Ruth Kastner and Michael Epperson, \"Taking Heisenberg's Potentia Seriously\".[37]\n",
      "timestamp": "2025-10-09 19:12:46.088702"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Stuart_Kauffman",
      "title": "Stuart Kauffman - Wikipedia",
      "description": "",
      "text": "\n Stuart Alan Kauffman (born September 28, 1939) is an American medical doctor, theoretical biologist, and complex systems researcher who studies the origin of life on Earth. He was a professor at the University of Chicago, University of Pennsylvania, and University of Calgary. He is currently emeritus professor of biochemistry at the University of Pennsylvania and affiliate faculty at the Institute for Systems Biology. He has a number of awards including a MacArthur Fellowship and a Wiener Medal.\n He is best known for arguing that the complexity of biological systems and organisms might result as much from self-organization and far-from-equilibrium dynamics as from Darwinian natural selection, as discussed in his book Origins of Order (1993). In 1967[1] and 1969[2] he used random Boolean networks to investigate generic self-organizing properties of gene regulatory networks, proposing that cell types are dynamical attractors in gene regulatory networks and that cell differentiation can be understood as transitions between attractors. Recent evidence suggests that cell types in humans and other organisms are attractors.[3][4] In 1971 he suggested that a zygote may not be able to access all the cell type attractors in its gene regulatory network during development and that some of the developmentally inaccessible cell types might be cancer cell types.[5] This suggested the possibility of \"cancer differentiation therapy\". He also proposed the self-organized emergence of collectively autocatalytic sets of polymers, specifically peptides, for the origin of molecular reproduction,[6][7] which have found experimental support.[8][9]\n Kauffman graduated from Dartmouth in 1960, was awarded the BA (Hons) by Oxford University (where he was a Marshall Scholar) in 1963, and completed a medical degree (M.D.) at the University of California, San Francisco in 1968. After completing his internship, he moved into developmental genetics of the fruit fly, holding appointments first at the University of Chicago from 1969 to 1973, the National Cancer Institute from 1973 to 1975, and then at the University of Pennsylvania from 1975 to 1994, where he rose to professor of biochemistry and biophysics.\n Kauffman became known through his association with the Santa Fe Institute (a non-profit research institute dedicated to the study of complex systems), where he was faculty in residence from 1986 to 1997, and through his work on models in various areas of biology. These included autocatalytic sets in origin of life research, gene regulatory networks in developmental biology, and fitness landscapes in evolutionary biology. With Marc Ballivet, Kauffman holds the founding broad biotechnology patents in combinatorial chemistry and applied molecular evolution, first issued in France in 1987,[10] in England in 1989, and later in North America.[11][12]\n In 1996, with Ernst and Young, Kauffman started BiosGroup, a Santa Fe, New Mexico-based for-profit company that applied complex systems methodology to business problems. BiosGroup was acquired by NuTech Solutions in early 2003. NuTech was bought by Netezza in 2008, and later by IBM.[13][14][15]\n From 2005 to 2009 Kauffman held a joint appointment at the University of Calgary in biological sciences, physics, and astronomy. He was also an adjunct professor in the Department of Philosophy at the University of Calgary. He was an iCORE (Informatics Research Circle of Excellence) chair and the director of the Institute for Biocomplexity and Informatics. Kauffman was also invited to help launch the Science and Religion initiative at Harvard Divinity School; serving as visiting professor in 2009.\n In January 2009 Kauffman became a Finland Distinguished Professor (FiDiPro) at Tampere University of Technology, Department of Signal Processing. The appointment ended in December, 2012. The subject of the FiDiPro research project is the development of delayed stochastic models of genetic regulatory networks based on gene expression data at the single molecule level.\n In January 2010 Kauffman joined the University of Vermont faculty where he continued his work for two years with UVM's Complex Systems Center.[16] From early 2011 to April 2013, Kauffman was a regular contributor to the NPR Blog 13.7, Cosmos and Culture,[17] with topics ranging from the life sciences, systems biology, and medicine, to spirituality, economics, and the law.[17]\n In May 2013 he joined the Institute for Systems Biology, in Seattle, Washington. Following the death of his wife, Kauffman cofounded Transforming Medicine: The Elizabeth Kauffman Institute.[18]\n In 2014, Kauffman with Samuli Niiranen and Gabor Vattay was issued a founding patent[19] on the poised realm (see below), an apparently new \"state of matter\" hovering reversibly between quantum and classical realms.[20]\n In 2015, he was invited to help initiate a general a discussion on rethinking economic growth for the United Nations.[21] Around the same time, he did research with University of Oxford professor Teppo Felin.[22]\n Kauffman's NK model defines a combinatorial phase space, consisting of every string (chosen from a given alphabet) of length \n\n\n\nN\n\n\n{\\displaystyle N}\n\n. For each string in this search space, a scalar value (called the fitness) is defined. If a distance metric is defined between strings, the resulting structure is a landscape.\n Fitness values are defined according to the specific incarnation of the model, but the key feature of the NK model is that the fitness of a given string \n\n\n\nS\n\n\n{\\displaystyle S}\n\n is the sum of contributions from each locus \n\n\n\n\nS\n\ni\n\n\n\n\n{\\displaystyle S_{i}}\n\n in the string:\n and the contribution from each locus in general depends on the value of \n\n\n\nK\n\n\n{\\displaystyle K}\n\n other loci:\n where \n\n\n\n\nS\n\nj\n\n\ni\n\n\n\n\n{\\displaystyle S_{j}^{i}}\n\n are the other loci upon which the fitness of \n\n\n\n\nS\n\ni\n\n\n\n\n{\\displaystyle S_{i}}\n\n depends.\n Hence, the fitness function \n\n\n\nf\n(\n\nS\n\ni\n\n\n,\n\nS\n\n1\n\n\ni\n\n\n,\n\u2026\n,\n\nS\n\nK\n\n\ni\n\n\n)\n\n\n{\\displaystyle f(S_{i},S_{1}^{i},\\dots ,S_{K}^{i})}\n\n is a mapping between strings of length K\u00a0+\u00a01 and scalars, which Weinberger's later work calls \"fitness contributions\". Such fitness contributions are often chosen randomly from some specified probability distribution.\n In 1991, Weinberger published a detailed analysis[23] of the case in which \n\n\n\n1\n<<\nk\n\u2264\nN\n\n\n{\\displaystyle 1<<k\\leq N}\n\n and the fitness contributions are chosen randomly. His analytical estimate of the number of local optima was later shown to be flawed.[citation needed] However, numerical experiments included in Weinberger's analysis support his analytical result that the expected fitness of a string is normally distributed with a mean of approximately\n\n\n\n\n\u03bc\n+\n\u03c3\n\n\n\n\n2\nln\n\u2061\n(\nk\n+\n1\n)\n\n\nk\n+\n1\n\n\n\n\n\n\n{\\displaystyle \\mu +\\sigma {\\sqrt {{2\\ln(k+1)} \\over {k+1}}}}\n\n\nand a variance of approximately\n\n\n\n\n\n\n\n(\nk\n+\n1\n)\n\n\u03c3\n\n2\n\n\n\n\nN\n[\nk\n+\n1\n+\n2\n(\nk\n+\n2\n)\nln\n\u2061\n(\nk\n+\n1\n)\n]\n\n\n\n\n\n{\\displaystyle {{(k+1)\\sigma ^{2}} \\over {N[k+1+2(k+2)\\ln(k+1)]}}}\n\n.\n Kauffman held a MacArthur Fellowship between 1987 and 1992. He also holds an Honorary Degree in Science from the University of Louvain (1997); He was awarded the Norbert Wiener Memorial Gold Medal for Cybernetics in 1973, the Gold Medal of the Accademia dei Lincei in Rome in 1990, the Trotter Prize for Information and Complexity in 2001, and the Herbert Simon award for Complex Systems in 2013. He became a Fellow of the Royal Society of Canada in 2009.\n Kauffman is best known for arguing that the complexity of biological systems and organisms might result as much from self-organization and far-from-equilibrium dynamics as from Darwinian natural selection in three areas of evolutionary biology, namely population dynamics, molecular evolution, and morphogenesis. With respect to molecular biology, Kauffman's structuralist approach has been criticized for ignoring the role of energy in driving biochemical reactions in cells, which can fairly be called self-catalyzing but which do not simply self-organize.[24] Some biologists and physicists working in Kauffman's area have questioned his claims about self-organization and evolution. A case in point is some comments in the 2001 book Self-Organization in Biological Systems.[25] Roger Sansom's 2011 book Ingenious Genes: How Gene Regulation Networks Evolve to Control Development is an extended criticism of Kauffman's model of self-organization in relation to gene regulatory networks.[26]\n Borrowing from spin glass models in physics, Kauffman invented \"N-K\" fitness landscapes, which have found applications in biology[27] and economics.[28][29] In related work, Kauffman and colleagues have examined subcritical, critical, and supracritical behavior in economic systems.[30]\n Kauffman's work translates his biological findings to the mind-body problem and issues in neuroscience, proposing attributes of a new \"poised realm\" that hovers indefinitely between quantum coherence and classicality. He published on this topic in his paper \"Answering Descartes: beyond Turing\".[31] With Giuseppe Longo and Ma\u00ebl Mont\u00e9vil, he wrote (January 2012) \"No Entailing Laws, But Enablement in the Evolution of the Biosphere\",[32] which argued that evolution is not \"law entailed\" like physics.\n Kauffman's work is posted on Physics ArXiv, including \"Beyond the Stalemate: Mind/Body, Quantum Mechanics, Free Will, Possible Panpsychism, Possible Solution to the Quantum Enigma\" (October 2014)[33] and \"Quantum Criticality at the Origin of Life\" (February 2015).[20]\n Kauffman has contributed to the emerging field of cumulative technological evolution by introducing a mathematics of the adjacent possible.[34][35]\n He has published over 350 articles and 6 books: The Origins of Order (1993), At Home in the Universe (1995), Investigations (2000), Reinventing the Sacred (2008), Humanity in a Creative Universe (2016), and A World Beyond Physics (2019).\n In 2016, Kauffman wrote a children's story, \"Patrick, Rupert, Sly & Gus Protocells\", a narrative about unprestatable niche creation in the biosphere, which was later produced as a short animated video.[36]\n In 2017, exploring the concept that reality consists of both ontologically real \"possibles\" (res potentia) and ontologically real \"actuals\" (res extensa), Kauffman co-authored, with Ruth Kastner and Michael Epperson, \"Taking Heisenberg's Potentia Seriously\".[37]\n",
      "timestamp": "2025-10-09 19:12:46.092665"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Stafford_Beer",
      "text": "\n Anthony Stafford Beer (25 September 1926 \u2013 23 August 2002) was a British theorist, consultant and professor at Manchester Business School.[1] He is known for his work in the fields of operational research and management cybernetics, and for his heuristic in systems thinking, \"the purpose of a system is what it does.\"[2]\n Anthony Stafford Beer was born in Putney, London, on 25 September 1926. His father was William John Beer, chief statistician at Lloyd's Register of Shipping, who shared a birthday with Stafford's mother, Doris Ethel Beer.[3] At the age of 17 Stafford was expelled from Whitgift School. He enrolled for a degree in philosophy at University College London before leaving to join the British Army as a gunner in the Royal Artillery in 1944, during the Second World War. He soon received commissions, first in the Royal Fusiliers, and then as a company commander in the 9th Gurkha Rifles. Beer served in the British Raj until 1947, when he returned to England and was assigned to the human factors branch of operations research at the War Office. In 1949 he was demobilised, having reached the rank of captain.[4]\n Beer did not use his given first name, \"Anthony\", instead preferring his middle name, \"Stafford\". His younger brother, Ian, also shared this middle name. When Ian was sixteen, Beer persuaded him to sign a document promising not to use \"Stafford\" as part of his name because Beer \"wanted the \u2018copyright\u2019 of [the name] Stafford Beer.\"[5]\n In 1956 he joined United Steel and persuaded the management to fund an operational research group, the Department of Operations Research and Cybernetics, which he headed. This was based in Cybor House, and they installed a Ferranti Pegasus computer, the first in the world dedicated to management cybernetics.[6]\n In 1961 he left United Steel to start an operational research consultancy in partnership with Roger Eddison called SIGMA (Science in General Management). Beer left SIGMA in 1966 to work for a SIGMA client, the International Publishing Corporation (IPC). He left IPC in 1970 to work as an independent consultant, focusing on his growing interest in social systems.[7] His engagement in Latin America began in the 1960s through SIGMA, which worked on industrial optimisation projects in Chile and unsuccessfully explored expansion into other regional markets.[8]\n In mid-1971 Beer was approached by Fernando Flores, then a high-ranking member of the Chilean Production Development Corporation (CORFO) in the newly elected socialist government of Salvador Allende, for advice on applying his cybernetic theories to the management of the state-run sector of the Chilean economy.[9][10]\n This led to Beer's involvement in the never-completed Cybersyn project, which aimed to use computers and a telex-based communication network to allow the government to maximise production while preserving the autonomy of workers and lower management.\n Beer also was reported to have read and been influenced by Leon Trotsky's critique of the Soviet bureaucracy.[11] According to another senior member of the Cybersyn team, Herman Schwember, Beer's political background and readings completely derived from works written by Trotsky and Trotskyists. Schwember himself disapproved of Trotsky's approach.[12]\n Although Cybersyn was abandoned after Allende's death during the Pinochet coup in 1973, Beer continued to work in the Americas, consulting for the governments of Canada, Mexico, Uruguay and Venezuela. Beer was particularly involved in the 1980s and 1990s on various governmental cybernetic projects, including Uruguay's successful URUCIB executive information system (1986\u20131988), Colombia's application of the Viable System Model to public sector reform (1990s\u20132000s) and unsuccessful ventures in Mexico and Venezuela that were undermined by corruption and political instability.[13][14]\n In the mid-1970s he moved to Mid Wales,[15] where he lived an almost austere lifestyle, developing strong interests in poetry and art. In the 1980s he established a second home on the west side of downtown Toronto.[16] He was a visiting professor at almost 30 universities and received an earned higher doctorate (DSc) from the University of Sunderland and honorary doctorates from the University of Leeds, the University of St. Gallen and the University of Valladolid. He was president of the World Organization of Systems and Cybernetics.[17]\n In July 1994 Beer ran a residential course at Falcondale House in Lampeter. Nine sessions were recorded as a video learning resource, and are collectively known as the Falcondale collection. They are available online at the Data Repository of Liverpool John Moores University.[18] The sessions covered art, science and philosophy as well as the practical application of cybernetics in society, government, community, management and business. Transcripts were made of the discussions and are also available from the same repository.[18]\n He was married twice, in 1947 to Cynthia Hannaway, and in 1968 to Sallie Steadman. His partner for the last twenty years of his life was Allenna Leonard, a fellow cybernetician. Beer had five sons and two daughters, one of whom is Vanilla Beer, an artist and essayist.[citation needed]\n According to Mike Jackson, a systems scientist, \"Beer was the first to apply cybernetics to management, defining cybernetics as the science of effective organization\". In the 1960s and early 1970s \"Beer was a prolific writer and an influential practitioner\" in management cybernetics. It was during that period that he developed the viable system model, to diagnose the faults in any existing organisational system. In that time Jay Wright Forrester invented systems dynamics, which \"held out the promise that the behaviour of whole systems could be represented and understood through modelling the dynamical feedback process going on within them\".[20]\n During the presidency of Salvador Allende in Chile in the early 1970s, Beer was closely involved with a visionary project, Cybersyn, to apply his cybernetic theories in government. The project's ultimate goal was to create a network of computers and communications equipment that would support the management of the state-run sector of Chile's economy; at its core would be an operations room where government managers could view important information about economic processes in real time, formulate plans of action, and transmit advice and directives to managers at plants and enterprises in the field.[21] However, consistent with cybernetic principles and the ideals of the Allende government, its designers aimed to preserve worker and lower-management autonomy instead of implementing a top-down system of centralised control. The system used a network of about 500 telex machines located at enterprises throughout the country and in government offices in Santiago, some of which were connected to a government-operated mainframe computer that would receive information on production operations, feed that information into economic modelling software, and report on variables (such as raw material supplies) that were outside normal parameters and might require attention. The project, implemented by a multidisciplinary group of both Chileans and foreigners, reached an advanced prototype stage, but was interrupted by the 1973 coup d'\u00e9tat.[21]\n The Viable System Model (VSM) is a model of the organisational structure of any viable or autonomous system. A viable system is any system organised in such a way as to meet the demands of surviving in the changing environment. One of the prime features of systems that survive is that they are adaptable. The VSM expresses a model for a viable system, which is an abstracted cybernetic description that is applicable to any organisation that is a viable system and capable of autonomy.\n Syntegrity is a formal model presented by Beer in the 1990s and now is a registered trademark. It is a form of non-hierarchical problem solving that can be used in a small team of 10 to 42 people. It is a business consultation product that is licensed out to consulting firms. The term comes from the words \"synergistic\" and \"tensegrity\".[22]\n Stafford Beer coined and frequently used the term POSIWID (the purpose of a system is what it does) to refer to the commonly observed phenomenon that the de facto purpose of a system is often at odds with its official purpose. In an address to the University of Valladolid in October 2001, he said \"According to the cybernetician the purpose of a system is what it does. This is a basic dictum. It stands for bald fact, which makes a better starting point in seeking understanding than the familiar attributions of good intention, prejudices about expectations, moral judgment or sheer ignorance of circumstances.\"[23] This principle has been used to describe Social Machines as intelligent, for example in the case of \"games with a purpose\",[24] and it provides a link between AI and cybernetics.\n Beer received awards from the Royal Swedish Academy of Engineering Sciences in 1958, from the United Kingdom Systems Society, the Cybernetics Society, the American Society for Cybernetics and the Operations Research Society of America.[citation needed]\n Beer wrote several books and articles:[26]\n Audio\n Video\n Organizations\n",
      "timestamp": "2025-10-09 19:12:46.622935"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Stafford_Beer",
      "title": "Stafford Beer - Wikipedia",
      "description": "",
      "text": "\n Anthony Stafford Beer (25 September 1926 \u2013 23 August 2002) was a British theorist, consultant and professor at Manchester Business School.[1] He is known for his work in the fields of operational research and management cybernetics, and for his heuristic in systems thinking, \"the purpose of a system is what it does.\"[2]\n Anthony Stafford Beer was born in Putney, London, on 25 September 1926. His father was William John Beer, chief statistician at Lloyd's Register of Shipping, who shared a birthday with Stafford's mother, Doris Ethel Beer.[3] At the age of 17 Stafford was expelled from Whitgift School. He enrolled for a degree in philosophy at University College London before leaving to join the British Army as a gunner in the Royal Artillery in 1944, during the Second World War. He soon received commissions, first in the Royal Fusiliers, and then as a company commander in the 9th Gurkha Rifles. Beer served in the British Raj until 1947, when he returned to England and was assigned to the human factors branch of operations research at the War Office. In 1949 he was demobilised, having reached the rank of captain.[4]\n Beer did not use his given first name, \"Anthony\", instead preferring his middle name, \"Stafford\". His younger brother, Ian, also shared this middle name. When Ian was sixteen, Beer persuaded him to sign a document promising not to use \"Stafford\" as part of his name because Beer \"wanted the \u2018copyright\u2019 of [the name] Stafford Beer.\"[5]\n In 1956 he joined United Steel and persuaded the management to fund an operational research group, the Department of Operations Research and Cybernetics, which he headed. This was based in Cybor House, and they installed a Ferranti Pegasus computer, the first in the world dedicated to management cybernetics.[6]\n In 1961 he left United Steel to start an operational research consultancy in partnership with Roger Eddison called SIGMA (Science in General Management). Beer left SIGMA in 1966 to work for a SIGMA client, the International Publishing Corporation (IPC). He left IPC in 1970 to work as an independent consultant, focusing on his growing interest in social systems.[7] His engagement in Latin America began in the 1960s through SIGMA, which worked on industrial optimisation projects in Chile and unsuccessfully explored expansion into other regional markets.[8]\n In mid-1971 Beer was approached by Fernando Flores, then a high-ranking member of the Chilean Production Development Corporation (CORFO) in the newly elected socialist government of Salvador Allende, for advice on applying his cybernetic theories to the management of the state-run sector of the Chilean economy.[9][10]\n This led to Beer's involvement in the never-completed Cybersyn project, which aimed to use computers and a telex-based communication network to allow the government to maximise production while preserving the autonomy of workers and lower management.\n Beer also was reported to have read and been influenced by Leon Trotsky's critique of the Soviet bureaucracy.[11] According to another senior member of the Cybersyn team, Herman Schwember, Beer's political background and readings completely derived from works written by Trotsky and Trotskyists. Schwember himself disapproved of Trotsky's approach.[12]\n Although Cybersyn was abandoned after Allende's death during the Pinochet coup in 1973, Beer continued to work in the Americas, consulting for the governments of Canada, Mexico, Uruguay and Venezuela. Beer was particularly involved in the 1980s and 1990s on various governmental cybernetic projects, including Uruguay's successful URUCIB executive information system (1986\u20131988), Colombia's application of the Viable System Model to public sector reform (1990s\u20132000s) and unsuccessful ventures in Mexico and Venezuela that were undermined by corruption and political instability.[13][14]\n In the mid-1970s he moved to Mid Wales,[15] where he lived an almost austere lifestyle, developing strong interests in poetry and art. In the 1980s he established a second home on the west side of downtown Toronto.[16] He was a visiting professor at almost 30 universities and received an earned higher doctorate (DSc) from the University of Sunderland and honorary doctorates from the University of Leeds, the University of St. Gallen and the University of Valladolid. He was president of the World Organization of Systems and Cybernetics.[17]\n In July 1994 Beer ran a residential course at Falcondale House in Lampeter. Nine sessions were recorded as a video learning resource, and are collectively known as the Falcondale collection. They are available online at the Data Repository of Liverpool John Moores University.[18] The sessions covered art, science and philosophy as well as the practical application of cybernetics in society, government, community, management and business. Transcripts were made of the discussions and are also available from the same repository.[18]\n He was married twice, in 1947 to Cynthia Hannaway, and in 1968 to Sallie Steadman. His partner for the last twenty years of his life was Allenna Leonard, a fellow cybernetician. Beer had five sons and two daughters, one of whom is Vanilla Beer, an artist and essayist.[citation needed]\n According to Mike Jackson, a systems scientist, \"Beer was the first to apply cybernetics to management, defining cybernetics as the science of effective organization\". In the 1960s and early 1970s \"Beer was a prolific writer and an influential practitioner\" in management cybernetics. It was during that period that he developed the viable system model, to diagnose the faults in any existing organisational system. In that time Jay Wright Forrester invented systems dynamics, which \"held out the promise that the behaviour of whole systems could be represented and understood through modelling the dynamical feedback process going on within them\".[20]\n During the presidency of Salvador Allende in Chile in the early 1970s, Beer was closely involved with a visionary project, Cybersyn, to apply his cybernetic theories in government. The project's ultimate goal was to create a network of computers and communications equipment that would support the management of the state-run sector of Chile's economy; at its core would be an operations room where government managers could view important information about economic processes in real time, formulate plans of action, and transmit advice and directives to managers at plants and enterprises in the field.[21] However, consistent with cybernetic principles and the ideals of the Allende government, its designers aimed to preserve worker and lower-management autonomy instead of implementing a top-down system of centralised control. The system used a network of about 500 telex machines located at enterprises throughout the country and in government offices in Santiago, some of which were connected to a government-operated mainframe computer that would receive information on production operations, feed that information into economic modelling software, and report on variables (such as raw material supplies) that were outside normal parameters and might require attention. The project, implemented by a multidisciplinary group of both Chileans and foreigners, reached an advanced prototype stage, but was interrupted by the 1973 coup d'\u00e9tat.[21]\n The Viable System Model (VSM) is a model of the organisational structure of any viable or autonomous system. A viable system is any system organised in such a way as to meet the demands of surviving in the changing environment. One of the prime features of systems that survive is that they are adaptable. The VSM expresses a model for a viable system, which is an abstracted cybernetic description that is applicable to any organisation that is a viable system and capable of autonomy.\n Syntegrity is a formal model presented by Beer in the 1990s and now is a registered trademark. It is a form of non-hierarchical problem solving that can be used in a small team of 10 to 42 people. It is a business consultation product that is licensed out to consulting firms. The term comes from the words \"synergistic\" and \"tensegrity\".[22]\n Stafford Beer coined and frequently used the term POSIWID (the purpose of a system is what it does) to refer to the commonly observed phenomenon that the de facto purpose of a system is often at odds with its official purpose. In an address to the University of Valladolid in October 2001, he said \"According to the cybernetician the purpose of a system is what it does. This is a basic dictum. It stands for bald fact, which makes a better starting point in seeking understanding than the familiar attributions of good intention, prejudices about expectations, moral judgment or sheer ignorance of circumstances.\"[23] This principle has been used to describe Social Machines as intelligent, for example in the case of \"games with a purpose\",[24] and it provides a link between AI and cybernetics.\n Beer received awards from the Royal Swedish Academy of Engineering Sciences in 1958, from the United Kingdom Systems Society, the Cybernetics Society, the American Society for Cybernetics and the Operations Research Society of America.[citation needed]\n Beer wrote several books and articles:[26]\n Audio\n Video\n Organizations\n",
      "timestamp": "2025-10-09 19:12:46.626167"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Sergei_P._Kurdyumov",
      "text": "Sergey Pavlovich Kurdyumov (Russian: \u0421\u0435\u0440\u0433\u0435\u0301\u0439 \u041f\u0430\u0301\u0432\u043b\u043e\u0432\u0438\u0447 \u041a\u0443\u0440\u0434\u044e\u0301\u043c\u043e\u0432; November 18, 1928 \u2013 December 2, 2004) was a specialist in mathematical physics, mathematical modeling, plasma physics, complexity studies and synergetics from Moscow, Russia.\n Kurdyumov graduated from the Moscow State University in 1957. Since 1953 he worked in the Keldysh Institute of Applied Mathematics. He was also the Head of the Applied Mathematics Department in the Moscow Institute of Physics and Technology. Since 1984 he was a corresponding member of the Russian Academy of Sciences.\n He was the author and co-author of more than 300 scientific works published in Russia and abroad, including about ten monographs. In 1969, he was the co-author of the scientific discovery of a new physical effect, \"Effect of T-layer\".\n Theoretical works in the field of nuclear power engineering, laser thermonuclear fusion, laser thermochemistry were carried out by him in person and under his direction. Methods of exploration of laser thermonuclear targets by means of computational experiments were elaborated by Kurdyumov (together with academician RAS Alexander Samarskii. These methods laid the foundation for the conception of low-entropic compression of shell targets and substantiated this conception which is generally accepted nowadays all over the world.\n Kurdyumov made an important contribution to the elaboration of fundamental problems of synergetics as well as to the theory of non-linear evolutionary equations. For quasi-linear equations of heat conductivity with a source, a theory of blow-up regimes was developed by Kurdyumov and his disciples. A spectrum of eigenfunctions of an open non-linear medium was studied, properties of diffusive chaos were investigated. The theory of blow-up regimes was extended to compressible media and the problems of dissipative and magnetic hydrodynamics. These results found an important application and experimental confirmation in the problems connected with laser thermonuclear fusion, in experiments on thermochemistry. From 1983 to 2004 under the direction of the Corresponding Member of the RAS Kurdyumov, and with his immediate assistance, a number of fundamental scientific results in the field of mathematical physics, nonlinear dynamics and synergetics were obtained. New methods of constructive analysis of solutions of a wide class of non-linear parabolic equations with sources and sinks were developed.\n The research results in the field were presented, in particular, in the monograph entitled \"Blow-up in quasi-linear parabolic equations\", Berlin: Walter de Gruyter, 1995 (in co-authorship with A. A. Samarskii, V. A. Galaktionov, A.P. Mikhailov) which has obtained worldwide recognition. A series of pioneer works connected with analysis of diffusive chaos and with complex order in systems reaction\u2013diffusion was accomplished; the works exert a great influence upon the development of studies in non-linear dynamics in Russia. The monograph \"Non-stationary Structures and Diffusion Chaos\", Moscow: Nauka Publishers, 1992, written in co-authorship with A.A. Samarskii, T.S. Achromeeva, G.G. Malinetskii is focused on these research results. Special attention is drawn to the application of the ideas of synergetics in such fields as strategic planning, analysis of the historical processes, the modelling of educational systems, the philosophical problems of natural sciences. Books devoted to these problems, especially, \u201cLaws of Evolution and Self-organization of Complex Systems\u201d, Moscow: Nauka Publishers, 1994, (in co-authorship with Helena N. Knyazeva), \u201cSynergetics and Forecast of the Future\u201d, Moscow: Nauka Publishers, 1997, (together with S.P. Kapitza and G.G. Malinetskii), \u201cFoundations of Synergetics: Blow-up Regimes, Self-organization, Tempo-worlds\u201d, Saint-Petersburg: Aletheia, 2002, (together with Helena N. Knyazeva), \"Synergetics: Non-linearity of Time and Landscapes of Co-evolution\", Moscow: KomKniga/URSS, 2007 (together with Helena N. Knyazeva), the collection of works of S.P. Kurdyumov and his disciples \u201cThe Blow-up Regimes. Evolution of the Idea. Laws of Co-evolution of Complex Systems\u201d, Moscow: Nauka Publishers, 1999, are of great interest for the scientific community. \n Kurdyumov was a great organizer of science in Russia. From 1989 to 1999, he was Director of the Keldysh Institute of Applied Mathematics of the RAS. Kurdyumov was Member of the Bureau of the Department of Computer Science, Computer Techniques and Automation. For many years, he was President of the International Computer Club, Vice-president of the National Committee on Mathematical Modelling. He was a Full Member of the European Academy of Sciences. Kurdyumov was a member of the editorial boards of five Russian and international scientific journals.\n He created a whole scientific school in nonlinear dynamics and synergetics in Russia. Ten theses for a Doctor's degree (Habilitation) and nineteen Ph.D. theses were upheld under his supervision. Kurdyumov was honoured for his prominent achievements with governmental awards. Among his awards were the Medal \"For Labour Valour\" (1956), the Medal \"For Distinguished Labour\" (1970), the Order of the Badge of Honour (1975) and \"Order of Honour\" (1999).\n",
      "timestamp": "2025-10-09 19:12:47.073164"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Sergei_P._Kurdyumov",
      "title": "Sergei Kurdyumov - Wikipedia",
      "description": "",
      "text": "Sergey Pavlovich Kurdyumov (Russian: \u0421\u0435\u0440\u0433\u0435\u0301\u0439 \u041f\u0430\u0301\u0432\u043b\u043e\u0432\u0438\u0447 \u041a\u0443\u0440\u0434\u044e\u0301\u043c\u043e\u0432; November 18, 1928 \u2013 December 2, 2004) was a specialist in mathematical physics, mathematical modeling, plasma physics, complexity studies and synergetics from Moscow, Russia.\n Kurdyumov graduated from the Moscow State University in 1957. Since 1953 he worked in the Keldysh Institute of Applied Mathematics. He was also the Head of the Applied Mathematics Department in the Moscow Institute of Physics and Technology. Since 1984 he was a corresponding member of the Russian Academy of Sciences.\n He was the author and co-author of more than 300 scientific works published in Russia and abroad, including about ten monographs. In 1969, he was the co-author of the scientific discovery of a new physical effect, \"Effect of T-layer\".\n Theoretical works in the field of nuclear power engineering, laser thermonuclear fusion, laser thermochemistry were carried out by him in person and under his direction. Methods of exploration of laser thermonuclear targets by means of computational experiments were elaborated by Kurdyumov (together with academician RAS Alexander Samarskii. These methods laid the foundation for the conception of low-entropic compression of shell targets and substantiated this conception which is generally accepted nowadays all over the world.\n Kurdyumov made an important contribution to the elaboration of fundamental problems of synergetics as well as to the theory of non-linear evolutionary equations. For quasi-linear equations of heat conductivity with a source, a theory of blow-up regimes was developed by Kurdyumov and his disciples. A spectrum of eigenfunctions of an open non-linear medium was studied, properties of diffusive chaos were investigated. The theory of blow-up regimes was extended to compressible media and the problems of dissipative and magnetic hydrodynamics. These results found an important application and experimental confirmation in the problems connected with laser thermonuclear fusion, in experiments on thermochemistry. From 1983 to 2004 under the direction of the Corresponding Member of the RAS Kurdyumov, and with his immediate assistance, a number of fundamental scientific results in the field of mathematical physics, nonlinear dynamics and synergetics were obtained. New methods of constructive analysis of solutions of a wide class of non-linear parabolic equations with sources and sinks were developed.\n The research results in the field were presented, in particular, in the monograph entitled \"Blow-up in quasi-linear parabolic equations\", Berlin: Walter de Gruyter, 1995 (in co-authorship with A. A. Samarskii, V. A. Galaktionov, A.P. Mikhailov) which has obtained worldwide recognition. A series of pioneer works connected with analysis of diffusive chaos and with complex order in systems reaction\u2013diffusion was accomplished; the works exert a great influence upon the development of studies in non-linear dynamics in Russia. The monograph \"Non-stationary Structures and Diffusion Chaos\", Moscow: Nauka Publishers, 1992, written in co-authorship with A.A. Samarskii, T.S. Achromeeva, G.G. Malinetskii is focused on these research results. Special attention is drawn to the application of the ideas of synergetics in such fields as strategic planning, analysis of the historical processes, the modelling of educational systems, the philosophical problems of natural sciences. Books devoted to these problems, especially, \u201cLaws of Evolution and Self-organization of Complex Systems\u201d, Moscow: Nauka Publishers, 1994, (in co-authorship with Helena N. Knyazeva), \u201cSynergetics and Forecast of the Future\u201d, Moscow: Nauka Publishers, 1997, (together with S.P. Kapitza and G.G. Malinetskii), \u201cFoundations of Synergetics: Blow-up Regimes, Self-organization, Tempo-worlds\u201d, Saint-Petersburg: Aletheia, 2002, (together with Helena N. Knyazeva), \"Synergetics: Non-linearity of Time and Landscapes of Co-evolution\", Moscow: KomKniga/URSS, 2007 (together with Helena N. Knyazeva), the collection of works of S.P. Kurdyumov and his disciples \u201cThe Blow-up Regimes. Evolution of the Idea. Laws of Co-evolution of Complex Systems\u201d, Moscow: Nauka Publishers, 1999, are of great interest for the scientific community. \n Kurdyumov was a great organizer of science in Russia. From 1989 to 1999, he was Director of the Keldysh Institute of Applied Mathematics of the RAS. Kurdyumov was Member of the Bureau of the Department of Computer Science, Computer Techniques and Automation. For many years, he was President of the International Computer Club, Vice-president of the National Committee on Mathematical Modelling. He was a Full Member of the European Academy of Sciences. Kurdyumov was a member of the editorial boards of five Russian and international scientific journals.\n He created a whole scientific school in nonlinear dynamics and synergetics in Russia. Ten theses for a Doctor's degree (Habilitation) and nineteen Ph.D. theses were upheld under his supervision. Kurdyumov was honoured for his prominent achievements with governmental awards. Among his awards were the Medal \"For Labour Valour\" (1956), the Medal \"For Distinguished Labour\" (1970), the Order of the Badge of Honour (1975) and \"Order of Honour\" (1999).\n",
      "timestamp": "2025-10-09 19:12:47.074475"
    }
  ],
  "last_update": "2025-10-10 10:28:48.795140",
  "stats": {
    "total_words_collected": 195812,
    "total_urls_processed": 100,
    "knowledge_categories": {
      "ai_ml": 4,
      "programming": 0,
      "research": 20
    }
  }
}